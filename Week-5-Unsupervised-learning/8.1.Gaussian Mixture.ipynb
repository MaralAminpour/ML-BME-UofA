{"cells":[{"cell_type":"markdown","metadata":{"id":"WU8f5bYDf_QB"},"source":["# Gaussian Mixture\n","\n","Welcome to this tutorial, where we're going to explore the world of Gaussian Mixture Models. This powerful statistical model allows us to model data that might not fit a single Gaussian distribution, but can be modeled as a mix of several Gaussian distributions.\n","\n","We'll first apply it to the **high-dimensional Wisconsin breast cancer dataset** and compare its performance with K-means clustering. We've chosen this dataset because it's complex enough to show the strengths of Gaussian Mixture Models, but also well-studied enough that we can confidently interpret the results.\n","\n","Next, we'll apply Gaussian Mixture Model to a more challenging task - the segmentation of **brain MRI** into three distinct categories: White Matter (WM), Gray Matter (GM), and Cerebro-spinal Fluid (CSF). This will allow us to explore various components of the model, such as image and class intensity distributions, also known as **likelihoods**, and the resulting probabilistic predictions, or **posteriors**.\n","\n","Let's get started!"]},{"cell_type":"markdown","metadata":{"id":"PpZQQSj4f_QC"},"source":["### Imports"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"FJ0V5zibf_QD"},"outputs":[],"source":["import matplotlib.pyplot as plt\n","%matplotlib inline\n","import numpy as np"]},{"cell_type":"markdown","metadata":{"id":"C5jvWsSJf_QE"},"source":["### Load Breast Cancer Dataset\n","\n","Now, we will proceed to load the Breast Cancer dataset. We'll construct a feature matrix, denoted as `X`, and a label vector, `y`. It's essential to bear in mind that although we're using the label vector, it's primarily for the assessment of our clustering results. The reason behind this is that clustering falls into the realm of unsupervised machine learning tasks, where we usually don't have access to any labels."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DGSGrhTzf_QE"},"outputs":[],"source":["from sklearn import datasets\n","\n","bc = datasets.load_breast_cancer()\n","\n","print('\\n Features: \\n', bc.feature_names)\n","print('\\n Labels: ', bc.target_names)\n","\n","# Feature matrix\n","X = bc.data\n","# Label vector\n","y = bc.target\n","print('\\n We have {} features.'.format(X.shape[1]))"]},{"cell_type":"markdown","metadata":{"id":"uCMSTUeWf_QE"},"source":["Throughout this module, you've learned how dimensionality reduction can help us make sense of high-dimensional datasets. `PCA` is our tool of choice for today to shed some light on our 30-dimensional Wisconsin Breast Cancer Dataset.\n","\n","**Activity 1:** Complete the code below to explore structure of the Wisconsin Breast Cancer Dataset."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_B9r9qr8f_QE"},"outputs":[],"source":["from sklearn.decomposition import PCA\n","from sklearn.preprocessing import StandardScaler\n","\n","# apply standard scaler to the feature matrix\n","X = StandardScaler().fit_transform(X)\n","\n","# create PCA feature transformer with 2 components\n","model = None\n","\n","# transform features using PCA\n","X2 = None\n","\n","# create function for plotting 2D two-class dataset\n","def PlotData(X,y):\n","    # plot\n","    plt.plot(X[y==0,0],X[y==0,1],'bo',alpha=0.5, label = bc.target_names[0])\n","    plt.plot(None,None,'r*',alpha=0.5, label = bc.target_names[1])\n","    # annotate\n","    plt.legend()\n","    plt.xlabel('Component 1')\n","    plt.ylabel('Component 2')\n","    plt.title('Wisconsin Breast Cancer Dataset')\n","\n","# Plot reduced dataset\n"]},{"cell_type":"markdown","metadata":{"id":"Tno7oYWVf_QF"},"source":["### K-means clustering\n","\n","Now it's time for some K-means clustering! We'll see how well it can pick out the natural clusters in our dataset and check if these clusters line up with healthy and cancerous cells.\n","\n","**Activity 2:** Ready for some coding? Let's get to it! Complete the code below to perform k-means clustering with 2 clusters. After that, we'll evaluate how accurate the results are compared to the actual labels. Do you think the 30D model will outperform the 2D models we discussed in the lecture? Let's find out!\n","\n","*Note: We need to print out two different scores because the clusters will be assigned labels 0 or 1 randomly. The higher score will be the measure of the performance.*\n","\n","**Answer:**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"EjA1K2uAf_QF"},"outputs":[],"source":["from sklearn.metrics import accuracy_score\n","from sklearn.cluster import KMeans\n","\n","# Create k-means model with 2 clusters\n","model = None\n","\n","# Fit the model and predict the labels\n","y_pred = None\n","\n","# Print out accuracy score compared to ground truth labels\n","print('Accuracy score: ', round(accuracy_score(y,y_pred),2))\n","print('Accuracy score: ', round(accuracy_score(y,1-y_pred),2))"]},{"cell_type":"markdown","metadata":{"id":"SDC4cN5qf_QF"},"source":["We will now plot the clustering result.\n","\n","**Activity 3:** Call the function `PlotData` with the predicted labels. Compare to the ground truth labels plotted above."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_TQqu5H3f_QF"},"outputs":[],"source":["# Plot the clustering result\n"]},{"cell_type":"markdown","metadata":{"id":"FFCHQtxAf_QF"},"source":["## Exercise 1\n","### Clustering high-dimensional dataset using Gaussian Mixture\n","\n","__Task 2.1:__ We're now going to perform clustering on slice2 using GaussianMixture. Set the number of clusters to 3 and the random state to 42. This ensures you'll get the same results every time you run your code. If you're unsure how to do that, don't hesitate to check the documentation for help. Here are the steps you need to follow:\n","\n","- First, you'll need to create the GaussianMixture model.\n","- Then, transform slice2 into a 2D array to create the feature matrix X.\n","\n","Once that's done,\n","\n","* Fit the model and predict the labels\n","* Print out accuracy score compared to ground truth labels\n","* Plot the clustering result\n","\n","Let's dive into it and have some fun!\n","\n","Which method performed better. Can you reason why?\n","\n","**Answer:**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"S_W0md9Rf_QF"},"outputs":[],"source":["from sklearn.mixture import GaussianMixture\n","\n","# Create GMM model with 2 clusters\n","model = None\n","\n","# Fit the model and predict the labels\n","y_pred = None\n","\n","# Print out accuracy score compared to ground truth labels\n","print('Accuracy score: ', round(accuracy_score(y,y_pred),2))\n","print('Accuracy score: ', round(accuracy_score(y,1-y_pred),2))\n","\n","# Plot the clustering result\n"]},{"cell_type":"markdown","metadata":{"id":"1acKogfcf_QG"},"source":["### Load brain MRI\n","Our 2D brain MRI image is saved in a pickle format as `slice.p`. The non-brain tissue has been removed and image has been padded with zeros. When performing GMM clustering to segment the WM, GM and CSF, we will need to exclude the zero values for the algorithm to work well.\n","\n","So, run the following code to load the brain MRI slice and display it. Take a good look at it, because we're going to dig deep into it. Ready? Let's go!"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0ofdzB4af_QG"},"outputs":[],"source":["import pickle\n","slice = pickle.load(open( \"datasets/slice.p\", \"rb\" ))\n","print('Slice dimesions: ',slice.shape)\n","\n","plt.imshow(slice)\n","plt.set_cmap('gray')"]},{"cell_type":"markdown","metadata":{"id":"Ul6rJ8fqf_QG"},"source":["Alright, let's go ahead and create a histogram of our image. You'll notice there's a big spike at zero - that's just the padding in the background of the image.\n","\n","*Note: don't worry about us using **`_=plt.hist`**. We're just doing that to prevent the histogram values from showing up in our output. Let's keep things nice and tidy!*"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"P2ThcuEKf_QG"},"outputs":[],"source":["# display histogram\n","_=plt.hist(slice.flatten(), bins = 100, density = True)"]},{"cell_type":"markdown","metadata":{"id":"qHOCxpjVf_QG"},"source":["To ensure that our GMM segmentation performs well, it's necessary to exclude the padding. That means we'll be focusing on and plotting a histogram of only the non-zero elements. This can be done by using the `np.where` function, but you could also use the logical array `slice>0` if that's more familiar to you.\n","\n","**Note:** Just remember, `slice2`—the array where we've stored the selected pixels—will now be a 1D array.\n","\n","Take a moment to look at the histogram. Can you identify the peaks that represent the white matter (WM), gray matter (GM), and cerebrospinal fluid (CSF)?"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_yubJlh-f_QG"},"outputs":[],"source":["# find indices of non-zero elements\n","ind = np.where(slice>0)\n","# select non-zero elemenst\n","slice2 = slice[ind]\n","# check the dimension\n","print('Shape od selected data is ', slice2.shape)\n","# plot histogram\n","_=plt.hist(slice2, bins = 100, density = True)"]},{"cell_type":"markdown","metadata":{"id":"SgZABqdtf_QG"},"source":["## Exercise 2\n","### Segmentation of brain MRI using Gaussian mixture\n","\n","__Task 2.1:__ Now perform clustering of `slice2` using `GaussianMixture`. Set number of clusters to `3` and random state to `42` to get the same result every time you rerun it. Check help how exactly to do that. Perform following tasks:\n","* Create the `GausianMixture` model\n","* Create the feature matrix `X` by reshaping `slice2` into 2D array\n","* Fit the model and predict the labels\n","* Reshape the predicted labels to the original shape of `slice`\n","* Display using `imshow`"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"SFTIlpUKf_QG"},"outputs":[],"source":["from sklearn.mixture import GaussianMixture\n","\n","# select model\n","model=None\n","\n","# create features - needs to be a 2D array\n","X = None\n","\n","# fit the model and predict the cluster labels\n","y_pred = None\n","\n","# Create array of 2D labels\n","labels2D = np.zeros(slice.shape)\n","\n","# put the labels into fields with non-zero indices\n","# You need to add one so that labels start from 1\n","labels2D[None]=None+1\n","\n","# display the label image\n","\n","plt.set_cmap('plasma')"]},{"cell_type":"markdown","metadata":{"id":"s3ouul9uf_QG"},"source":["**Task 2.2:** Predict the probabilistic segmentations for each tissue class.  Display the maps in a figure with three subplots.\n","\n","To do that perform the following steps:\n","* predict the probabilistic segmentations using function `predict_proba`\n","* check the size of the resulting predicted probability matrix\n","* write a `for` loop over the tissue types\n","* select the probability map for the current class from the predicted probability matrix\n","* create an array of zeros the same shape as `slice`\n","* insert the class-dependent probability into the right locations in this array\n","* display the array using `subplot` and `imshow`"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"LN8ovSGaf_QH"},"outputs":[],"source":["# predict probabilistic segmentations\n","proba = None\n","\n","# check the dimensions\n","print('Dimensions of proba ', None)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_tSnmOu_f_QH"},"outputs":[],"source":["#display\n","plt.figure(figsize = [15,4])\n","plt.set_cmap('gray')\n","\n","for i in range(3):\n","    # take only posteriors for class i\n","    post = None\n","\n","    # reshape to the 3D image\n","    post2D = None\n","    post2D[None]=None\n","\n","    # display\n","    plt.subplot(1,3,i+1)\n","    plt.imshow(post2D)"]},{"cell_type":"markdown","metadata":{"id":"wQ_uF2Bjf_QH"},"source":["## Exercise 3 (optional)\n","\n","### Explore Gaussian Mixture model\n","\n"," In this exercise, we're going to unravel some fascinating theoretical concepts, including **likelihoods** and **posteriors**. Remember the `GaussianMixture` model from Exercise 2? We're going to use it again to perform some brain MRI segmentation magic!\n","\n","### Posterior probabilities\n","\n","Now, let's think about probabilistic segmentation $p_{ik}$. Probabilitic segmentation $p_{ik}$ gives us probability that pixel $i$ to belong the class $k$. This gives us the chance of pixel $i$ belonging to class $k$. But wait, these are actually what we call posterior probabilities! In other words, they're $$p(z_i=k|x_i, \\mu_k, \\sigma_k,c_k)$$ probabilities for the labels $z_i$, given the intensity value $x_i$ and the parameters $\\mu_k, \\sigma_k,c_k$ of the Gaussian intensity distribution for class $k$.\n","\n","**Task 3.1:** ow let's plot how the posterior probability for each class changes with pixel intensity value. Ready to fill in the missing code below and bring those probability curves to life? Here we go!"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"U7qp4tsZf_QH"},"outputs":[],"source":["# pixel intensity value range\n","intensity_range = np.linspace(0, np.max(slice2),200)\n","\n","# predict posterior probabilities for the intensity range\n","# do not forget to reshape the intensity range to 2D array for the prediction\n","proba_curves = None\n","\n","# display\n","plt.figure(figsize = [14,10])\n","# plot normalised histogram\n","# normalisation is achieved by parameter density\n","plt.subplot(211)\n","_=plt.hist(slice2, bins = 100, density = True)\n","plt.title('Normalised Intensity Histogram')\n","\n","# plot posterior probabilities in a for loop\n","plt.subplot(212)\n","for i in range(0,3):\n","    plt.plot(None,None, linewidth = 3, label = 'Class {}'.format(i))\n","\n","# annotate the subplot\n","plt.title('Posterior probabilities')\n","plt.xlabel('intensity')\n","plt.ylabel('posterior probability')\n","plt.legend(loc = 'upper left')"]},{"cell_type":"markdown","metadata":{"id":"3sDhVQo_f_QH"},"source":["### Class-dependent likelihood\n","\n","When we talk about class-dependent likelihoods, we're actually dealing with Gaussian distributions, and these are scaled by the mixing proportions. In mathematical terms, this looks like\n","\n","$$p(x_i|z_i=k,\\mu_k,\\sigma_k, c_k)=G(x_i,\\mu_k,\\sigma_k)c_k$$\n","\n","To visualize these distributions over a normalized histogram (make sure to use `density=True`), we'll need to extract some key parameters from our fitted model. These are `means_`, `covariances_`, and `weights_`.\n","\n","Our next step is to calculate the Gaussian distributions using these parameters. The handy `norm.pdf` function from the `scipy.stats` module is perfect for this. Finally, we need to multiply these distributions by the weights and plot the results.\n","\n","Task 3.2: Time to bring these Gaussian intensity distributions to life! Let's plot them for each class $k$ over the normalized image histogram. Just fill in the missing code below and run the cell to see them in action. Ready to give it a shot? Here we go!\n","\n","\n","---\n","\n","\n","If you want more details:\n","\n"," In this equation\n","\n","$$p(x_i|z_i=k,\\mu_k,\\sigma_k, c_k)=G(x_i,\\mu_k,\\sigma_k)c_k$$\n","\n"," , we are dealing with several parameters related to Gaussian Mixture Models:\n","\n","$x_i$ : This represents the data point or feature that we are considering. In this case, it can represent the intensity of a pixel in an image.\n","\n","$z_i$ : This is the latent (or hidden) variable that denotes the class or cluster to which the data point $x_i$ belongs.\n","\n","$k$ : This is the class or cluster index in the Gaussian Mixture Model. The model tries to determine to which class a data point belongs.\n","\n","$\\mu_k$ : This represents the mean of the Gaussian distribution for class $k$.\n","\n","$\\sigma_k$ : This stands for the standard deviation (or variance) of the Gaussian distribution for class $k$. It determines the width of the Gaussian.\n","\n","$c_k$ : This is the mixing proportion or weight for class $k$, which represents the prior probability of that class. It indicates how much each Gaussian component contributes to the overall model.\n","\n","$G(x_i,\\mu_k,\\sigma_k)$ : This represents the Gaussian distribution for class $k$ with mean $\\mu_k$ and standard deviation $\\sigma_k$.\n","\n","The equation itself represents the conditional probability of the intensity $x_i$ given that it belongs to class $k$, considering the parameters of the Gaussian distribution and its mixing proportion. It's used to model the likelihood of $x_i$ under the given Gaussian distribution parameters."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"APw0m9Rmf_QH"},"outputs":[],"source":["# to calculate gaussian distribution\n","from scipy.stats import norm\n","\n","# get parameters of GMM\n","# use flatten to make 1D arrays\n","\n","# means\n","m = None\n","\n","# standard deviation (you need to take sqrt of covariances)\n","s = None\n","\n","# mixing proportions\n","w = None\n","\n","# display\n","plt.figure(figsize = [14,5])\n","\n","# histogram\n","_=plt.hist(None, bins = 100, None)\n","\n","# class-dependent likelihoods - Gaussian PDFs\n","for i in range(0,3):\n","    likelihood = None\n","    plt.plot(intensity_range,likelihood, linewidth = 3, label = 'Class {}'.format(i))\n","plt.legend()\n","plt.title('Class specific likelihood functions')\n"]},{"cell_type":"markdown","metadata":{"id":"Gft1ylvof_QH"},"source":["### Likelihood\n","\n","The likelihood for each pixel intensity $x_i$ given the Gaussian Mixture Model parameters $\\phi = (\\mu_k,\\sigma_k,c_k),k=1,...,K$ can be evaluated as\n","$$p(x_i|\\phi)=\\sum_{k=1}^KG(x_i,\\mu_k,\\sigma_k)c_k $$\n","\n","We can calculate this function by simply adding together the class-dependent likelihoods. Or, there's an alternative route: we can use the handy `score_samples` function provided by the `GaussianMixture` model, which returns the **log-likelihood**.\n","\n","**Task 3.3:** Now, let's make the likelihood function come alive for the whole intensity range over the normalized image histogram. To do this, first evaluate log-likelihood over the intensity range using the `score_samples` function. Then, calculate the exponential using `np.exp` and plot the result. Excited to see how it looks? Let's get plotting!"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"CO12wV-Yf_QH"},"outputs":[],"source":["# Compare histogram with fitted Gaussian mixture likelihood function\n","plt.figure(figsize = [14,5])\n","# histogram\n","_=plt.hist(None, None, None)\n","# calculate likelihood\n","log_likelihood = None\n","likelihood = np.exp(None)\n","# plot likelihood\n","plt.plot(intensity_range, None, linewidth = 3, c='k')\n","plt.title('Fitted likelihood function')"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.3"},"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":0}