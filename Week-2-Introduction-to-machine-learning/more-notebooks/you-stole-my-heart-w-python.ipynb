{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Heart Failure EDA & Prediction using Logistic Regression, SVM, RandomForest, XGBoost, and Neural Networks with Python \n",
    "  \n",
    "  ## Table of Contents  \n",
    "* [Introduction](#introduction)\n",
    "* [Understanding the Dataset](#understand)\n",
    "* [Exploratory Data Analysis](#EDA)\n",
    "    - [Categorical Data](#categorical)\n",
    "    - [Numerical Data](#numerical)\n",
    "* [Outlier Detection](#outlier)\n",
    "* [Predictive Analysis](#predictive)\n",
    "    - [Data Preprocessing](#preprocessing)\n",
    "    - [Logistic Regression](#lr)\n",
    "    - [Support Vector Machines](#svm)\n",
    "    - [RandomForest Classifier](#rf)\n",
    "    - [XGBoost](#xgb)\n",
    "    - [FeedForward Neural Networks](#nn)\n",
    "* [Verdict](#verdict)\n",
    "\n",
    "\n",
    "**IMPORTANT**  - Some of the code (Gridsearch) have been commented out because it takes a long time to run. You can try those parts by uncommenting the codeblocks.\n",
    "\n",
    "<a id=\"introduction\"></a>  \n",
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Importing the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../input/heart-failure-prediction/heart.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 7\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mseaborn\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01msns\u001b[39;00m\n\u001b[0;32m----> 7\u001b[0m heart \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m../input/heart-failure-prediction/heart.csv\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      8\u001b[0m heart\u001b[38;5;241m.\u001b[39mhead()\n",
      "File \u001b[0;32m/opt/local/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/pandas/io/parsers/readers.py:912\u001b[0m, in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m    899\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[1;32m    900\u001b[0m     dialect,\n\u001b[1;32m    901\u001b[0m     delimiter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    908\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[1;32m    909\u001b[0m )\n\u001b[1;32m    910\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[0;32m--> 912\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/local/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/pandas/io/parsers/readers.py:577\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    574\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[1;32m    576\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[0;32m--> 577\u001b[0m parser \u001b[38;5;241m=\u001b[39m \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    579\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[1;32m    580\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[0;32m/opt/local/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/pandas/io/parsers/readers.py:1407\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1404\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m   1406\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1407\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/local/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/pandas/io/parsers/readers.py:1661\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1659\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[1;32m   1660\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m-> 1661\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1662\u001b[0m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1663\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1664\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1665\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcompression\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1666\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmemory_map\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1667\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1668\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding_errors\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstrict\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1669\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstorage_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1670\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1671\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1672\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[0;32m/opt/local/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/pandas/io/common.py:859\u001b[0m, in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    854\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m    855\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[1;32m    856\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[1;32m    857\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[1;32m    858\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[0;32m--> 859\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m    860\u001b[0m \u001b[43m            \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    861\u001b[0m \u001b[43m            \u001b[49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    862\u001b[0m \u001b[43m            \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    863\u001b[0m \u001b[43m            \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    864\u001b[0m \u001b[43m            \u001b[49m\u001b[43mnewline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    865\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    866\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    867\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[1;32m    868\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../input/heart-failure-prediction/heart.csv'"
     ]
    }
   ],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "heart = pd.read_csv('../input/heart-failure-prediction/heart.csv')\n",
    "heart.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"understand\"></a>  \n",
    "#### Understanding what variables in the dataset means and their types\n",
    "  1. Age - Age of the Patient - **Numerical**\n",
    "  2. Sex - Gender of the Patient - **Categorical**\n",
    "        * M - Male\n",
    "        * F - Female\n",
    "  3. ChestPainType - Floor is made of floor - **Categorical**\n",
    "        * TA - [Typical Angina](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5680106) - Substernal chest pain precipitated by physical exertion or emotional stress\n",
    "        * ATA - [ATypical Angina](https://www.ncbi.nlm.nih.gov/medgen/149267) Angina pectoris which does not have associated classical symptoms of chest pain. Symptoms - weakness, nausea, or sweating\n",
    "        * NAP - [Non-Anginal Chest Pain](https://my.clevelandclinic.org/health/diseases/15851-gerd-non-cardiac-chest-pain) - Pain in the chest that is NOT caused by Heart Disease or Heart Attack\n",
    "        * ASY - [Asymptomatic](https://www.mayoclinic.org/diseases-conditions/heart-attack/expert-answers/silent-heart-attack/faq-20057777) - No symptoms\n",
    "  4. RestingBP - [Resting Blood Pressure (mm/Hg)](https://www.medicinenet.com/blood_pressure_chart_reading_by_age/article.htm) - **Numerical**\n",
    "  5. Cholesterol - [Serum Cholesterol (mm/dl)](https://www.medicalnewstoday.com/articles/321519) - **Numerical**\n",
    "  6. FastingBS - [Fasting Blood Sugar](https://www.mayoclinic.org/diseases-conditions/diabetes/diagnosis-treatment/drc-20371451) - **Categorical (1: if FastingBS > 120 mg/dl, 0: otherwise)**\n",
    "  7. RestingECG - Resting ElectroCardiogram Results - **Categorical**\n",
    "        * Normal - [Normal ECG Reading](https://ecgwaves.com/topic/ecg-normal-p-wave-qrs-complex-st-segment-t-wave-j-point/) \n",
    "        * ST - [Abnormality in ST-T Wave Part of ECG](https://www.healio.com/cardiology/learn-the-heart/ecg-review/ecg-interpretation-tutorial/68-causes-of-t-wave-st-segment-abnormalities) \n",
    "        * LVH - [Probable or definite Left Ventricular hypertrophy](https://www.healio.com/cardiology/learn-the-heart/ecg-review/ecg-interpretation-tutorial/68-causes-of-t-wave-st-segment-abnormalities) \n",
    "  8. MaxHR - Maximum Heart Rate Achieved (60-202) - **Numeric**\n",
    "  9. ExerciseAngina - [Exercise Induced Angina](https://www.mayoclinic.org/diseases-conditions/angina/symptoms-causes/syc-20369373) - When your Heart wants more blood,but narrowed arteries slow down the blood flow - **Categorical (Yes/No)**\n",
    "  10. Oldpeak - [ST Depression](https://en.wikipedia.org/wiki/ST_depression) - **Numerical**\n",
    "  11. ST_Slope - [Slope](https://pubmed.ncbi.nlm.nih.gov/3739881/) of the peak exercise ST Segment - **Categorical**\n",
    "        * Up - Upward Slope\n",
    "        * Flat - Slope is zero\n",
    "        * Down - Downward Slope\n",
    "  12. HeartDisease - Output Class - **Categorical (1: Heart Disease,0: Normal)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Shape of the dataset\n",
    "print(\"Shape of the dataset is: \"+str(heart.shape))\n",
    "#Is there any null values in the data?\n",
    "print(\"Amount of null values in data: \"+ str(heart.isnull().sum().sum()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, there are 918 observations in the dataset. We don't have any missing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check balance of the output variables\n",
    "heart.groupby(['HeartDisease'])['HeartDisease'].count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that there is a slight imbalance in our dataset. We'll use methods such as balancing the dataset by removing excess observations (Results in loss of data, Using performance metrics such as precision, Recall, F1-Score other than accuracy, etc.."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"EDA\"></a>  \n",
    "### Exploratory Data Analysis\n",
    "<a id=\"categorical\"></a>  \n",
    "#### Categorical Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
    "fig.patch.set_facecolor('lightblue')\n",
    "fig.patch.set_alpha(0.3)\n",
    "\n",
    "sns.countplot(x = heart['RestingECG'], data = heart, palette='colorblind', ax=axes[0,0],hue = \"HeartDisease\")\n",
    "sns.countplot(x = heart['ChestPainType'], data = heart, palette='colorblind', ax=axes[0,1],hue = \"HeartDisease\")\n",
    "\n",
    "sns.countplot(x = heart['ExerciseAngina'], data = heart, palette='colorblind', ax=axes[0,2],hue = \"HeartDisease\")\n",
    "sns.countplot(x = heart['FastingBS'], data = heart, palette='colorblind', ax=axes[1,0],hue = \"HeartDisease\")\n",
    "sns.countplot(x = heart['Sex'], data = heart, palette='colorblind', ax=axes[1,1],hue = \"HeartDisease\")\n",
    "sns.countplot(x = heart['ST_Slope'], data = heart, palette='colorblind', ax=axes[1,2],hue = \"HeartDisease\")\n",
    "plt.suptitle('Categorical Variables EDA', fontsize=25)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Decoding these plots can be somewhat tricky.  \n",
    "We had more data for people with heart disease than not. So, small differences between two hues are ignored. After carefully analysing these plots, we can get following insights,\n",
    "* People with ST-T wave abnormalities are more likely to have a heart disease.\n",
    "* There is an alarmingly high amount of patients with Asymptomatic heart diseases.\n",
    "* People who get Exercise induced Angina are more likely to have a heart disease.\n",
    "* People with high fasting blood sugar levels are more likely to have a heart disease.\n",
    "* Men are more likely to get heart diseases than women.\n",
    "* People with flat or down ST_Slope are more likely to have a heart disease."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"numerical\"></a>  \n",
    "#### Numerical Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2, 3, figsize=(17, 10))\n",
    "fig.patch.set_facecolor('lightgreen')\n",
    "fig.patch.set_alpha(0.2)\n",
    "\n",
    "sns.histplot(data = heart,x = heart['Age'],hue = \"HeartDisease\",element=\"poly\",ax=axes[0,0])\n",
    "sns.histplot(data = heart,x = heart['RestingBP'],hue = \"HeartDisease\",element=\"poly\",ax=axes[0,1])\n",
    "sns.histplot(data = heart,x = heart['Cholesterol'],hue = \"HeartDisease\",element=\"poly\",ax=axes[0,2])\n",
    "sns.histplot(data = heart,x = heart['MaxHR'],hue = \"HeartDisease\",element=\"poly\",ax=axes[1,0])\n",
    "sns.histplot(data = heart,x = heart['Oldpeak'],hue = \"HeartDisease\",element=\"poly\",ax=axes[1,1])\n",
    "fig.delaxes(axes[1][2])\n",
    "plt.suptitle('Numerical Variables EDA', fontsize=25)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can get following insights from Numerical variables,\n",
    "* People tend to get more susceptible to heart diseases as they get older.\n",
    "* Resting blood pressures of people with and without heart diseases are kinda similar. Maybe Resting BP is slightly higher for people with heart disease.\n",
    "* **There are unusually high amount of people with zero cholesterol level. This might be a error in data. We have to see about that while considering variables for Machine Learning.**\n",
    "* Maximum heart rate is lower (Highest count is around 120) for most people with heart diseases. Healthy (No heart Disease) people tend to have higher Maximum Heart Rate.\n",
    "* ST wave depression peak value is generally higher for people with heart disease."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation heatmap for Numerical and Binary Variables\n",
    "sns.heatmap(heart.corr(), annot=True, cmap='mako')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"outlier\"></a>  \n",
    "#### Outlier Detection\n",
    "Now, We can try to find Outliers from Numerical Variables and determine whether they should be removed or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "sns.boxplot(x=heart['RestingBP'],ax=axes[0,0])\n",
    "sns.boxplot(x=heart['Cholesterol'],ax=axes[0,1])\n",
    "sns.boxplot(x=heart['MaxHR'],ax=axes[1,0])\n",
    "sns.boxplot(x=heart['Oldpeak'],ax=axes[1,1])\n",
    "plt.suptitle(\"Detecting Outliers\",fontsize = 15)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Resting Blood Pressure**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#It is impossible to have a resting heart bloop pressure of zero.\n",
    "print((heart['RestingBP']==0).sum())\n",
    "\n",
    "#We need to impute this datapoint (not the whole row). I will do this with the median Resting blood pressure\n",
    "for i in heart['RestingBP']:\n",
    "    if i == 0:\n",
    "        heart['RestingBP'] = heart['RestingBP'].replace(i, heart['RestingBP'].median())\n",
    "\n",
    "#Checking if high BP has a correlation with the outcome\n",
    "qHigh = heart['RestingBP'].quantile(0.80)\n",
    "print(\"High Resting BP on Heart Disease: \")\n",
    "print(heart[(heart['RestingBP'] >= qHigh)].value_counts(heart['HeartDisease']))\n",
    "\n",
    "#We can see that there is a increase in heart disease with high RestingBP. So we will not change this outliers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Cholesterol**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Low cholesterol levels seems suspicious. But there are many datapoints.\n",
    "print((heart['Cholesterol']==0).sum())\n",
    "#Check correlation of heart disease with zero cholesterol level.\n",
    "print(\"Zero Cholesterol on Heart Disease: \")\n",
    "print(heart[(heart['Cholesterol'] == 0)].value_counts(heart['HeartDisease']))\n",
    "\n",
    "#BUTTT, WE CAN SEE THAT IT HEART DISEASE IS COMMON WITH VERY LOW CHOLESTEROL LEVELS SO WE will keep the data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Maximum Heart Rate**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#There are some rows with very low maximum heart rates. Lets see how it affects the Heart disease\n",
    "print(\"Low Maximum Heart Rate on Heart Disease: \")\n",
    "print(heart[(heart['MaxHR'] <= 75)].value_counts(heart['HeartDisease']))\n",
    "#WE CAN SEE THAT LOW MaxHR is ASSOCIATED WITH Heart Disease. So we will not touch this datapoints"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**OldPeak**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#There are some rows with very low and very high Old peaks. Lets see how it affects the Heart diseas\n",
    "print(\"Low Oldpeak on Heart Disease: \")\n",
    "print(heart[(heart['Oldpeak'] <= -1)].value_counts(heart['HeartDisease']))\n",
    "\n",
    "print(\"High Oldpeak on Heart Disease: \")\n",
    "print(heart[(heart['Oldpeak'] >= 3.5)].value_counts(heart['HeartDisease']))\n",
    "\n",
    "# We can see that very low and very high old peaks are associated with Heart Disease. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"predictive\"></a>  \n",
    "### Predictive Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"preprocessing\"></a>  \n",
    "#### Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Setting up X and y Variables\n",
    "X = heart.iloc[:,0:-1] # All columns except HeartDisease Columnn\n",
    "y = heart.iloc[:,-1] # HeartDisease Column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#One Hot Encoding using Pandas Get Dummies function\n",
    "X = pd.get_dummies(X, columns=['Sex','ChestPainType','RestingECG','ExerciseAngina','ST_Slope'], \n",
    "                   drop_first=True) #We dont need to encode Fasting BS because it's already in a 0 or 1 format\n",
    "#drop_first = true to avoid dummy variable trap. We are removing one encoded column from every categorical variable\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Splitting data into training and test set\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"lr\"></a>  \n",
    "#### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#For Logistics Regression we dont need to scale data manually\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "LR = LogisticRegression(solver='liblinear',random_state =0)\n",
    "#If you dont include solver='liblinear', you'll get an error for some reason (This doesnt happen when i do it offline)\n",
    "LR.fit(X_train,y_train)\n",
    "y_pred = LR.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Confusion Matrix and Accuracy\n",
    "from sklearn.metrics import accuracy_score, f1_score,classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "cm = confusion_matrix(y_test,y_pred)\n",
    "sns.heatmap(cm, annot=True)\n",
    "accuracy = round(accuracy_score(y_test,y_pred),4)\n",
    "print(\"Accuracy: \",accuracy)\n",
    "f1 = f1_score(y_test,y_pred)\n",
    "print(\"F1 Score:\\n \",f1)\n",
    "report = classification_report(y_test,y_pred)\n",
    "print(\"Classification Report:\\n \",report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Using KFold Cross Validation\n",
    "from sklearn.model_selection import KFold\n",
    "cv = KFold(n_splits=5, random_state=1, shuffle=True)\n",
    "from sklearn.model_selection import cross_validate\n",
    "scores = cross_validate(LogisticRegression(solver='liblinear',random_state =0), \n",
    "                        X, y, cv=cv,scoring = ['accuracy','f1_macro','precision','recall'])\n",
    "\n",
    "print('Accuracy: %.4f (std: %.3f)' % (np.mean(scores['test_accuracy']), np.std(scores['test_accuracy'])))\n",
    "print('F1 Score : %.4f' % (np.mean(scores['test_f1_macro'])))\n",
    "print('Precision : %.4f' % (np.mean(scores['test_precision'])))\n",
    "print('Recall : %.4f' % (np.mean(scores['test_recall'])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"svm\"></a>  \n",
    "#### Support Vector Machines (Classification)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Scaling is needed\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "sc = StandardScaler() \n",
    "'''\n",
    "LOG - #We will only scale numerical X values and concat that with categorical part (Excluding FastingBS im too lazy lol)\n",
    "Turns out this don't work very well..., Scaling all variables equaly performs better (Done below)\n",
    "This vs Equal Scaling Results are as below(F1 Score),\n",
    "SVC (grid search) - 0.8424 vs 0.8533\n",
    "RandomForest (grid_search) - 0.8501 vs 0.8501\n",
    "NN (grid_search) - It changes a lot LMAO\n",
    "X_scaled = pd.concat([pd.DataFrame(sc.fit_transform(X.iloc[:,0:6]),columns = X.columns[0:6]),X.iloc[:,6:]],axis = 1)\n",
    "'''\n",
    "X_scaled = pd.DataFrame(sc.fit_transform(X),columns = X.columns)\n",
    "X_train_scaled, X_test_scaled, y_train, y_test = train_test_split(X_scaled, y, test_size=0.20, random_state=0)\n",
    "X_scaled.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Simple Model\n",
    "from sklearn.svm import SVC\n",
    "svm = SVC(random_state=1)\n",
    "svm.fit(X_train_scaled,y_train)\n",
    "y_pred = svm.predict(X_test_scaled)\n",
    "\n",
    "cm = confusion_matrix(y_test,y_pred)\n",
    "sns.heatmap(cm, annot=True)\n",
    "accuracy = round(accuracy_score(y_test,y_pred),4)\n",
    "print(\"Accuracy: \",accuracy)\n",
    "f1 = f1_score(y_test,y_pred)\n",
    "print(\"F1 Score:\\n \",f1)\n",
    "report = classification_report(y_test,y_pred)\n",
    "print(\"Classification Report:\\n \",report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#It's already better than  Logistic Regression even with stock parameters (C=1,rbf,degree = 3,gamma = scale)\n",
    "\n",
    "#Now I will Optimize Hyperparameters\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "'''\n",
    "params = [ {'C':[1, 10, 100,1000], 'kernel':['linear']},\n",
    "               {'C':[1, 10, 100,1000], 'kernel':['rbf'], 'gamma':[0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9,'scale']},\n",
    "               {'C':[1, 10, 100,1000], 'kernel':['poly'], 'degree': [2,3,4,5] ,'gamma':[0.01,0.02,0.03,0.04,0.05,'scale']},\n",
    "                {'C':[1, 10, 100,1000], 'kernel':['sigmoid']}\n",
    "              ]\n",
    "grid = GridSearchCV(estimator = svm,  \n",
    "                           param_grid = params,\n",
    "                           scoring = 'f1', #F1 because false positives and false negatives are crucial\n",
    "                           cv = 5,\n",
    "                           verbose=0)\n",
    "grid.fit(X_scaled,y)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#y_pred = grid.predict(X_test_scaled)\n",
    "#cm = confusion_matrix(y_test,y_pred)\n",
    "#accuracy = round(accuracy_score(y_test,y_pred),4)\n",
    "#print(\"Accuracy: \",accuracy)# 0.8913\n",
    "#sns.heatmap(cm,annot=True,fmt = 'g')\n",
    "#Getting Best Scores and Best Parameters\n",
    "#print(\"Best Score: \", grid.best_score_) #F1 - 0.8529593322446811 , Accuracy  - 0.8913\n",
    "#print(\"Best Parameters : \", grid.best_params_) #  {'C': 1, 'degree': 5, 'gamma': 'scale', 'kernel': 'poly'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating a model according to best parameters \n",
    "svm = SVC(C = 1,degree = 5,kernel = 'poly',gamma = 'scale',random_state=1)\n",
    "svm.fit(X_train_scaled,y_train)\n",
    "y_pred = svm.predict(X_test_scaled)\n",
    "\n",
    "cm = confusion_matrix(y_test,y_pred)\n",
    "sns.heatmap(cm, annot=True)\n",
    "accuracy = round(accuracy_score(y_test,y_pred),4)\n",
    "print(\"Accuracy: \",accuracy) # Accuracy & F1 we get here is lower than grid search model probably due to less avalable data\n",
    "f1 = f1_score(y_test,y_pred)\n",
    "print(\"F1 Score:\\n \",f1)\n",
    "report = classification_report(y_test,y_pred)\n",
    "print(\"Classification Report:\\n \",report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"rf\"></a>  \n",
    "#### Random Forest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#I will start with Grid Search (Takes so long to run)\n",
    "from sklearn.ensemble import RandomForestClassifier \n",
    "'''\n",
    "RF = RandomForestClassifier(random_state = 0)\n",
    "params = { \n",
    "    'n_estimators': [10,20,30,50,100,200,300,400,500,750,900],\n",
    "    'max_features': ['auto', 'sqrt', 'log2'],\n",
    "    'max_depth' : [4,5,6,7,8,None],\n",
    "    'criterion' :['gini', 'entropy']\n",
    "}\n",
    "grid_RF = GridSearchCV(estimator = RF,  \n",
    "                           param_grid = params,\n",
    "                           scoring = 'f1', #F1 because false positives and false negatives are crucial\n",
    "                           cv = 5,\n",
    "                           verbose=0)\n",
    "grid_RF.fit(X_scaled,y)\n",
    "\n",
    "y_pred = grid_RF.predict(X_test_scaled)\n",
    "cm = confusion_matrix(y_test,y_pred)\n",
    "sns.heatmap(cm,annot=True)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Getting Best Scores and Best Parameters\n",
    "#print(\"Best Score: \", grid_RF.best_score_) #0.8500657209209763\n",
    "#accuracy = round(accuracy_score(y_test,y_pred),4) #0.8478\n",
    "#print(\"Accuracy: \",accuracy)# 0.8913\n",
    "#print(\"Best Parameters : \", grid_RF.best_params_) #  {'criterion': 'entropy', 'max_depth': 4, 'max_features': 'auto', 'n_estimators': 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Creating a model according to best parameters\n",
    "RF = RandomForestClassifier(criterion = 'entropy',max_depth = 4,max_features = 'auto',n_estimators = 20,random_state=0)\n",
    "RF.fit(X_train_scaled,y_train)\n",
    "y_pred = RF.predict(X_test_scaled)\n",
    "\n",
    "cm = confusion_matrix(y_test,y_pred)\n",
    "sns.heatmap(cm, annot=True)\n",
    "accuracy = round(accuracy_score(y_test,y_pred),4)\n",
    "print(\"Accuracy: \",accuracy)\n",
    "f1 = f1_score(y_test,y_pred)\n",
    "print(\"F1 Score:\\n \",f1)\n",
    "report = classification_report(y_test,y_pred)\n",
    "print(\"Classification Report:\\n \",report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"xgb\"></a>  \n",
    "#### XGBoost Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#I will start with Grid Search (Takes so long to run)\n",
    "\n",
    "from xgboost import XGBClassifier\n",
    "'''\n",
    "xgb = XGBClassifier(random_state = 0,verbosity = 0,use_label_encoder = False)\n",
    "#{'booster':['gblinear'],'lambda':[0.01, 0.1, 0.5],'updater':['shotgun','coord_descent'],\n",
    "               # 'feature_selector':['cyclic','shuffle'],'top_k':[0,1,2,3,4,5]},\n",
    "params = [ {'learning_rate':[0.01, 0.1, 0.2,0.3,0.4], 'booster':['gbtree'], 'min_split_loss':[0,0.01,0.1],\n",
    "                   'max_depth':[3,4,5,6,7,8,9]},\n",
    "               {'learning_rate':[0.01, 0.1, 0.2,0.3,0.4], 'booster':['dart'], 'min_split_loss':[0,0.01,0.1],\n",
    "                   'max_depth':[3,4,5,6,7,8,9]}\n",
    "              ]\n",
    "grid_xg = GridSearchCV(estimator = xgb,  \n",
    "                           param_grid = params,\n",
    "                           scoring = 'f1', #F1 because false positives and false negatives are crucial\n",
    "                           cv = 5,\n",
    "                           verbose=0)\n",
    "grid_xg.fit(X_scaled,y)\n",
    "\n",
    "y_pred = grid_xg.predict(X_test_scaled)\n",
    "cm = confusion_matrix(y_test,y_pred)\n",
    "sns.heatmap(cm,annot=True,fmt = 'g')\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Getting Best Scores and Best Parameters\n",
    "#print(\"Best Score: \", grid_xg.best_score_) #0.839683048346517\n",
    "#accuracy = round(accuracy_score(y_test,y_pred),4) #0.9076\n",
    "#print(\"Accuracy: \",accuracy)# 0.9076\n",
    "#print(\"Best Parameters : \", grid_xg.best_params_) #  {'booster': 'gbtree', 'learning_rate': 0.1, 'max_depth': 3, 'min_split_loss': 0}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating a model according to best parameters\n",
    "xgb = XGBClassifier(booster = 'gbtree',learning_rate = 0.1,max_depth = 3,\n",
    "                             min_split_loss = 0,random_state=0,verbosity = 0,use_label_encoder = False)\n",
    "xgb.fit(X_train_scaled,y_train)\n",
    "y_pred = xgb.predict(X_test_scaled)\n",
    "\n",
    "cm = confusion_matrix(y_test,y_pred)\n",
    "sns.heatmap(cm, annot=True)\n",
    "accuracy = round(accuracy_score(y_test,y_pred),4)\n",
    "print(\"Accuracy: \",accuracy)\n",
    "f1 = f1_score(y_test,y_pred)\n",
    "print(\"F1 Score:\\n \",f1)\n",
    "report = classification_report(y_test,y_pred)\n",
    "print(\"Classification Report:\\n \",report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"nn\"></a>  \n",
    "### Feedforward Artificial Neural Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Dropout, BatchNormalization\n",
    "from keras import metrics\n",
    "from tensorflow.keras.wrappers.scikit_learn import KerasClassifier\n",
    "from tensorflow.keras import callbacks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Simple Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Simple Model\n",
    "simpleModel = Sequential(\n",
    "    [\n",
    "        Dense(units = 15, activation=\"relu\",input_dim = 15),\n",
    "        Dense(7, activation=\"relu\"),\n",
    "        Dense(4,activation = 'relu'),\n",
    "        Dense(1,activation = 'sigmoid') #output layer\n",
    "    ]\n",
    ")\n",
    "simpleModel.compile(loss = 'binary_crossentropy',optimizer = 'adam', metrics = ['binary_accuracy'])\n",
    "\n",
    "simpleHistory = simpleModel.fit(X_train_scaled,y_train,epochs = 1000 ,batch_size = 100,verbose = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# summarize history for accuracy\n",
    "plt.plot(simpleHistory.history['binary_accuracy'])\n",
    "#plt.plot(history.history['val_accuracy'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "#plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()\n",
    "# summarize history for loss\n",
    "plt.plot(simpleHistory.history['loss'])\n",
    "#plt.plot(history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "#plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks like we need more regularization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = simpleModel.predict(X_test_scaled)\n",
    "#print(y_pred)\n",
    "y_pred = y_pred>0.1\n",
    "#print(y_pred)\n",
    "cm = confusion_matrix(y_test,y_pred)\n",
    "sns.heatmap(cm, annot=True)\n",
    "accuracy = round(accuracy_score(y_test,y_pred),4)\n",
    "print(\"Accuracy: \",accuracy)\n",
    "f1 = f1_score(y_test,y_pred)\n",
    "print(\"F1 Score:\\n \",f1)\n",
    "report = classification_report(y_test,y_pred)\n",
    "print(\"Classification Report:\\n \",report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Tuned Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Swish](https://medium.com/@neuralnets/swish-activation-function-by-google-53e1ea86f820) Activation function performs slightly better than ReLU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def baseline_model():\n",
    "    model = Sequential(\n",
    "    [\n",
    "        Dense(units = 15, activation='swish',input_dim = 15),\n",
    "        Dense(10, activation=\"swish\"),\n",
    "        Dropout(0.1),\n",
    "        Dense(7,activation = 'swish'),\n",
    "        Dropout(0.2),\n",
    "        Dense(1,activation = 'sigmoid') #output layer\n",
    "    ]\n",
    "    )\n",
    "    model.compile(loss = 'binary_crossentropy',optimizer = 'adam', metrics = metrics.BinaryAccuracy(threshold = 0.6))\n",
    "    return model\n",
    "\n",
    "model  = baseline_model()\n",
    "\n",
    "#Using early stopping to counter overfitting\n",
    "earlystopping = callbacks.EarlyStopping(monitor='val_loss',\n",
    "                                        mode='min',\n",
    "                                        verbose=0,\n",
    "                                        patience=30) #patience is number of epochs to wait after min monitoring variable\n",
    "\n",
    "history = model.fit(X_train_scaled,y_train,validation_data = (X_test_scaled,y_test),\n",
    "                    epochs = 500 ,batch_size = 200,verbose = 0,callbacks =[earlystopping])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# summarize history for accuracy\n",
    "plt.plot(history.history['binary_accuracy'])\n",
    "plt.plot(history.history['val_binary_accuracy'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()\n",
    "# summarize history for loss\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()\n",
    "\n",
    "y_pred = model.predict(X_test_scaled)\n",
    "#print(y_pred)\n",
    "\n",
    "i = 0\n",
    "maxacc = 0;\n",
    "bestthres = 0;\n",
    "while i <1:\n",
    "    y_pred_thres = y_pred>i\n",
    "    accuracy  = accuracy_score(y_test, y_pred_thres)\n",
    "    if maxacc< accuracy:\n",
    "        maxacc = accuracy\n",
    "        bestthres = i;\n",
    "    i=i+0.05\n",
    "print(\"Best Accuracy: \",maxacc,\"at threshold: \",bestthres)\n",
    "\n",
    "y_pred = y_pred>bestthres\n",
    "\n",
    "cm = confusion_matrix(y_test,y_pred)\n",
    "sns.heatmap(cm, annot=True,fmt='g')\n",
    "accuracy = round(accuracy_score(y_test,y_pred),4)\n",
    "print(\"Accuracy: \",accuracy)\n",
    "f1 = f1_score(y_test,y_pred)\n",
    "print(\"F1 Score:\\n \",f1)\n",
    "report = classification_report(y_test,y_pred)\n",
    "print(\"Classification Report:\\n \",report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#K-Fold Cross Validation\n",
    "estimator = KerasClassifier(build_fn=baseline_model, epochs=500, batch_size=200,verbose = 0)\n",
    "\n",
    "kfold = KFold(n_splits=10, shuffle=True)\n",
    "results = cross_validate(estimator, X_scaled, y, cv=kfold,scoring = ['accuracy','f1_macro','precision','recall'],\n",
    "                         fit_params={'callbacks':earlystopping})\n",
    "print('Accuracy: %.4f (std: %.3f)' % (np.mean(results['test_accuracy']), np.std(results['test_accuracy'])))\n",
    "print('F1 Score : %.4f (std: %.3f)' % (np.mean(results['test_f1_macro']) ,np.std(results['test_f1_macro'])))\n",
    "print('Precision : %.4f' % (np.mean(results['test_precision'])))\n",
    "print('Recall : %.4f' % (np.mean(results['test_recall'])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"verdict\"></a>  \n",
    "### Verdict   \n",
    "  \n",
    "  With these results, It's clear that using neural networks might not be the best idea for this dataset. This is probably due to the limited amount of data we have. Most of the models performs pretty similar to each other. XGBoost (Grid Search) acheives the best accuracy but f1 score is not optimal. Feedforward Neural Networks model acheives better F1 Scores."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
