{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"1Zpk_XXGeDjU8g2Ml-E9jY4zOobYuBVUk","timestamp":1687895160832}],"authorship_tag":"ABX9TyO64H66GfbioZYnjJoQGArx"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# Datasets"],"metadata":{"id":"HxXz0k1GZonD"}},{"cell_type":"markdown","source":["- https://www.kaggle.com/paultimothymooney/chest-xray-pneumonia\n","- https://www.kaggle.com/tawsifurrahman/tuberculosis-tb-chest-xray-dataset\n","- https://www.kaggle.com/prashant268/chest-xray-covid19-pneumonia\n","- https://github.com/ieee8023/covid-chestxray-dataset\n","- https://www.kaggle.com/datasets/amanullahasraf/covid19-pneumonia-normal-chest-xray-pa-dataset\n","\n","-selected  https://www.kaggle.com/code/tahsin/chest-x-ray-image-classification\n","\n","- the best: https://www.kaggle.com/code/tahsin/chest-x-ray-image-classification\n","- also the best: Programming Assignment: Chest X-Ray Medical Diagnosis with Deep Learning\n","realted to week 1 of diagnosis in coursera\n","\n","\n","-selected pytorch: https://www.kaggle.com/code/portgasray/covid-19-detection-with-x-ray-covid19-pytorch\n","\n","\n","\n","- https://www.kaggle.com/code/tahsin/chest-x-ray-image-classification\n","\n","- https://www.kaggle.com/search?q=xray+chest+covid19+tutorial\n","\n","- https://www.kaggle.com/code/arunrk7/covid-19-detection-pytorch-tutorial\n","\n","-https://www.kaggle.com/code/portgasray/covid-19-detection-with-x-ray-covid19-pytorch\n","\n","- Coronavirus Detection of Chest X rays using VGG16\n","https://www.kaggle.com/code/ratul6/coronavirus-detection-of-chest-x-rays-using-vgg16\n","\n","- Xray Classification #Keras\n","https://www.kaggle.com/code/adrynh/xray-classification-keras\n","\n"],"metadata":{"id":"HvT0OHocZXZ_"}},{"cell_type":"markdown","source":["PCA and LDA Implementation\n","â€‹\n","Breast Cancer Wisconsin (Diagnostic) Data Set\n","\n","- https://www.kaggle.com/code/gpreda/breast-cancer-prediction-from-cytopathology-data\n","\n","- https://www.kaggle.com/code/gpreda/breast-cancer-prediction-from-cytopathology-data\n","\n","-https://www.kaggle.com/code/berkayalan/principal-component-analysis-pca-complete-guide"],"metadata":{"id":"I1qpgAp9j7NT"}},{"cell_type":"markdown","source":["Let's think about this exponentially growing number of cells as an enormous beehive. To keep every cell buzzing with activity (data), we would need an enormous swarm of bees (or a huge amount of training data). If we don't have enough bees, we end up with empty cells, and it becomes too easy to accidentally draw a line (or a separating hyperplane) that separates our cells without any real pattern. This is when we risk overfitting.\n","\n","So, what we really need is a clever way to shrink our beehive but still keep all the buzzing activity. We want our number of cells (features) to be in line with our swarm size (number of data examples). Now, there are a few ways to do this: we can choose the most active bees (feature selection, which you'll learn about soon) or we can rethink the layout of our beehive (manifold learning), which is what we're going to chat about next."],"metadata":{"id":"cg8dT8PkpwVP"}},{"cell_type":"markdown","source":["In numerous medical scenarios, we often find ourselves dealing with datasets that consist of a small number of samples, but each sample has a lot of features - meaning it's high dimensional. This situation can increase the risk of overfitting. Why? Well, as the dimensionality of the dataset rises, the same amount of data occupies the feature space more thinly. To visualize this, imagine dividing a region of space into equal-sized cubes. The number of these cubes expands rapidly with the growing number of dimensions."],"metadata":{"id":"G9bndWTwp0cA"}},{"cell_type":"markdown","source":["For a fun mental picture, imagine we're dividing a space into lots of regular little blocks, like a giant game of 3D tic-tac-toe. As we add more dimensions to our game, the number of blocks zooms up really fast. That's exactly what happens with our data in higher dimensions!"],"metadata":{"id":"WfiPEnFUp95M"}},{"cell_type":"markdown","source":["Manifold Learning is a set of techniques that simplifies complex data by mapping it onto a simpler space, all while taking advantage of the data's natural shape and structure. Because the real world follows certain rules and limits, the data it produces often have a hidden simplicity: they live on a simple structure, or 'low-dimensional manifold,' hiding within the complexity of a 'high-dimensional feature space.'\n","\n","Manifold Learning is like a team of crafty cartographers who try to draw a simpler, flat map (lower dimensional space) that still captures the twists and turns of a complex, hilly landscape (the data). They're able to do this because real-world data, much like real-world landscapes, have to follow certain laws of nature, which limit how wild they can be. This often results in data, created by the laws of our world, resting on a simple underlying structure (a low dimensional manifold), even though it might seem complicated in the high dimensional space of features.\n","\n","To help understand this better, let's consider a simple toy example. Think of a delicious Swiss roll cake: it's basically a flat cake sheet rolled up into a spiral. Now, imagine each layer of the roll as a dimension in our data. Stay with me, this will get exciting!"],"metadata":{"id":"2zTfTbJWqLGQ"}},{"cell_type":"markdown","source":["Here it is clear that whilst the data lives in three dimensions, the geometry of the data is better represented in two (the result of unrolling the embedded representation). This underlying geometry can be determined by exploiting the local neighbourhood structure of the data. In this way it can be shown that the euclidean distance between two different points on the roll in three dimensions can vastly mispresent the true distance along the underlying manifold."],"metadata":{"id":"qzr-e85XrQpl"}},{"cell_type":"markdown","source":["So, manifold learning techniques are like smart data-mappers, designed to transform the data into a new space that mirrors the data's original shape and structure. This simplifies the process for other learning methods to do their jobs, like clustering or classifying the data, more effectively. There are many ways to do this data-mapping, and they can be grouped into two main categories: linear methods (which simplify the data by giving it a linear makeover) and non-linear methods (that take full advantage of the data's local neighborhood layout). In this session, we'll focus on linear methods, specifically PCA and ICA. But don't worry, we'll cover the more complex non-linear methods like Laplacian Eigenmaps and Spectral clustering next week."],"metadata":{"id":"UAl0kb0QrYix"}}]}