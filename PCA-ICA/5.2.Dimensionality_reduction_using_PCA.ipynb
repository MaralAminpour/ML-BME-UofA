{"cells":[{"cell_type":"markdown","metadata":{"id":"3loVV50XbK64"},"source":["# Dimensionality reduction using Principal component analysis"]},{"cell_type":"markdown","metadata":{"id":"URBVvZotbK66"},"source":["In previous lectures you have seen that if the number of features are too large compared to the number of samples, predictive models can often result in overfitting and do not generalise well for unseen data.\n","\n","Recall the example where we used multivariate linear regression to predict GA of a preterm baby. The model was fitted really well to the training data, but the cross-validation score was rather poor."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2TDjJmc9bK67"},"outputs":[],"source":["# Load data\n","import pandas as pd\n","import numpy as np\n","\n","df = pd.read_csv(\"GA-structure-volumes-preterm.csv\",header=None)\n","structure_volumes = df.values\n","Features = structure_volumes[:,1:] # volumes - we have 86 features and 164 samples\n","Labels = structure_volumes[:,0] # GA - 164\n","\n","# Multivariate linear regression\n","from sklearn.linear_model import LinearRegression\n","from sklearn.model_selection import cross_val_score\n","\n","model = LinearRegression()\n","model.fit(Features,Labels)\n","R2_score = model.score(Features,Labels)\n","cv_score = cross_val_score(model, Features, Labels, cv=5).mean()\n","\n","rmse = np.sqrt(-cross_val_score(model, Features, Labels, cv=5,scoring='neg_mean_squared_error').mean())\n","print(\"R2 score is {} and cross-validated R2 score is {}\".format(round(R2_score,2), round(cv_score,2)))\n","print(\"Cross-validated root mean squared error is {} weeks GA\".format(round(rmse,2)))"]},{"cell_type":"markdown","metadata":{"id":"nEES0LXPbK68"},"source":["We have seen that the overfitting can be reduced and model performance improved by penalising the weights of the regression with ridge or lasso penalty.\n","\n","Now we will now explore an alternative strategy - reducing dimension of the feature vector using principal component analysis."]},{"cell_type":"markdown","metadata":{"id":"z_3y1r_KbK68"},"source":["__Task 1:__ Using sklearn object ```PCA``` estimate and plot the cumulative distribution of ordered eigenvalues (or explained variance for each component).\n","\n","*Hint:* a cumulative sum can be estimated using the ```np.cumsum()``` function.\n","\n","***Note:*** The sklearn ```PCA``` function automatically orders components by eigenvalue (starting with largest). This will not necessarily be true of the functions you build for yourself."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"olHtmYAcbK68"},"outputs":[],"source":["from sklearn.decomposition import PCA\n","import matplotlib.pyplot as plt\n","%matplotlib inline\n","\n","# create PCA model\n","pca = # fill in\n","\n","# fit the data\n","# note that PCA is unsupervised - we use only Features for fitting!\n","# fill in\n","\n","# estimate the cumulative distribution of the eigenvalues\n","cumulative_sum=np.cumsum(pca.explained_variance_ratio_)\n","\n","# plot the distribution\n","plt.plot(cumulative_sum)\n","plt.xticks(np.arange(0,pca.explained_variance_ratio_.shape[0],5))\n","plt.title('cumulative distribution of PCA eigenvalues')\n","plt.ylabel('cumulative sum')\n","plt.ylabel('cumulative sum')\n"]},{"cell_type":"markdown","metadata":{"id":"AOFSQBiObK69"},"source":["__Task 2:__ Now try to determine an appropriate cut off by also plotting lines parallel to the y-axis"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"WlBrEEmybK69"},"outputs":[],"source":["# re-plot the distribution\n","plt.plot(cumulative_sum)\n","plt.xticks(np.arange(0,pca.explained_variance_ratio_.shape[0],5))\n","plt.title('cumulative distribution of PCA eigenvalues')\n","plt.ylabel('cumulative sum')\n","plt.ylabel('cumulative sum')\n","\n","\n","# now select lines parallel to the y-axis\n","# play with different values of cutoff to determine a good one\n","cutoff = None\n","x=np.ones(10)*cutoff\n","y=np.linspace(pca.explained_variance_ratio_[0],1,10)\n","plt.plot(x,y)"]},{"cell_type":"markdown","metadata":{"id":"doGixNVTbK69"},"source":["__Task 3__: Use these to determine an appropriate cut off for PCA dimensionality reduction. Re-run PCA with this reduced set.\n","\n","*Hint:* use parameter ```n_components```"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"THyjGU3ubK69"},"outputs":[],"source":["# create PCA model with number of components equal to the chosen cutoff\n","# fill in this line making pca the name of your model\n","pca=None\n","\n","# now fit the PCA\n","\n","\n","# Transform the original feature vectore to the reduced feature vector\n","newFeatures = pca.transform(Features)\n","\n","# check the size of the newFeatures\n","print('Size of original features: ', Features.shape)\n","print('Size of new features: ', newFeatures.shape)\n"]},{"cell_type":"markdown","metadata":{"id":"bnwuvsNBbK69"},"source":["__Task 4:__ Run the code below to perform linear regression on ```newFeatures``` evaluate performance. How does it compare to the linear regression using all 86 features?"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"LEE4uVuFbK6-"},"outputs":[],"source":["# Perform linear regression using the new features\n","model_new = LinearRegression()\n","model_new.fit(newFeatures,Labels)\n","R2_score_new = model_new.score(newFeatures,Labels)\n","cv_score_new = cross_val_score(model_new, newFeatures, Labels, cv=5).mean()\n","rmse_new = np.sqrt(-cross_val_score(model_new, newFeatures, Labels, cv=5, scoring='neg_mean_squared_error').mean())\n","\n","print(\"New model R2 score is {} and cross-validated R2 score is {}\".format(round(R2_score_new,2), round(cv_score_new,2)))\n","print(\"Cross-validated root mean squared error after using PCA is {} weeks GA\".format(round(rmse_new,2)))"]},{"cell_type":"markdown","metadata":{"id":"mb7z1pVIbK6-"},"source":["We have obtained a very good result. Cross-validated RMSE is in similar range than for penalised linear regression. We can therefore see that dimensionality reduction using PCA prevented the problem of overfitting.\n","\n","__Task 5:__ Change dimensionality of PCA explore the impact on the result."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DOD3JRe5bK6-"},"outputs":[],"source":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.8"},"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":0}