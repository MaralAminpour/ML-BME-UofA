{"cells":[{"cell_type":"markdown","metadata":{"id":"5rcNKpM1wK1h"},"source":["# Independent Component Analysis (ICA)"]},{"cell_type":"markdown","metadata":{"collapsed":true,"id":"MBsb9Zg8wK1j"},"source":["Remember our last session where we dived into the world of PCA? It was all about finding a new orthogonal basis, kind of like twisting and turning our data into a different frame of reference. The goal? To align more accurately with the primary directions of variance in the data.\n","\n","This time around, we're going to tackle Independent Component Analysis, or ICA for short. This method takes a slightly different route. Instead of searching for an orthogonal basis, ICA is on a quest for a basis where the eigenvectors are as independent as possible from one another (maximally independent or minimally correlated), if you will.\n","\n","But before we begin our exploration, let's import the essential modules we'll need for our journey. Excited to learn more about ICA? Let's get started by importing key modules!"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DtkO7EJ8wK1j"},"outputs":[],"source":["import numpy as np\n","import matplotlib.pyplot as plt\n","import IPython\n","from scipy import signal\n","from scipy.io import wavfile\n","\n","%matplotlib inline"]},{"cell_type":"markdown","metadata":{"id":"-oa93HbjwK1k"},"source":["## The Cocktail Party Problem"]},{"cell_type":"markdown","metadata":{"id":"ieh-c7BKwK1k"},"source":["ICA is commonly viewed as a technique for blind source separation. This concept is commonly explained through description of the 'Cocktail Party Problem.'\n","\n","Here, we assume that we record audio from several microphones, placed at different locations, in a room with many different sources of sound i.e. a room of people talking. Each microphone will record a different mixture of sounds. The goal of blind source separation is to recover the original sources.\n","\n","For example, lets pick an example of two different mixtures of audio files.\n","\n","First play audio file one:"]},{"cell_type":"code","execution_count":null,"metadata":{"hideCode":true,"id":"UluKXEpZwK1k"},"outputs":[],"source":["IPython.display.Audio(\"Data/mix1.wav\")"]},{"cell_type":"markdown","metadata":{"id":"sEg2ND5hwK1k"},"source":["Now audio file two:"]},{"cell_type":"code","execution_count":null,"metadata":{"hideCode":true,"id":"u6LRUV8ZwK1k"},"outputs":[],"source":["IPython.display.Audio(\"Data/mix2.wav\")"]},{"cell_type":"markdown","metadata":{"id":"cLz4QQuTwK1k"},"source":["Our goal is to unmix these signals such that we recover the original sources. Picture it as trying to separate mixed paint back into its original colors.  i.e.:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Ak9NYt9TwK1l"},"outputs":[],"source":["IPython.display.Audio(\"Data/source1.wav\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Z87n8xuwwK1l"},"outputs":[],"source":["IPython.display.Audio(\"Data/source2.wav\")"]},{"cell_type":"markdown","metadata":{"id":"xaQ7bDoSwK1l"},"source":["## Defining ICA"]},{"cell_type":"markdown","metadata":{"id":"LY27k-8_wK1l"},"source":["We seek to separate our sources using ICA. ICA is a matrix factorisation approach; it represents a linear model where the original data $\\mathbf{X}$ may be represented as a combination of independent source signals $\\mathbf{S}$ and mixing coefficients $\\mathbf{A}$ such that $\\mathbf{X=AS}$. i.e. for one microphone $m$ the mixed signal received at each point in time is written as:\n","\n","$$ X^m(t)=\\sum_i^n A_i^m S_i(t) $$\n","\n","Here, n represents the number of sources estimated. Thus, for the above example with two source signals, the signal received for mixture 1, at time point 1, is a weighted sum of contributions from each of the sources at the equivalent point in time. The weights are given by the coefficient matrix $\\mathbf{A}$, where each row reflects the coefficients for a different mixture.\n","\n","Our goal through application of ICA is therefore to find the matrix $\\mathbf{A}^{-1}$ that un-mixes the independent source $\\mathbf{S}$:\n","\n","$$ \\mathbf{S}=\\mathbf{A^{-1}X}=\\mathbf{WX}$$\n","\n","Where from now on we refer to the unmixing matrix $\\mathbf{A}^{-1}$ as $\\mathbf{W}$."]},{"cell_type":"markdown","metadata":{"id":"9mm6DQlDwK1l"},"source":["## Solving for ICA"]},{"cell_type":"markdown","metadata":{"id":"Ja_yUdkSwK1l"},"source":["Solutions for ICA assume:\n","- The source signals are independent of each other.\n","- The values in each source signal have non-Gaussian distributions"]},{"cell_type":"markdown","metadata":{"hidePrompt":false,"id":"whHsRPT7wK1l"},"source":["### What do we mean by independence?"]},{"cell_type":"markdown","metadata":{"id":"pi1tq1JIwK1l"},"source":["One way to solve for $\\mathbf{W}$ is to optimise so as to maximise the independence of the recovered source signals. But what do we mean by independent signals?\n","\n","Two events or timeseries A and B are defined as independent if:\n","- Information on the value of A does not give any information on the value of B, and vice versa.\n","- The joint probability equals the product of their probabilities: $P(A\\cap B)=P(A)P(B)$\n","\n","The latter constraint can be rewritten as requiring:\n","\n","$$P(A)=\\frac{P(A\\cap B)}{P(B)} = P(A|B)$$\n","\n","Thus when the joint probability is equal to the product of the marginals the conditional probability of A given B is equal to the marginal probability of A  and thus A is independent of B"]},{"cell_type":"markdown","metadata":{"id":"0EC42gPiwK1l"},"source":["### Why non-gaussian?"]},{"cell_type":"markdown","metadata":{"id":"qkBpqCKowK1m"},"source":["Let's say that our sources come from 2 Gaussian signals with unit variance: $S \\sim  \\mathcal{N}(0,I)$:\n","\n","<img src=\"imgs/signal_with _diagonal_covariance.png\" style=\"max-width:100%; width: 25%; max-width: none\">\n","\n","i.e. S will have a circular distribution centered on origin. In this case $\\mathbf{X=AS}$ is also Gaussian with zero mean and covariance since:\n","\n","$$ E[\\mathbf{XX^T}]= E[\\mathbf{ASS^TA^T}]= \\mathbf{AA^T} $$\n","\n","where we use $\\mathbf{SS^T=I}$ as the S are independent and thus orthogonal.\n","\n","Our goal with ICA is to be able to predict unique sources. However, if the original source signals our Gaussian we can imagine a situation where we propose a new coefficients matrix $\\mathbf{A}$ through a rotation $\\mathbf{R}$ (where by definitation $\\mathbf{RR^T=R^TR=I}$), such that $\\mathbf{A'=AR}$.\n","\n","Thus we observe the effect of a new combination of coefficients: $\\mathbf{A'}$ instead of A $\\mathbf{R}$, which, if we are to be able to distinguish their effect through a new mixture $\\mathbf{X'=A'S}$, we would require that the distribution we observe with $\\mathbf{X'}$ differs from that of $\\mathbf{X}$. Unfortuntely, in the case of Gaussian variables however, this becomes impossible. The distribution of $\\mathbf{X'}$ is also Gaussian with zero mean and unit covariance:\n","\n","$$ E[\\mathbf{X'X'^T}]= E[\\mathbf{A'SS^TA'^T}]= E[\\mathbf{ARSS^TR^TA^T}]= \\mathbf{ARR^TA^T} = \\mathbf{AA^T}$$\n","\n","If we cannot find a unique A then we cannot find a unique W and we cannot recover the original sources. Thus, signals must be non-Gaussian."]},{"cell_type":"markdown","metadata":{"id":"l9V40tmQwK1m"},"source":["## FastICA"]},{"cell_type":"markdown","metadata":{"id":"01AhW0hpwK1m"},"source":["Through using these two assumptions we can solve the ICA problem using a variety of approaches. In this lecture, however, we will focus on FastICA [Hyv2000].\n","\n","This approaches the problem in two stages:\n","- Pre-whitening\n","- Iterative component extraction\n"]},{"cell_type":"markdown","metadata":{"id":"qxqNYMSxwK1m"},"source":["### Pre-whitening (proof is not examinable)"]},{"cell_type":"markdown","metadata":{"id":"NNLljYz4wK1m"},"source":["Pre-weightening is related to PCA and refers to the transformation of the data covariance matrix from $\\mathbf{X}$ to $\\mathbf{Y}$ such that it has zero mean and unit covariance (i.e. $cov(\\mathbf{Y})=\\mathbf{I}$). This is performed so as to transform the required mixing matrix into an orthogonal matrix reducing the number of parameters that needs to be estimated.\n","\n","Whitening is performed by firest demeaning the data matrix s.t. $x_{ij} \\leftarrow x_{ij}-\\frac{1}{M}\\sum_j x_{ij}$. Then data $\\mathbf{X}$ is transformed into $\\mathbf{Y}$ through singular value decomposition of the original covariance matrix $\\mathbf{\\Sigma} = Cov(\\mathbf{X})= \\mathbf{EDE^{-1}}$. Specifically, rewriting:\n","\n","$$\\mathbf{\\Sigma} = \\mathbf{EDE^-1} \\rightarrow \\mathbf{D} = \\mathbf{E^{-1}\\Sigma E} $$\n","\n","We want: $\\mathbf{Y}=\\mathbf{W_wX}$, where $Cov(\\mathbf{Y})=\\mathbf{I}$ and $\\mathbf{W_w}$ is the transformation matrix.\n","\n","From the standard definition that $\\mathbf{D}$ multiplied by its inverse is the identity ($\\mathbf{DD^1=I}$, we can define:\n","\n","$$\\mathbf{D^{-1}D} = \\mathbf{D^{-1}E^{-1}\\Sigma E} \\rightarrow  \\mathbf{I} = \\mathbf{D^{-1}E^{-1}\\Sigma E} $$\n","\n","Rewriting $\\mathbf{D^{-1}}$ as $\\mathbf{D^{-1/2}D^{-1/2}}$ and reordering (this is possible since both $\\mathbf{D}$ and $\\mathbf{D^{1/2}}$ are diagonal matrices:\n","\n","$$ \\mathbf{I} = \\mathbf{D^{-1/2}E^{-1}\\Sigma E D^{-1/2}} $$\n","\n","We need the covariance (rewritten in terms of W and X) to be the identity:\n","\n","$$ Cov(\\mathbf{Y}) = (1/n) \\mathbf{W_wXX^TW_w^T}= \\mathbf{I}$$\n","\n","We know the above expression in $\\mathbf{D}$ is equal to the identity. We can therefore substitute for I:\n","\n","$$  (1/n) \\mathbf{W_wXX^TW_w^T}= \\mathbf{D^{-1/2}E^{-1}\\Sigma E D^{-1/2}} $$\n","\n","Replace $\\mathbf{XX^T}/n$ as $\\mathbf{Sigma}$:\n","\n","$$  (1/n) \\mathbf{W_w\\Sigma W_w^T}= \\mathbf{D^{-1/2}E^{-1}\\Sigma E D^{-1/2}} $$\n","\n","Then, from the symmetry on left and right sides, you can see that this means that $\\mathbf{D^{-1/2}E^{-1}}$ must equal $\\mathbf{W_w}$. Finally we can write as $\\mathbf{W_w=D^{-1/2}E^T}$, since $\\mathbf{E^T}=\\mathbf{E^{-1}}$, since as an eigenvector, it is a orthonormal.\n","\n","Thus, essentially whitening performs the following transformations:\n","\n","<img src=\"imgs/whitening.png\" style=\"max-width:100%; width: 10%; max-width: none\">\n","\n","First multiplication by eigenvectors $\\mathbf{E}$ decorrelate the data (thought of as a rotation that reorients the data so that the principal axes of the data are aligned with the axes along which the data has the largest (orthogonal) variance). Then data variance is squeezed along a dimensions where it is larger than one, and stretched along dimensions where it is less than one.\n","\n","Typically W is also premultiplied by $\\mathbf{E^{-1}}$ in order to rotate data back to its original space. Thus we define:\n","\n","$$\\mathbf{W_w=E^{-1}D^{-1/2}E^T} $$"]},{"cell_type":"markdown","metadata":{"id":"LBiUbnJbwK1m"},"source":["## (optional) Exercise : implement pre-whitening in numpy\n","\n","Given the above definition for the whitening transform $\\mathbf{W_w}$, estimate the whitening transform of a sample data matrix $\\mathbf{X}$"]},{"cell_type":"markdown","metadata":{"id":"8sgtkI5JwK1m"},"source":[" __Task 1__: The code below loads the signals from the mixed audio files demoed above. Run the code and fill the code to visualise the signals."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DMrjq6IhwK1m"},"outputs":[],"source":["# load mixed audio files (sampled above) and use these to define a data matrix X\n","\n","# Load first signal\n","samplingRate, signal1 = wavfile.read('Data/mix1.wav')\n","print(\"Sampling rate = \", samplingRate)\n","print(\"Data type is \", signal1.dtype)\n","\n","# Plot first signal\n","plt.figure(figsize = [15,5])\n","plt.subplot(121)\n","plt.plot(signal1,'k')\n","plt.title('Mix 1')\n","\n","# Load second signal\n","samplingRate, signal2 = wavfile.read('Data/mix2.wav')\n","print(\"Sampling rate = \", samplingRate)\n","print(\"Data type is \", signal2.dtype)\n","\n","# Plot second signal\n","plt.subplot(122)\n","plt.plot(signal2,'k')\n","plt.title('Mix 2')\n","\n","# input both signals into the matrix X\n","X= np.stack((signal1,signal2),axis=0) # mixed signals\n","# X is shape (m x t) where m is the number of mixtures and t is the number of time points\n","print('Shape of the matrix X is', X.shape)\n","\n"]},{"cell_type":"markdown","metadata":{"id":"eRS2cawgwK1m"},"source":["__Task 2:__ Implement estimate the whitening transform from the SVD of the covariance of $\\mathbf{X}$.\n","\n","To do that perform following tasks:\n","1. Calculate covariance matrix $\\mathbf{\\Sigma} = Cov(\\mathbf{X})$. Use function `np.cov`. Verify that the dimension of covariance matrix is $2\\times 2$.\n","2. Perform singular value decomposition $\\mathbf{\\Sigma} = \\mathbf{EDE^{-1}}$ using function `np.linalg.svd` to estimate eigenvectors and eigenvalues of $\\mathbf{\\Sigma}$. Numpy returns the singular values `D` as a vector, we need to convert to a matrix using `np.diag`.\n","3. Estimate whitening matrix $\\mathbf{W_w=E^{-1}D^{-1/2}E^T}$:\n","    * to invert matrix $E$ use `np.linalg.inv`\n","    * to calculate $D^{-1/2}$ use `fractional_matrix_power`\n","    * to multiply three matrices together, use `np.linalg.multi_dot`\n","\n","For further info check:\n","* https://docs.scipy.org/doc/numpy-1.14.0/reference/generated/numpy.linalg.multi_dot.html\n","* https://docs.scipy.org/doc/scipy-0.15.1/reference/generated/scipy.linalg.fractional_matrix_power.html"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"IKooYPS7wK1m"},"outputs":[],"source":["from scipy.linalg import fractional_matrix_power\n","\n","# Step 1: estimate covariance matrix using np.cov\n","covX = # fill in\n","print('Dimension of the covariance matrix: ', covX.shape)\n","\n","# Step 2: estimate eigenvalues and vectors of the covariance matrix using SVD\n","\n","# E [M x M] eigenvectors of cov(X).\n","# S: [M x 1] eigenvalues of cov(X).\n","# E_trans: [M x M] transpose of E\n","E,D,E_trans = # fill in\n","\n","# convert vector D to a diagonal matrix using numpy\n","D_mat = # fill in\n","\n","# Step 3: estimate whitening matrix : E^-1 * D^-0.5 * E^T\n","\n","# Invert matrix E using  (hint see - np.linalg.inv )\n","E_inv = # fill in\n","\n","# Calculate D^-0.5 using  fractional_matrix_power\n","D_invsqrt = # fill in\n","\n","# multiply E^-1 * D^-0.5 * E^T using np.linalg.multi_dot\n","W_w=np.linalg.multi_dot([E_inv, D_invsqrt,E_trans])\n","\n","\n","print('Pre-withening matrix: ')\n","print(W_w)"]},{"cell_type":"markdown","metadata":{"id":"D0d5u8EawK1n"},"source":["__Task 3:__ Transform $X$ using the learnt transform $W_W$ to obtained the new covariance matrix  $\\mathbf{Y}=\\mathbf{W_wX}$. Print the new covariance matrix of the transformed data $\\mathbf{Y}$ to verify that it has mean zero and unit variance: $cov(\\mathbf{Y})=\\mathbf{I}$."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8vfd70HiwK1n"},"outputs":[],"source":["# calculate Y\n","Y=np.dot(W_w,X)\n","print('Dimension of transformed data is ', Y.shape)\n","\n","# Calculate covariance of Y\n","covY = # fill in\n","print('The new covariance matrix is: \\n {} '.format(covY))"]},{"cell_type":"markdown","metadata":{"id":"zZhXhU37wK1n"},"source":["## FastICA optimisation (optional)"]},{"cell_type":"markdown","metadata":{"id":"udnjMnh8wK1n"},"source":["Following whitening the FastICA algorithm proceeds to estimate components through optimising for non-Gaussianity. The motivation for this is the Central Limit Theorem, which states:\n","\n","<center> \"_The distribution of a sum of independent random variables tends toward a gaussian distribution_\" <center>\n","\n","<br>\n","In other words, a mixture of two independent random variables will have a distribution that is closer to Gaussian than that of either of the two original signals.\n","\n","Thus, we can use this to form the basis of an algorithm to separate the components. What we are looking for is for each source to be represented as a linear sum of the mixture components with coefficients given by the inverse of our mixing matrix $\\mathbf{S}=\\mathbf{A^{-1}X}= \\mathbf{WX}$\n","\n","We can try different linear combinations of mixtures $\\mathbf{x}$ (where $\\mathbf{x}$ is one row of $\\mathbf{X}$) e.g.\n","\n","$$\\mathbf{y}= \\mathbf{w^Tx}= \\sum_i w_i x_i $$\n","\n","knowing that when we get the right values for $\\mathbf{w}$ then it will form a row of the un-mixing matrix $\\mathbf{W}$. By re-writing $\\mathbf{y}$ as:\n","\n","$$\\mathbf{y}= \\mathbf{w^Tx}= \\mathbf{w^TAs}= \\mathbf{z^Ts} $$\n","\n","We can see that, in fact, $\\mathbf{y}$ can be shown to be a linear combination of the source signals, $\\mathbf{s}$. $\\mathbf{z^Ts}$ is least Gaussian when only one of the elements of $\\mathbf{z}$ is non-zero i.e. when it is equal to one of the source signals. Therefore, by seeking to maximise non-Gaussianity of $\\mathbf{w^Tx}$ we can recover $\\mathbf{s}$.\n","\n","In FastICA maximal non-Gaussianty is, in fact, achieved by optimising to minimis the negatave (neg) entropy, since:\n","\n","<center> \"_a gaussian variable has the largest entropy among all random variables of equal variance (Cover and Thomas, 1991)_\" <center>\n","    \n","and, estimates of neg-entropy can be shown to be less sensitive to outliers than standard measures of non-gaussianity such as kurtosis. Negative entropy provides a measure of non-gaussianity which is zero for a gaussian variable and always nonnegative. It is defined as:\n","\n","$$J(y)=H(y_{gauss}) - H(y) $$\n","\n","where $y_{gauss}$ is a Gaussian random variable of the same covariance matrix as $y$. Entropy $H$ is defined as:\n","\n","$$ H(Y)=\\sum_i P(Y=a_i) \\log P(Y=a_i) $$\n","\n","Unfortunately, neg-entropy is, in general, very difficult to computationally estimate because it requires an estimate (possibly nonparametric) of the pdf. Thus, for FastICA [Hyv2000], Hyvarinen instead proposed the following approximate form (based on estimates of maximum entropy) where $v$ is a Gaussian variable of zero mean and unit variance, $y$ is assumed to be of zero mean and unit variance, $E$ is the expectation:\n","\n","$$J(y) \\propto [E[G(y)] -E[G(v)]]^2 $$\n","\n","Here, $G$ may be virtually any nonquadratic function, but the most robust estimators are found by choosing a $G$ that does not grow too fast e.g.\n","\n","$$G(u) = -e^{-u^2/2} $$\n","    \n","This leads to the following form of the FastICA algorithm\n","\n","<img src=\"imgs/FastICA_1comp.png\" style=\"max-width:100%; width: 40%; max-width: none\">\n","\n","Here, $g(\\mathbf{w^Tx})$ is the first derivative of $G(\\mathbf{w^Tx})$ and $g'(\\mathbf{w^Tx})$ is the second derivative\n"]},{"cell_type":"markdown","metadata":{"id":"G3y1jxKcwK1n"},"source":["### Deriving the update equation"]},{"cell_type":"markdown","metadata":{"id":"lGGOjD6VwK1n"},"source":["The specific update equation is derived by observing that the function for neg-entropy is maximized for certain optima of the expectation of $G$ ($E[G(\\mathbf{w^Tx})]$), which under the constraints that the variance of approximation ($\\mathbf{w^Tx}$) must equal unity (since the $\\mathbf{x}$ have been whitened):\n","\n","$$E[(\\mathbf{w^Tx}^2)]= ||\\mathbf{w}||^2=1 $$\n","\n","Can be solved for using following equation:\n","\n","$$F(\\mathbf {w})= E[\\mathbf{x}g(\\mathbf{w^Tx})] - \\beta \\mathbf {w} = 0 $$\n","\n","Strictly this is known as a form of ‘kuhn–Tucker' (KT) condition equation, where $\\beta$ here is a KT multiplier (similar to a lagrange multiplier) on the gradient of the constraint equation, which allows optimisation via the derivative of the expection function, under the regularity condition that the magnitude of the variance be 1.\n","\n","We can then solve using Newtons method:\n","\n","$$\\mathbf {w^+}= \\mathbf {w} -F(\\mathbf {w})/JF(\\mathbf {w}) $$   \n","\n","This requires calculation of the Jacobian ($JF(\\mathbf {w})= E[\\mathbf{xx^T}g'(\\mathbf{w^Tx})]$),  which can be simplified using the fact that the data is whitened (and thus $E[\\mathbf{xx^T}]=1$ :\n","\n","$$ E[\\mathbf{xx^T}g'(\\mathbf{w^Tx})] \\approx E[\\mathbf{xx^T}]E[g'(\\mathbf{w^Tx})]=E[g'(\\mathbf{w^Tx})] $$\n","\n","Leading to the following update:\n","\n","$$\\mathbf {w^+}= \\mathbf {w} -\\frac{E[\\mathbf{x}g(\\mathbf{w^Tx})] - \\beta \\mathbf {w}}{E[g'(\\mathbf{w^Tx})]-\\beta} $$   \n","\n","Which through multiply through by $[\\beta -E[g'(\\mathbf{w^Tx})]]$ reduces to:\n","\n","$$\\mathbf {w^+}=E[\\mathbf{x}g(\\mathbf{w^Tx})]-E[g'(\\mathbf{w^Tx})]\\mathbf {w}$$"]},{"cell_type":"markdown","metadata":{"id":"DPtLaNO5wK1n"},"source":["### Moving to more components"]},{"cell_type":"markdown","metadata":{"id":"JlRBIPb9wK1n"},"source":["Finally, for estimation of multiple components (independent, and thus decorrelated from one another), the following update protocol is followed. For the $(p+1)$th component:\n","\n","$$\\mathbf{w}_{p+1}= \\mathbf{w}_{p+1} =\\sum_{j=1}^p \\mathbf{w}_{p+1}^T \\mathbf{w}_i \\mathbf{w}_i \\mathbf{w}_j $$\n","$$\\mathbf{w}_{p+1}= \\frac{\\mathbf{w}_{p+1}}{\\sqrt{\\mathbf{w}_{p+1}^T \\mathbf{w}_{p+1}}} $$\n","\n","\n","This decorrrelates outputs after every iteration by subtracting from $\\mathbf{w}_{p+1}$ the “projections” of the previously estimated $p$ vectors; followed by normalisation of $\\mathbf{w}_{p+1}$"]},{"cell_type":"markdown","metadata":{"id":"PwBK8qbcwK1n"},"source":["# Differences Between ICA and PCA"]},{"cell_type":"markdown","metadata":{"id":"2sjZB4GwwK1n"},"source":["PCA and ICA approach the problem of learning a linear sub-space very differently. Essentially PCA seeks to find a new sub-space that best explains the variance in the data, whereas ICA seeks a sub-space in which the components are maximally independent. In laymans terms PCA is best for dimensionality reduction and ICA is best for separating data."]},{"cell_type":"markdown","metadata":{"id":"E_H2_wCnwK1o"},"source":["## Exercise 2: Blind Source Separation"]},{"cell_type":"markdown","metadata":{"id":"ayAFjtLTwK1t"},"source":["To demonstrate the differences between PCA and ICA we will apply both methods to the task of separating the audio files introduced earlier in the lesson (and loaded up in Exercise 1). Looking at timeseries plots of the data we can see that whilst the mixed signals look qualitively very similar:\n","\n","<img src=\"imgs/mixedsignals.png\" style=\"max-width:100%; width: 50%; max-width: none\">\n","\n","The source signals are quite distinct:\n","\n","<img src=\"imgs/sourcesignals.png\" style=\"max-width:100%; width: 50%; max-width: none\">\n","\n"]},{"cell_type":"markdown","metadata":{"id":"IZSJPFmkwK1u"},"source":["The goal of this exercise is to implement source separation with PCA and ICA using Scikit-Learn, and compare. First, loading all modules we will need:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"LfJi7_SowK1u"},"outputs":[],"source":["from sklearn.decomposition import FastICA, PCA"]},{"cell_type":"markdown","metadata":{"id":"8jeUFRtqwK1u"},"source":["Now performing decomposition using ```FastICA``` and ```PCA``` functions supplied with Scikit Learn. Look at the documentation:\n","* for FastICA http://scikit-learn.org/stable/modules/generated/sklearn.decomposition.FastICA.html\n","* and for PCA  http://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html.\n","\n","__Task 2.1__: Perform fast ICA and PCA decomposition. For both methods:\n","* First create the model, setting `n_components=2` and `whiten=True`\n","* Fit the model to data `X` and transdorm. Both of these steps can be performed using function `fit_transform`\n","\n","*Hint:* using the optional argument ```whiten=True``` in the model constructors will ensure the data is pre-whitened  for you."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6fRq4Tg3wK1u"},"outputs":[],"source":["# Create data matrix X\n","X= np.stack((signal1,signal2),axis=0).T # mixed signals transposed\n","\n","######################## PERFORM DECOMPOSITION ##########################\n","\n","# Create ICA model which will extract two components from the data (apply whitening)\n","ica = # fill in\n","# Fit the ICA model to the data, and project data onto the components\n","S_ = ica.fit_transform(X)  # Reconstruct signals\n","\n","# For comparison, compute PCA\n","pca = # fill in\n","H = pca.fit_transform(X)  # Reconstruct signals based on orthogonal components\n"]},{"cell_type":"markdown","metadata":{"id":"5n-RmZSOwK1u"},"source":["The FastICA offers a variety of options for the function $G(u)$, as well as enabling the option to supply a user defined function. This is controlled using the ```fun``` argument. Here, we use the function defined in the notes through use of ```fun='exp'```.\n","\n","__Optional task:__ See whether you can listen to the output audio files - do they sound well separated? (Uncomment and run the code belowto do that)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"pmenRD02wK1u"},"outputs":[],"source":["# save output\n","# need to make our timeseries a sequence of integers to save it as a .wav properly\n","S_rescaled = np.array((S_ / np.max(np.abs(S_))) * 32767, dtype=np.int16)\n","H_rescaled = np.array((H / np.max(np.abs(H))) * 32767, dtype=np.int16)\n","\n","wavfile.write('Data/unmix_FastICA1.wav',samplingRate,S_rescaled[:,0])\n","wavfile.write('Data/unmix_FastICA2.wav',samplingRate,S_rescaled[:,1])\n","\n","wavfile.write('Data/unmix_PCA1.wav',samplingRate,H_rescaled[:,0])\n","wavfile.write('Data/unmix_PCA2.wav',samplingRate,H_rescaled[:,1])"]},{"cell_type":"markdown","metadata":{"id":"EAbeBUYXwK1u"},"source":["__Task 2.2__: Run the code below to plot the time-series. Which approach does a better job?"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"lKBXsIKFwK1u"},"outputs":[],"source":["# plot ICA components\n","f, (ax1,ax2)=plt.subplots(2,1)\n","\n","ax1.plot(S_[:,0],'k')\n","ax2.plot(S_[:,1],'k')\n","f.suptitle('ICA components')\n","\n","# plot source signals\n","f, (ax1,ax2)=plt.subplots(2,1)\n","\n","ax1.plot(H[:,0],'k')\n","ax2.plot(H[:,1],'k')\n","f.suptitle('PCA components')\n"]},{"cell_type":"markdown","metadata":{"id":"PUk_lo2KwK1u"},"source":["## Biomedical case study: applying ICA to fMRI data"]},{"cell_type":"markdown","metadata":{"id":"Sh2XuhSIwK1u"},"source":["We finish by showing how ICA is most frequently used in biomedical imaging: for identifying patterns of brain activity from functional Magnetic Resonance Imaging (fMRI).\n","\n","Generic MRI generates contrast by taking advantage of the fact that different tissues in the body have differing magnetic properties. This is related to the magnetisation of the water molecules, and reflects the differing proportions of water in different tissues, as well as their varying patterns of diffusion.\n","\n","fMRI goes further to utlise the fact that oxygenated and de-oxygenated blood have different magnetic properties, and oxygenated blood flow in the brain is required to supply energy for neuronal firing. Differences in fMRI contrast due to the influx and uptake of oxygen are referred to ask Blood Oxygenated Level Dependent (BOLD) contrast.\n","\n","An exemplar movie of temporally evolving BOLD contrast on the brain's surface is shown below:"]},{"cell_type":"code","execution_count":null,"metadata":{"hideCode":true,"id":"PKBSKSd9wK1u"},"outputs":[],"source":["import io\n","import base64\n","from IPython.display import HTML\n","\n","\n","HTML(\"\"\"\n","<video width=\"500\" height=\"250\" controls>\n","  <source src=\"imgs/fMRI.mp4\" type=\"video/mp4\">\n","</video>\n","\"\"\")"]},{"cell_type":"markdown","metadata":{"id":"HNFT3ZYBwK1v"},"source":["Here, yellow represents 'active' brain regions and blue 'deactivated' regions. Each point on the brain, therefore, can be represented by a timeseries of brain activity:\n","<br>\n","\n","\n","<img src=\"imgs/brain_and_timeseries.png\" style=\"max-width:100%; width: 30%; max-width: none\">\n","\n","There is a saying in fMRI that 'what wires together, fires together'. In other words we assume that there are a relatively small number of brain regions (below left), that each have a common pattern of brain activity (or temporal signature):\n","\n","<br>\n","\n","<img src=\"imgs/parcellation_and_network.png\" style=\"max-width:100%; width: 50%; max-width: none\">\n","<br>\n","\n","\n","These regions can be found by clustering points with common timeseries. Correlation between the temporal signatures of different regions allows inference of the connectional structure of the brain (above right). In other words we assume that if regions follow similar patterns of activity they must be connected via physical neuronal pathways, which enable them to communicate and coordinate with one another.\n","\n","ICA can be used to find these regions, by separating out clusters of points on the surface with common timeseries.\n","\n","<br>\n","\n","<img src=\"imgs/fMRIICA.png\" style=\"max-width:100%; width: 60%; max-width: none\">\n","\n","<br>\n","\n","In fMRI spatial-ICA is used this means that the algorithm is looking for a series of independent 'spatial maps'. The maps (as shown below) represent regions with common timeseries, where yellow regions represent the cluster(s) of points, identified bvy the algorithm as having a common pattern of activity. Each spatial map is then associated with a timecourse (obtained from the columns of $\\mathbf{A}$):\n","\n","<br>\n","\n","<img src=\"imgs/ICAcomponents.png\" style=\"max-width:100%; width: 60%; max-width: none\">\n","\n","From this, regions and time courses can be used to build simple ’network’ models of the brain. Network models of fMRI imaging data have been shown to characterise different aspects of behaviour, cognition and disease.\n"]},{"cell_type":"markdown","metadata":{"id":"gIwPL5z1wK1v"},"source":["## Summary"]},{"cell_type":"markdown","metadata":{"id":"oTiaQu0uwK1v"},"source":["- PCA decomposition rotates data into a new basis that better reflects sources of variance in the data, where PCA components may be estimated from:\n","  - Eigen-decomposition of the data covariance matrix or\n","  - Singular Value Decomposition\n","- ICA allows un-mixing of signals and is estimated using pre-whitening and FastICA\n","- Both methods allow dimensionality reduction of the data\n","\n","- PCA has applications in:\n"," - biomedical shape modelling\n"," - image classification\n"," - Image denoising\n","- ICA is commonly used to derive brain networks from fMRI\n"]},{"cell_type":"markdown","metadata":{"collapsed":true,"hideCode":true,"hideOutput":false,"id":"U_-LQO3KwK1v"},"source":["## References"]},{"cell_type":"markdown","metadata":{"id":"gzN344L1wK1v"},"source":["[Hyv2000] Hyvärinen, Aapo, and Erkki Oja. \"Independent component analysis: algorithms and applications.\" Neural networks 13.4-5 (2000): 411-430.\n","\n"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.8"},"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":0}