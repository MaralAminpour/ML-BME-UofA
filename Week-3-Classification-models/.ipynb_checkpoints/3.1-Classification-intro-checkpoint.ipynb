{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3a664f09-1cb3-4f9b-b798-d01190827bcd",
   "metadata": {},
   "source": [
    "# Classification\n",
    "\n",
    "This week, we will cover machine learning for classification. First, we will cover some of the basic concepts. Then we will look at some hands-on examples using Scikit-Learn."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32b8259e-265d-46eb-bca0-714156a657a8",
   "metadata": {},
   "source": [
    "## What is classification?\n",
    "\n",
    "For a **classification** machine learning problem, we are interesting in predicting which **class** a sample will be in based on one or more features of the sample. Classes sometimes also termed labels. The goal of our model is to predict which class a sample is in. This is a type of supervised learning where we will train our models using labeled data. Examples of classes include \"healthy\" vs \"disease\"; or \"healthy\" vs \"minor disease\" vs \"serious disease\". If we have exactly two classes, then we have a **binary** classification problem. For a binary classification problem, it is common to designate one class as positive (or label 1) and another class as negative (or label 0). If we have more than two classes, then we have a **multiclass** (or **multilabel**) problem. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93d9f8d5-9125-4788-970a-40e32ae7c9d1",
   "metadata": {},
   "source": [
    "## Prediction of labeling and prediction of confidence\n",
    "\n",
    "The minimum information that our model needs to provide about each sample is the predicted class. It is often also useful for the model to also provide a confidence for the prediction or predicted probabilities for each class."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a79a895c-045e-4dc1-abeb-e4b138db217c",
   "metadata": {},
   "source": [
    "## Confusion matrix\n",
    "\n",
    "One of the most useful visualizations of the predictions of a classification model is the confusion matrix. This will show the number (or fraction) of the predictions that fall into different sets.\n",
    "\n",
    "For binary classification:\n",
    "\n",
    "True positive:\n",
    "True negative:\n",
    "False positive:\n",
    "False negative:\n",
    "\n",
    "[Show images of confusion matrices here.]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cd9a9a1-ae9c-4734-981f-59e3d303a07c",
   "metadata": {},
   "source": [
    "## Scoring classification models\n",
    "\n",
    "There are several scores (or metrics) that are useful for evaluating the perfomance of a classification model, and for comparing different classification models.\n",
    "\n",
    "The simplest score is accuracy.\n",
    "\n",
    "[Define accuracy]\n",
    "[Define accuracy for ]\n",
    "\n",
    "However, there is an issue with accuracy. Often, we want to minimize false positives. Or we may want t\n",
    "\n",
    "[Examples of cases where you would want to minimize false positives or false negatives]\n",
    "\n",
    "In the biomedical world, the following metrics are commonly used for evaluating binary classification models:\n",
    "\n",
    "[Define sensitivity]\n",
    "[Define specificity]\n",
    "\n",
    "In the machine learning world, the following related metrics are more commonly used:\n",
    "\n",
    "[Define recall]\n",
    "[Define precision]\n",
    "[Extend definitions for the multiclass case]\n",
    "\n",
    "Furthermore, the F1-score is combination of recall and precision, and is often used a single score to evaluate models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "250e808e-f78c-450a-bd6b-495a1a4da870",
   "metadata": {},
   "source": [
    "## Receiver-operator-characteristic (ROC) curves\n",
    "\n",
    "Another technique to evaluate binary classification models are receiver-operator characteristics curves.\n",
    "\n",
    "We need a model that returns a confidence score or has an adjustable decision boundary.\n",
    "\n",
    "We plot the true positive rate (or ...) on one axis and the true negative rate (or ...) on the other axis. Then we plot...\n",
    "\n",
    "[Show curve examples]\n",
    "\n",
    "After computing the ROC curve, we can calculate the area under the curve (AUC). The AUC can be used as a metric to compare models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39296d7f-9804-46c8-b16b-3692a04fb492",
   "metadata": {},
   "source": [
    "## Models\n",
    "\n",
    "There are many types of models for classification. Different models will be appropriate for different types of data. The model can be selected by testing the performance of the model on your data, or by experience. In the following notebooks will go through examples using each model.\n",
    "\n",
    "### Perceptron\n",
    "\n",
    "### Logistic Regression Classifier\n",
    "\n",
    "### Support Vector Classifier\n",
    "\n",
    "#### Non-linear SVC with kernels\n",
    "\n",
    "### Decision Tree Classifier\n",
    "\n",
    "### Random Forest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99a64268-dccc-47d6-9908-635cbd24ddc8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e02ee42e-3b1c-43e4-ab30-290412a27136",
   "metadata": {},
   "source": [
    "## Linear Binary Classification\n",
    "\n",
    "<img src=\"imgs/LinearBinaryClassification3.png\">\n",
    "\n",
    "Let's now look in detail how linear binary classification works. The prediction is based on decision function, which is a multivariate linear function.\n",
    "\n",
    "The decision boundary is defined by decision function being equal to zero.\n",
    "\n",
    "If the value of decision function is positive, we will predict label 1. If the value of decision function is negative, we will predict label 0."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83ea4c01-f150-44f5-ab9a-927f5fba00a1",
   "metadata": {},
   "source": [
    "## Perceptron\n",
    "\n",
    "<img src=\"imgs/Perceptron2.png\">\n",
    "\n",
    "The linear perceptron is a simple model for classification. A linear perceptron is simple model that will find a line (or in higher dimensions a plane or hyper-plane) that divides the data into two classes. It can also be used for multiclass problems (see a later notebook).\n",
    "\n",
    "So how can we find the decision function for our dataset? There are various algorithms to do that. We have already introduced Perceptron model before and we will revisit it now.\n",
    "\n",
    "In perceptron model we find the decision function that minimises perceptron\n",
    "criterion. This loss function penalises misclassified samples proportionally to their distance from the decision boundary, which is expressed by the absolute value of the decision function. The perceptron learning algorithm is simple. We will first pick a random sample. If the sample is misclassified we will update the weight vector. The algorithm iterates until convergence. The value eta is the learning rate and is usually set to 1. This algorithm has some disadvantages. It does not always have a unique solution and is not always guaranteed to converge. But it generally works in practice."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4418682b-6dec-4966-90dd-e90634c25d31",
   "metadata": {},
   "source": [
    "## Logistic Regression Classifier\n",
    "\n",
    "The next model we'll look at is the logistic regression classifier. Note that despite the name this is a classification model not a regression model!\n",
    "\n",
    "An advantage of the logistic regression classifier is that the output of the model is a probability.\n",
    "\n",
    "<img src=\"imgs/LogisticRegression.png\">\n",
    "\n",
    "Logistic regression model allows us to do that. It converts the output of the decision function h to probability of the positive class using sigmoid function. This function squashes the output of decision function into rage [0,1].\n",
    "\n",
    "Probability of label 1 given the feature x is therefore sigmoid of h(x), plotted here using the red solid line. The probability of label 0 for the same feature is 1 minus probability of label 1. It is plotted using the blue dotted line.\n",
    "\n",
    "<img src=\"imgs/CrossEntropy2.png\">\n",
    "\n",
    "So how do we fit the logistic regression model? We minimise cross entropy loss. Letâ€™s consider a single sample with index i and see what cross entropy loss means for this sample. The probability pi for this sample is the probability of the class one.\n",
    "\n",
    "If the label for this sample is 1, the penalty will be equal to minus logarithm pi. If pi is one, it will result in zero penalty. If pi is close to zero, it will result in large penalty. The loss function is therefore forcing the probability to 1 for samples with label 1.\n",
    "\n",
    "If the label for this sample is 0, the penalty will be equal to minus logarithm 1\n",
    "pi. If pi is zero, the penalty is zero as well. If pi is close to 1, it will result in large penalty. For samples with label 0 the loss function forces probability to zero as well.\n",
    "\n",
    "We can therefore see that minimisation of cross entropy ensures that probabilities pi are similar to labels yi . The solution is found using numerical methods and in this case, the convergence is guaranteed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ba17139-a8f1-471c-9718-856a5a3d6be2",
   "metadata": {},
   "source": [
    "## Support Vector Classifier \n",
    "\n",
    "Next we'll take a look at the support vector classifier (SVC). This is also often called support vector machine (SVM).\n",
    "\n",
    "<img src=\"imgs/LinearlySeparableDataset.png\" width = \"300\" style=\"float: right;\">\n",
    "\n",
    "- First, let's assume we have a linearly separable dataset\n",
    "- All 3 decision boundaries result in accuracy = 1\n",
    "- Which boundary is likely to generalise well?\n",
    "- The red boundary is most likely to generalise well\n",
    "\n",
    "Linearly separable datasets can be perfectly separated by a linear decision boundary and we can achieve classification accuracy 1. In our example of diagnosis of heart failure, this is the case for healthy patients and patients with severe heart failure.\n",
    "\n",
    "There are many decision boundaries with accuracy 1 for separable datasets. So how do we choose the one that is most likely to generalise well?\n",
    "\n",
    "The red boundary seem to be the best because it is far from the samples unlike the other two.\n",
    "\n",
    "**Large margin classifier (hard margin)**\n",
    "\n",
    "<img src=\"imgs/HardMargin.png\" width = \"400\" style=\"float: right;\">\n",
    "\n",
    "With a large margin classifier, the decision boundary is\n",
    "- as far as possible from the samples\n",
    "- determined by samples on the margins - **support vectors**\n",
    "\n",
    "Support vector classifier is a large margin classifier, which means that it searches for a decision boundary that is as far as possible from the samples.\n",
    "\n",
    "The decision boundary is determined by samples that lie on the margins and are called support vectors, here denoted by pink circles.\n",
    "\n",
    "<img src=\"imgs/SoftMargin.png\">\n",
    "\n",
    "Large margin classifier can be generalised to non-separable datasets by minimising the margin violations. The decision boundary is again determined by support vectors, which lie on or inside the margin or on the wrong side of the decision boundary.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea0dab8c-929d-431d-8959-4c215d03fe94",
   "metadata": {},
   "source": [
    "Now let's look at an example using a support vector classifier\n",
    "\n",
    "# Support vector classification\n",
    "\n",
    "In this notebook we will explore Support Vector Classifier (SVC). Linear binary SVC is very similar to the perceptron and logistic regression in a sense that it finds the optimal hyperplane to separate two classes. These methods, however, have different objectives through which they decide what is the optimal decision boundary.\n",
    "\n",
    "There are three different SVC classifiers in `sklearn` library:\n",
    "1. `LinearSVC` implements linear classifier optimised for performance but does not support the kernel trick\n",
    "2. `SVC` implements SVC with kernel trick. Setting `kernel='linear'` produces the same result as `LinearSVC` but is less efficient in terms of computational time. Setting `kernel='rbf'` produces non-linear classifier with Gaussian kernel.\n",
    "3. `SGDclassifier` implements various classifiers that are optimised using stochastic gradient descent. Its default setting for loss function is `loss='hinge'` which is another implementation of a linear SVC.\n",
    "\n",
    "SVC result also depends on hyperparameter `C` which controls the width of the margin and regularises the decision function. Larger `C` means smaller margin, less regularisation, and closer approximation of hard margin objective. Smaller `C` means larger margin, and smoother boundary for non-linear SVC. Note, that `C` has an opposite role to the parameter `alpha` that we used for penalised regression (e.g. `Ridge`). This is because it multiplies the data term rather than the penalty term."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5daa67c6-4605-4ec3-ae1b-0f4f6f71d6f7",
   "metadata": {},
   "source": [
    "## Extra techniques\n",
    "\n",
    "Finally we will cover examples of some additional techniques that are important for classification problems:\n",
    "\n",
    "1. Encoding classes\n",
    "2. One-hot encoding\n",
    "3. Creating training and test sets\n",
    "4. Handling unbalance datasets"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
