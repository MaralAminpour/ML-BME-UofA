{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9EqIiL3W-jEw"
   },
   "source": [
    "# **Exercise-The Titanic Kaggle challenge machine learning: A case study for classification**\n",
    "\n",
    "Titanic Kaggle Challenge is a competition where you'll use data to predict who could've survived the infamous Titanic disaster.\n",
    "\n",
    "Classification \"survived\" or \"not survived\"\"\n",
    "\n",
    "<img src='https://drive.google.com/uc?export=view&id=1jbTFiD4AHpp7zqom8MWc86PVDglbhsWI' width=500px align=\"center\">\n",
    "\n",
    "Let's explore machine learning with a fun example from Kaggle, a competition site owned by Google. These contests can be for giggles, cash prizes, or even a job offer sometimes! The Titanic Kaggle Challenge is known as one of the classic examples for learning classification in a hands-on way.\n",
    "\n",
    "One beginner's challenge is based on the Titanic disaster, a famous shipwreck that happened on April 15, 1912. The ship, thought to be unsinkable, hit an iceberg and sank, sadly causing 1502 out of 2224 people onboard to lose their lives because there weren't enough lifeboats.\n",
    "\n",
    "But it's not just about guessing who made it. It's about deeply exploring the data, finding patterns, and understanding how different factors might have affected survival rates. It poses questions like 'Did socioeconomic status influence survival rates?' and 'What was the impact of the \"women and children first\" policy? Was the 'women and children first' policy strictly followed?'\n",
    "\n",
    "Here's the interesting part: it seems that some people were more likely to survive than others. The challenge asks us to figure out who these folks were, using data like their names, ages, genders, and social classes.\n",
    "\n",
    "We get a file with details about 891 passengers, including whether they survived or not. We'll use this data to teach our machine to make smart guesses.\n",
    "\n",
    "But the real test comes with another file, this one has information on 418 passengers, but doesn't tell us if they survived. That's where our machine's predictions come in!\n",
    "\n",
    "Suggested tutorial about Kaggle's titanic challange: [Titanic - Machine Learning from Disaster](https://www.kaggle.com/competitions/titanic)\n",
    "\n",
    "You can follow Chris White's tutorial on this subject through this Jupyter notebook: [01-intro-classification.ipynb](https://colab.research.google.com/github/ualberta-rcg/python-machine-learning/blob/main/notebooks/01-intro-classification.ipynb#scrollTo=Gd80ekh_zQu4)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SK1wS_yBCwQn"
   },
   "source": [
    "#**One-hot encoding**\n",
    "\n",
    "<img src='https://drive.google.com/uc?export=view&id=1r-2ngkSjI7aues9ILuih25kby705yzLx' width=500px align=\"center\">\n",
    "\n",
    "Machine learning is a bit like a kid who only likes to play with numbers and not with words. So, when we have categories like **'apple' and 'peer'**, we need to find a way to turn them into numbers.\n",
    "\n",
    "This is where one-hot encoding comes in handy! It's a cool trick that turns these categories into something machine learning algorithms can work with. It's like giving each category its own 'on' and 'off' switch.\n",
    "\n",
    "And guess what? There's an easy way to do this if you're using pandas, a tool in Python. It has a function called 'get_dummies' that does all the work for you.\n",
    "\n",
    "**I thought it would be intresting for you to know**: the term \"one-hot\" in \"one-hot encoding\" comes from the way digital circuits are designed. In digital electronics, a one-hot signal is a group of bits among which the legal combinations of values are only those with a single high bit (1) and all the others low (0)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0LrgglXZG7sz"
   },
   "outputs": [],
   "source": [
    "X = pd.get_dummies(train_df[features])\n",
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JtSPgugeF3ts"
   },
   "source": [
    "The 'get_dummies' function is pretty cool. It only changes the columns that need changing and leaves the ones with numbers just the way they are.\n",
    "\n",
    "By the way, even though **passenger class **is expressed as a number, have you ever wondered if we should treat it as a category instead? Just a thought!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gRksDHhjGkm-"
   },
   "source": [
    "We're now going to split our data into two groups. One group is like our study material that we'll use to teach our machine. The other group is like a pop quiz to see how well the machine learned from the study material. The machine won't have seen this quiz data during its 'study' time.\n",
    "\n",
    "Here's how we do it: we'll use two-thirds of our data for teaching (or 'training') and keep one-third for the quiz (or 'testing'). We want to see if our machine's guesses on the quiz match the actual answers.\n",
    "\n",
    "There's a handy tool in scikit-learn, a library in Python, called 'train_test_split' that does the splitting for us.\n",
    "\n",
    "Here's a little code snippet:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "K6EpaxTjG0F6"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WTE-J5W_Go6R"
   },
   "source": [
    "Let's break down what this does:\n",
    "\n",
    "- X_train is the data we use for training (two-thirds of the total data)\n",
    "- y_train is the actual answers for the training data\n",
    "- X_test is the quiz questions (the remaining one-third of the data)\n",
    "- y_test is the real answers for the quiz. We'll use these to check how well our machine did."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5hnjcYK_HCzZ"
   },
   "source": [
    "# **Our first machine learning model: Decision Tree**\n",
    "\n",
    "Let's kick off our machine learning adventure with a model called a decision tree. Imagine it like a game of 20 questions. The model asks questions about the data and makes decisions based on the answers. And the cool thing? It can change or fine-tune its answers as it gets more information!\n",
    "\n",
    "<img src='https://drive.google.com/uc?export=view&id=1pd3ojKWIOaPmMzKMaZE06MQg1rDv3-g7' width=500px align=\"center\">\n",
    "\n",
    "(Image courtsey: Audrey Fukman and Andy Wright on SFoodie, via Serious Eats)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9Fd3-KLHIuD0"
   },
   "source": [
    "*sklearn.tree.DecisionTreeClassifier*\n",
    "\n",
    "Think of sklearn's DecisionTreeClassifier as a super-smart detective. It checks out your data and figures out all the right questions to ask on its own automatically. We can even tell it how deep we want it to dig into the data with an option called 'max_depth'(basically, how deep do we want the questions to go).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tWcPj40gJxtl"
   },
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "model = DecisionTreeClassifier(max_depth=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Y3jIToX_JzbC"
   },
   "source": [
    "---\n",
    "\n",
    "#**Fit**\n",
    "\n",
    "Most, if not all, models from Scikit-learn have a handy tool called a 'fit' method. It's like a personal trainer for your model, helping it learn from your data features and labels.\n",
    "Think of it as the model's learning buttonâ€”it uses your features and labels to teach the model.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0F0y6uN2LP_O"
   },
   "outputs": [],
   "source": [
    "# fit doesn't modify the model in place\n",
    "# (returns a trained model)\n",
    "\n",
    "model = model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "N5IQKQUxLSZi"
   },
   "source": [
    "---\n",
    "\n",
    "#**Predict**\n",
    "\n",
    "Almost every model in Scikit-learn comes with a neat feature called a 'predict' method. Once your model has been trained, 'predict' lets it make guesses about new data that doesn't have labels.\n",
    "\n",
    "Here we use our **test data** (data that the model hasn't seen before) as an input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BAmhKfgaMRFX"
   },
   "outputs": [],
   "source": [
    "predictions = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8OA5lfN8MSBQ"
   },
   "outputs": [],
   "source": [
    "print(\"The first ten predicted labels for the test data\")\n",
    "print(list(predictions[:10]))\n",
    "print('The ten actual labels')\n",
    "print(list(y_test[:10]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UkOWUj_4Mam9"
   },
   "source": [
    "#**But what did the decision tree do?**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3rcRvHs1Mils"
   },
   "outputs": [],
   "source": [
    "# If you want to install graphviz ....\n",
    "# Note for conda, you may have to install both graphviz and python-graphviz\n",
    "# !pip install graphviz\n",
    "# !conda install graphviz python-graphviz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vqKulYCtMjjz"
   },
   "outputs": [],
   "source": [
    "import graphviz\n",
    "from sklearn.tree import export_graphviz\n",
    "from graphviz import Source\n",
    "\n",
    "dot_data = export_graphviz(model,\n",
    "                           feature_names=X.columns,\n",
    "                           class_names=['Died', 'Survived'],\n",
    "                           filled=True, rounded=True,\n",
    "                           special_characters=True,\n",
    "                           out_file=None)\n",
    "graph = graphviz.Source(dot_data)\n",
    "graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oFlAQ4_PM3sO"
   },
   "source": [
    "#**Measuring the quality of predictions in classification models**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hhkaW33dNPlB"
   },
   "source": [
    "Alright, it's time to get into the world of stats and technical terms! We're going to explore something called a **'confusion matrix**'. This is a handy tool that allows us to compare our model's predictions with the actual results in our test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Sj__jz6uNa6Z"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "confusion_matrix(y_test, predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "y2qJyaXFOAQt"
   },
   "source": [
    "A confusion matrix is a bit like a report card for our predictions. It gives us the numbers for when we got things right and when we got them wrong, in a handy little table:\n",
    "\n",
    "|          | Predicted 0         | Predicted 1         |\n",
    "|----------|---------------------|---------------------|\n",
    "| **Actual 0** | True negative (TN)  | False positive (FP) |\n",
    "| **Actual 1** | False negative (FN) | True positive (TP)  |\n",
    "\n",
    "Predicted Didn't Survive\tPredicted Survived\n",
    "Actually Didn't Survive\tTrue negative (TN)\tFalse positive (FP)\n",
    "Actually Survived\tFalse negative (FN)\tTrue positive (TP)\n",
    "The true negatives and true positives are when our predictions match reality. True negatives are when we said a person wouldn't survive, and they didn't. True positives are when we said a person would survive, and they did.\n",
    "\n",
    "False negatives and false positives are where we slipped up. False negatives are when we said a person wouldn't survive, but they ended up surviving. False positives are when we said a person would survive, but they didn't.\n",
    "\n",
    "We can calculate accuracy as the percentage of times we got it right. It's the number of true negatives and true positives divided by the total number of predictions. We can do this in Scikit-learn using the accuracy_score function. Here's how:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yTLl9nPWOrcL"
   },
   "source": [
    "Consider a team of medical researchers trying to predict whether or not a patient is at risk of heart failure:\n",
    "Just like in our previous examples, accuracy is the proportion of correct predictions. But, as we noted earlier, if heart failures are rare, a model that always predicts \"no heart failure\" might have a high accuracy but it's not really helpful in the medical context. So, it's important to consider more than just accuracy when evaluating such predictions!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DfhfsOogPokr"
   },
   "source": [
    "<img src='https://drive.google.com/uc?export=view&id=1os_zF6e9JpaxsP8Yw-jWTrQCDFUW6TTH' width=500px align=\"center\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JWTD8U77QV0M"
   },
   "source": [
    "Accuracy is basically how often our model gets its guesses right. It's calculated by adding up all the times our model correctly predicted heart failure (true positives) and correctly predicted no heart failure (true negatives), then dividing that by the total number of predictions. In other words,\n",
    "\n",
    "accuracy = (TN + TP) / (TN + TP + FN + FP).\n",
    "\n",
    "In Scikit-learn, we can calculate this using something called 'accuracy_score'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zFQ9GhOyQZ6C"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "accuracy_score(predictions, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Cd3uZlHYQeJC"
   },
   "source": [
    "Accuracy might not always give us the full picture. Let's say we want to predict a rare disease that only affects 1 in every 1,000 people. If we create a model that always predicts that no one has the disease, it will be 99.9% accurate, but it's not very useful, right?\n",
    "\n",
    "The same can be applied to our **Titanic challenge**. If we made a model that simply predicted everyone on board died, it would be 62% accurate. But that model wouldn't tell us much about who actually had a better chance of survival. So, accuracy isn't always the best judge of a model's performance!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RY6Jiy1PQ8rR"
   },
   "source": [
    "Precision and recall might not be as straightforward as accuracy, but in certain cases, they can give us a better understanding of how our model is performing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1rLPsoFrR2X7"
   },
   "source": [
    "Imagine a circle that represents all the patients we predicted would experience heart failure. Now, the left part of that circle represents the patients who actually did experience heart failure.\n",
    "\n",
    "**Precision** is like asking,\n",
    "\n",
    "\"*Out of all the patients we predicted would experience heart failure, how many actually did?*\" We calculate it as:\n",
    "\n",
    "TP / (TP + FP).\n",
    "\n",
    "**Recall**, on the other hand, is like asking,\n",
    "\n",
    "\"*Out of all the patients who actually experienced heart failure, how many did we correctly predict?*\" We calculate it as:\n",
    "\n",
    "TP / (TP + FN).\n",
    "\n",
    "Now, there are times when precision or recall matters more:\n",
    "\n",
    "**High precision** is key when we really want to avoid false positives.\n",
    "\n",
    "- For example, in court trials, we want to be sure that if we declare someone guilty, they really are.\n",
    "\n",
    "- Another example is email spam filters. We don't want to accidentally label important emails as spam.\n",
    "\n",
    "**High recall** is critical when we want to avoid false negatives.\n",
    "\n",
    "- For example, in cancer screenings, a false negative could mean a patient who actually has cancer gets a clean bill of health.\n",
    "\n",
    "Think about it, what do you think is more important for recommendation engines like YouTube, Netflix, or Spotify? Would it be precision or recall?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5cBuMtDFSypQ"
   },
   "source": [
    "The scores for our model ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JLx_iRVsSz4J"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score\n",
    "\n",
    "print('Accuracy: {}'.format(accuracy_score(y_test, predictions)))\n",
    "print('Precision: {}'.format(precision_score(y_test, predictions)))\n",
    "print('Recall: {}'.format(recall_score(y_test, predictions)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oNqRQGZ40PMF"
   },
   "source": [
    "#**Bringing it all together...**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OgijolYOTBaK"
   },
   "source": [
    "We've been working on different pieces in separate parts of this notebook. Let's now bring all of that code together. This way, we can see the big picture of our entire process more clearly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ypqUHCVhTDa4"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score\n",
    "\n",
    "# More about this line shortly ...\n",
    "# np.random.seed(1337)\n",
    "\n",
    "# Load data\n",
    "train_df = pd.read_csv('data/titanic/train.csv')\n",
    "\n",
    "# Choose features and lables\n",
    "features = [\"Pclass\", \"Sex\", \"SibSp\", \"Parch\"]\n",
    "X = pd.get_dummies(train_df[features], drop_first=True)\n",
    "y = train_df['Survived']\n",
    "\n",
    "# Split data into training and test data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33)\n",
    "\n",
    "# Initialize model and fit to training data\n",
    "model = DecisionTreeClassifier(max_depth=3)\n",
    "model = model.fit(X_train, y_train)\n",
    "\n",
    "# Use model to predict on unseen test data\n",
    "predictions = model.predict(X_test)\n",
    "\n",
    "# Evaluate how well the model did\n",
    "print('Accuracy: {}'.format(accuracy_score(y_test, predictions)))\n",
    "print('Precision: {}'.format(precision_score(y_test, predictions)))\n",
    "print('Recall: {}'.format(recall_score(y_test, predictions)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AdSFRCBdTlYG"
   },
   "source": [
    "# **Repeatability ...**\n",
    "\n",
    "Try running the previous section a few times. Notice anything different each time? A lot of machine learning code relies on random number generators, which can make it tricky to get the same results every time.\n",
    "\n",
    "Scikit-learn, the toolkit we're using, relies on a random number generator from a library called NumPy. Fortunately, we can set a starting point for this random number generator (think of it like setting a starting line for the random numbers). This makes sure that we get the same sequence of random numbers every time.\n",
    "\n",
    "See that line in the code that reads: np.random.seed(1337)? If you remove the '#' sign in front of it, you can run the code several times and get the same results each time.\n",
    "\n",
    "Don't worry about the number 1337 - it's just a common choice because in internet slang it stands for 'leet' or 'elite'. Feel free to choose any number that makes you smile! Just remember to use it every time for the same process if you want to get repeatable results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SBSEBCOaUOrB"
   },
   "source": [
    "#**Random forest**\n",
    "**If a tree is good, is a forest better?**\n",
    "\n",
    "<img src='https://drive.google.com/uc?export=view&id=1_eGAlGv9sC4qGGJvnd63PFjjRLF6FoDk' width=500px align=\"center\">\n",
    "\n",
    "<img src='https://drive.google.com/uc?export=view&id=15dNmxx7L9ISn5nQi0Vy72Wq8Gqj5eNbG' width=500px align=\"right\">\n",
    "\n",
    "Think about it this way - if one tree is handy, wouldn't a whole forest be even better?\n",
    "\n",
    "Imagine we could create a bunch of decision trees, each one asking different questions. Then, we could combine their answers to make a final prediction. This is exactly what a Random Forest does.\n",
    "\n",
    "A Random Forest is what we call an 'ensemble model'. It's like a supergroup band made up of lots of individual musicians, all working together to create a harmonious sound. Here, each model contributes to a final, hopefully better, prediction.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0nOIIQgMY_5a"
   },
   "source": [
    "In our toolkit, Scikit-learn, there's a way to use Random Forest. It's called *'sklearn.ensemble.RandomForestClassifier'*.\n",
    "\n",
    "Just like with DecisionTree, we can decide how deep we want the decision trees to go using 'max_depth'.\n",
    "\n",
    "But there's another cool setting we can adjust. It's called 'n_estimators', and it lets us decide how many trees to use in our forest. For example, if we want to use 100 trees and set our max_depth to 3, we'd write:\n",
    "\n",
    "model = RandomForestClassifier(n_estimators=100, max_depth=3). Pretty neat, right?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0QBwyKlSZRgq"
   },
   "source": [
    "#**Exercise: use a RandomForestClassifier**\n",
    "\n",
    "Try your hand at setting up a machine learning pipeline using a RandomForestClassifier model. Follow these steps:\n",
    "\n",
    "- Start by grabbing the **Titanic data** from the file.\n",
    "- Divide the data into two sections: training data and testing data.\n",
    "- Next, fit a random forest model using the training data.\n",
    "- Then, use the test data to make predictions.\n",
    "- Finally, see how your model did by evaluating its performance.\n",
    "\n",
    "Give it a shot! It's a great way to practice what you've learned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gK-lH4fUZqTY"
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Your code goes here ...\n",
    "# Hint: other than a couple of lines of code, it should look\n",
    "#   very much like the decision tree pipeline above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "f366qawiZsj4"
   },
   "outputs": [],
   "source": [
    "# PRINT SOLUTION (copy/paste output into a cell to run)\n",
    "show_solution('titanic-random-forest-pipeline.py')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Y6xI5S5WZ-71"
   },
   "source": [
    "# **Handling missing data**\n",
    "Alright, let's take another peek at the details of our data set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Cvt-WkM2aMGe"
   },
   "outputs": [],
   "source": [
    "train_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5nrXvbT4aMm4"
   },
   "source": [
    "Imagine if we believed that age could really help us predict survival from heart failure. However, we hit a bit of a snag:\n",
    "\n",
    "Some of our records don't include data for age. This is a problem because most machine learning algorithms get stumped when they encounter missing data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jLdPMycpaqB9"
   },
   "source": [
    "Okay, there are a few ways we can tackle this problem:\n",
    "\n",
    "We could simply get rid of any rows with missing data.\n",
    "Alternatively, we could fill in the blanks with a default value. This could be the average value, if available, or something like zero, -1, or 9999.\n",
    "Or, perhaps the absence of a value could be informative in itself? For instance, we could record a one if there's a cabin number for the patient, and a zero otherwise.\n",
    "Now, let's explore how to discard rows and how to replace missing values with the average."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fb34n5g1auf2"
   },
   "source": [
    "# **Removing Rows**\n",
    "\n",
    "Firstly, let's use a handy tool to identify where we have missing (or null) values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "p_Q9y3J8a3Ll"
   },
   "outputs": [],
   "source": [
    "train_df.isnull()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SVxAe4VCa5rM"
   },
   "outputs": [],
   "source": [
    "# Count the missing data in each column\n",
    "train_df.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "soQX2uxmbY-o"
   },
   "source": [
    "Pandas dataframes have this useful feature called 'dropna' that can come to the rescue. It helps us filter out rows that have missing data. We can even specify which columns we want to focus on using a special keyword called 'subset'. The best part? It doesn't mess with our original dataframe. Instead, it gives us a fresh new dataframe, all tidy and missing-data-free!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "55kcTWkJbeP4"
   },
   "outputs": [],
   "source": [
    "age_non_null_train_df = train_df.dropna(subset=['Age'])\n",
    "age_non_null_train_df.info()\n",
    "\n",
    "# If we decide to go further down this road, we might do either:\n",
    "#    train_df.dropna(subset=['Age'], inplace=True)\n",
    "#                or\n",
    "#    train_df = train_df.dropna(subset=['Age'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nfkL0OC1bjcK"
   },
   "source": [
    "# **Replacing missing data with a mean**\n",
    "\n",
    "Here's another approach: Instead of tossing out rows, we can get creative and fill in the missing age values with a \"fictional\" age. Let's calculate the mean age from the available non-null values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zuyFudS-bxpg"
   },
   "outputs": [],
   "source": [
    "train_df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zG6yYCTTb1gY"
   },
   "source": [
    "Pandas Series have a method called fillna that allows us to replace null values in a column with a value of our choice.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WhRKlIZHcHby"
   },
   "outputs": [],
   "source": [
    "# We can make a copy of the dataframe if we don't want to\n",
    "# modify the original (optional)...\n",
    "age_mean_train_df = train_df.copy()\n",
    "# Overwrite the column with new data with the missing data filled\n",
    "age_mean_train_df['Age'] = train_df['Age'].fillna(train_df['Age'].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YZwV7Mb8cJvQ"
   },
   "outputs": [],
   "source": [
    "age_mean_train_df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LPMXx9EscWLF"
   },
   "source": [
    "**Here's a helpful tip:** if you're curious about the age distribution in the data, you can use the 'value_counts' method with the 'bins' option to group the ages into different ranges. This can give you a sense of how the ages are distributed across the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1sAI2oebcZ1p"
   },
   "outputs": [],
   "source": [
    "train_df[\"Age\"].value_counts(bins=10, sort=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f6HRFOaacaez"
   },
   "source": [
    "Alright, here's a fun exercise for you:\n",
    "\n",
    "Let's include the age of the passengers as an additional feature in our machine learning pipeline.\n",
    "- You can choose whether to use the dataset with the null rows thrown out (e.g., train_df = age_non_null_train_df) or the dataset with the missing age data replaced by the mean (e.g., train_df = age_mean_train_df). It's up to you to decide which approach to take.\n",
    "\n",
    "- Next, you can pick any classifier algorithm you like, such as DecisionTreeClassifier or RandomForestClassifier.\n",
    "- Feel free to experiment with different keyword arguments that you think might make a difference. Try changing them and see if they have any effect on the results.\n",
    "\n",
    "It's a great way to explore and understand the impact of different settings on the model's performance. Have fun tinkering with it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gs6G-FP7c6_A"
   },
   "outputs": [],
   "source": [
    "# Your code here ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gMs9uVO3c8_q"
   },
   "outputs": [],
   "source": [
    "# PRINT SOLUTION (copy/paste output into a cell to run)\n",
    "# (one possible solution ...)\n",
    "show_solution('titanic-age-dropna.py')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "P7IOxr_7dKB9"
   },
   "source": [
    "\n",
    "Alright, here's an exciting exercise for you:\n",
    "\n",
    "Let's use the most recently trained model to predict the survival of yourself (or your entire family, if you'd like!).\n",
    "\n",
    "Here's an example to get you started:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zXb3460hdSY-"
   },
   "outputs": [],
   "source": [
    "# You may need to double-check the order of features, it should look like:\n",
    "# [\"Pclass\", \"SibSp\", \"Parch\", \"Age\", \"Sex_male\"]\n",
    "print(X_train.columns)\n",
    "features = X_train.columns\n",
    "\n",
    "family = [\n",
    "  [2, 1, 1, 53.0, 1],  # Me\n",
    "  [2, 1, 1, 52.0, 0],  # Wife\n",
    "  [2, 0, 2, 10.0, 0]   # Daughter\n",
    "]\n",
    "\n",
    "family_df = pd.DataFrame(family, columns=features)\n",
    "model.predict(family_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Fq9S3ATYdTZS"
   },
   "source": [
    "Feel free to modify the 'family' list with the information of your own family members or even predict your own survival individually. The model will predict the survival outcome based on the provided data. Have fun running your predictions!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "M4VKmR-ngZ1P"
   },
   "source": [
    "# Analogy to explain Precision and Recall\n",
    "\n",
    " [Here](https://towardsdatascience.com/precision-and-recall-88a3776c8007) is a great analogy to explain Precision and Recall, and I've paraphrased it below:\n",
    "\n",
    "Think of it like fishing with a net. If you cast a wide net into a lake and catch 80 out of 100 fish, that's 80% recall. However, you also end up with 80 rocks in your net, which means your precision is 50% since half of the net's contents are unwanted junk.\n",
    "\n",
    "On the other hand, you could use a smaller net and focus on a specific area of the lake where there are lots of fish and no rocks. In this case, you might only catch 20 out of the fish, but you'll have zero rocks. This results in 20% recall and 100% precision. *italicized text*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DEhCEUjwjQs-"
   },
   "source": [
    "<img src='https://drive.google.com/uc?export=view&id=1hR6BOW_L-3vXt_VTYqG48SfstZHqcesh' width=300px align=\"right\">\n",
    "\n",
    "<img src='https://drive.google.com/uc?export=view&id=18W2wnGFpAbAnKOI1I5aPrpXjx89L4lFI' width=300px align=\"left\">\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YcbytLTl0hpu"
   },
   "source": [
    "#These are the notebooks for the classification:\n",
    "- Practical Guide to 6 Classification Algorithms\n",
    "link: https://www.kaggle.com/code/faressayah/practical-guide-to-6-classification-algorithms\n",
    "\n",
    "- Basic Machine Learning with Cancer | Kaggle\n",
    "https://www.kaggle.com/code/gargmanish/basic-machine-learning-with-cancer\n",
    "\n",
    "- BreastCancerEDA | Kaggle\n",
    "https://www.kaggle.com/code/cboychinedu/breastcancereda\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
