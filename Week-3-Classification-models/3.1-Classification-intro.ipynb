{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MaralAminpour/ML-BME-UofA/blob/main/Week-3-Classification-models/3.1-Classification-intro.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3a664f09-1cb3-4f9b-b798-d01190827bcd",
      "metadata": {
        "id": "3a664f09-1cb3-4f9b-b798-d01190827bcd"
      },
      "source": [
        "# Classification\n",
        "\n",
        "This week, we will cover machine learning for classification. First, we will cover some of the basic concepts. Then we will look at some hands-on examples using Scikit-Learn."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "32b8259e-265d-46eb-bca0-714156a657a8",
      "metadata": {
        "id": "32b8259e-265d-46eb-bca0-714156a657a8"
      },
      "source": [
        "### What is classification?\n",
        "\n",
        "In a classification task in machine learning, we want to determine the **class or group** a particular **sample ** belongs to based on **one or more of its features**. These groups are sometimes referred to as **labels**. Our model aims to accurately predict the correct label for each sample.\n",
        "\n",
        "We carry out this task through supervised learning, which means that we use a **dataset** where the **correct labels are already known** to **train** our model. For instance, we might have a dataset where samples are labeled either \"**healthy\" or \"disease.**\" In scenarios like this with just two possible labels, we are dealing with a binary classification problem. It is standard practice to represent one class as **positive (or label 1)** and the other as **negative (or label 0)**.\n",
        "\n",
        "When there are more than two potential labels for our samples, we are facing a **multiclass or multilabel classificatio**n problem. For instance, samples could be categorized as **\"healthy**,\" \"**minor disease**,\" or \"**serious disease**.\"\n",
        "\n",
        "**In multiclass problems, each sample is assigned to one and only one label, while in multilabel problems, a sample can be associated with multiple labels. **"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "93d9f8d5-9125-4788-970a-40e32ae7c9d1",
      "metadata": {
        "id": "93d9f8d5-9125-4788-970a-40e32ae7c9d1"
      },
      "source": [
        "### Prediction of labeling and prediction of confidence\n",
        "\n",
        "At the very least, our model should **indicate the predicted class for each sample**. Additionally, it can be beneficial for the model to offer a **level of confidence** in its predictions or to give the **predicted probabilities for each class**."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f4a511d3-20d3-464b-8b6e-2b2c71e61f7a",
      "metadata": {
        "id": "f4a511d3-20d3-464b-8b6e-2b2c71e61f7a"
      },
      "source": [
        "### Confusion matrix\n",
        "\n",
        "Alright, it's time to get into the world of stats and technical terms! We're going to explore something called a 'confusion matrix'. This is a handy tool that allows us to **compare our model's predictions with the actual results in our test data**.\n",
        "\n",
        "The matrix itself can be confusing for people who haven’t seen it before. Maybe that’s **why it was called confusion **matrix. It’s a good idea to extract some understandable data from the matrix, that’s why we have **precision, accuracy, recall, and F1-score.**\n",
        "\n",
        "**First**, we need to **select** one class to be the **\"positive\" class** and the other class to be the **\"negative\" class**. Usually we will use label 0 as the negative class and label 1 and the positive class.\n",
        "\n",
        "A confusion matrix is a bit like a report card for our predictions. It gives us the numbers for when we got things right and when we got them wrong, in a handy little table:\n",
        "\n",
        "<img src=\"https://raw.githubusercontent.com/SirTurtle/ML-BME-UofA-imgs/main/Week-3-Classification-models/imgs/Confusion-Matrix-2.png\" width = \"350\" style=\"float: left;\">\n",
        "\n",
        "<img src='https://raw.githubusercontent.com/SirTurtle/ML-BME-UofA-imgs/main/Week-3-Classification-models/imgs/confusion-matrix.png' width=500px align=\"center\">\n",
        "\n",
        "**True positive (TP)**: The actual and predicted labels are both positive.\n",
        "\n",
        "**True negative (TN)**: The actual and predicted labels are both negative.\n",
        "\n",
        "**False positive (FP)**: The actual label is negative but the predicted label is positive.\n",
        "\n",
        "**False negative (FN)**: The actual label is positive but the predicted label is negative.\n",
        "\n",
        "The **True negative** and **true positives** are when our predictions match reality.\n",
        "\n",
        "**True negatives are when we said a person wouldn't survive, and they didn't.**\n",
        "\n",
        "True positives are when we said a person would survive, and they did.\n",
        "\n",
        "**False negatives and false positives are where we slipped up**. False negatives are when we said a person wouldn't survive, but they ended up surviving. Ooops!\n",
        "\n",
        "False positives are when we said a person would survive, but they didn't. Sorry!\n",
        "\n",
        "**Confusion matrices** can also be used for multiclass classification, not just binary classification."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Classification models metrics\n",
        "\n",
        "There are several scores (or metrics) that are useful for evaluating the perfomance of a classification model, and for comparing different classification models."
      ],
      "metadata": {
        "id": "pC3NYmUexfTv"
      },
      "id": "pC3NYmUexfTv"
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Accuracy\n",
        "\n",
        "<font color=blue>*Accuracy: correct prediction/total number of predictions.*</font>\n",
        "\n",
        "That’s the sum of items in the diagonal divided by the total number of items!\n",
        "\n",
        "**accuracy** = $\\frac{(TP + TN)}{TP + TN + FP + FN}$\n",
        "\n",
        "The simplest score is accuracy. **We can calculate accuracy as the percentage of times we got it right.** It's the number of true negatives and true positives divided by the total number of predictions.\n",
        "\n",
        "Consider a team of medical researchers trying to predict whether or not a patient is at risk of heart failure:\n",
        "Just like in our previous examples, **accuracy is the proportion of correct predictions**.\n",
        "\n",
        "**Issue with accuracy**\n",
        "\n",
        "But, as we noted earlier, if heart failures are **rare**, a model that always predicts \"**no heart failure**\" might have a **high accuracy** but it's not really helpful in the medical context. So, it's important to consider more than just accuracy when evaluating such predictions!\n",
        "\n",
        "**So, there is an issue with accuracy. **Accuracy might not always give us the **full picture**. Let's say we want to predict a **rare disease** that only affects 1 in every 1,000 people. If we create a model that always predicts that no one has the disease, it will be 99.9% accurate, but it's not very useful, right?\n",
        "\n",
        "In the machine learning world, **recall and precision** are more commonly used:\n"
      ],
      "metadata": {
        "id": "JtUKh-F8xT_U"
      },
      "id": "JtUKh-F8xT_U"
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Precision\n",
        "\n",
        "<font color=blue>*Precision: correct positive prediction/total number of positive prediction*</font>\n",
        "\n",
        "Precision only cares about positive class most of the time (Sometimes we have negative predictive value, and it only cares about negative class).\n",
        "\n",
        "**precision** = $\\frac{TP}{TP + FP}$\n",
        "\n",
        "Precision is like asking,\n",
        "\n",
        "\"*Out of all the patients we predicted would experience heart failure, how many actually did?*\"\n",
        "\n",
        "**High precision** is key when we really want to avoid **false positives**.\n",
        "\n",
        "- For example, in court trials, we want to be sure that if we declare someone guilty, they really are.\n",
        "\n",
        "- Another example is email **spam filters**. We don't want to accidentally label important emails as spam."
      ],
      "metadata": {
        "id": "3nwYWqbkxoX6"
      },
      "id": "3nwYWqbkxoX6"
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "<img src=\"https://raw.githubusercontent.com/MaralAminpour/ML-BME-UofA/main/Week-3-Classification-models/imgs/Precision_recall_Representation_1052507280.png\" width = \"350\" style=\"float: left;\">"
      ],
      "metadata": {
        "id": "yh3W4R6Dx40U"
      },
      "id": "yh3W4R6Dx40U"
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "\n",
        "####4. Specificity\n",
        "\n",
        "<font color=blue>Specificity: True Negative predictions / (True Negative predictions + False Positive predictions)</font>\n",
        "\n",
        "**specificity** = **true negative rate** = $\\frac{TN}{TN + FP}$\n",
        "\n",
        "Specificity is a metric used in the context of binary classification in machine learning, which quantifies the ability of the classification model to correctly identify the negative instances from all the actual negative instances available. It's a very important metric, especially in medical diagnosis where we want to be very sure not to falsely identify a condition as being present (positive).\n",
        "\n",
        "We want our test to have a high specificity, which means it should correctly identify as many healthy individuals as possible without labeling them as diseased.\n",
        "\n",
        "\n",
        " If the diagnostic test has a specificity of 99%, indicating that it has a very high accuracy in correctly identifying individuals who do not have the disease, thus avoiding unnecessary distress and further testing for those individuals. This also shows that the test has a very low rate of false alarms, which is very important to prevent overdiagnosis and overtreatment."
      ],
      "metadata": {
        "id": "SMJeIXdMx_LU"
      },
      "id": "SMJeIXdMx_LU"
    },
    {
      "cell_type": "markdown",
      "id": "6cd9a9a1-ae9c-4734-981f-59e3d303a07c",
      "metadata": {
        "id": "6cd9a9a1-ae9c-4734-981f-59e3d303a07c"
      },
      "source": [
        "####3. Recall(sensitivity)\n",
        "\n",
        "<font color=blue>*Recall: correct positive prediction/total number of positive class (in original data)*</font>\n",
        "\n",
        "**recall** = **true positive rate** = $\\frac{TP}{TP + FN}$\n",
        "\n",
        "The recall of the positive class is the same as sensitivity. The recall of the negative class is the same as specificity.\n",
        "\n",
        "Recall, on the other hand, is like asking,\n",
        "\n",
        "\"*Out of all the patients who actually experienced heart failure, how many did we correctly predict?*\"\n",
        "\n",
        "**High recall** is critical when we want to avoid **false negatives**.\n",
        "\n",
        "- For example, in cancer screenings, a false negative could mean a patient who actually has cancer gets a clean bill of health.\n",
        "\n",
        "Think about it, what do you think is more important for recommendation engines like YouTube, Netflix, or Spotify? Would it be precision or recall?"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 5. $F_1$ score\n",
        "\n",
        "Furthermore, the **$F_1$ score is a combination of recall and precision**, and is often used a single score to evaluate models.\n",
        "\n",
        "$F_1 = 2 \\frac{\\text{precision} \\cdot \\text{recall}}{\\text{precision} + \\text{recall}}$\n",
        "\n",
        "Imagine a circle that represents all the patients we predicted would experience heart failure. Now, the left part of that circle represents the patients who actually did experience heart failure."
      ],
      "metadata": {
        "id": "6gBmvFdEyHr_"
      },
      "id": "6gBmvFdEyHr_"
    },
    {
      "cell_type": "markdown",
      "id": "036b6281-b407-40d9-8018-57c0e7e09417",
      "metadata": {
        "id": "036b6281-b407-40d9-8018-57c0e7e09417"
      },
      "source": [
        "#### Analogy to explain precision and recall\n",
        "\n",
        "[Here](https://towardsdatascience.com/precision-and-recall-88a3776c8007) is a great analogy to explain Precision and Recall, and I tried to summarize it below:\n",
        "\n",
        "**Fishing with a net**\n",
        "\n",
        "Think of it like fishing with a net. If you cast a **wide net** into a lake and **catch 80 out of 100 fish**, that's 80% **recall**. However, you also end up with 80 rocks in your net, which means your **precision is 50%** since half of the net's contents are unwanted junk.\n",
        "\n",
        "On the other hand, you could use a **smaller net** and focus on a **specific area of the lake** where there are lots of fish and no rocks. In this case, you might only catch 20 out of 100 the fish, but you'll have zero rocks. This results in **20% recall and 100% precision**.\n",
        "\n",
        "<img src='https://raw.githubusercontent.com/SirTurtle/ML-BME-UofA-imgs/main/Week-3-Classification-models/imgs/precision_recall.gif' width=400px align=\"right\">\n",
        "\n",
        "<img src='https://raw.githubusercontent.com/SirTurtle/ML-BME-UofA-imgs/main/Week-3-Classification-models/imgs/recall.gif' width=400px align=\"left\">"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "250e808e-f78c-450a-bd6b-495a1a4da870",
      "metadata": {
        "id": "250e808e-f78c-450a-bd6b-495a1a4da870"
      },
      "source": [
        "### Receiver-operator-characteristic (ROC) curves\n",
        "\n",
        "ROC curves are a handy tool to assess the **performance** of binary classification models, helping us visualize how well our model is doing.\n",
        "\n",
        "Before focusing on ROC curves, ensure your model can provide **confidence scores** or allows for tuning of the **decision threshold**.\n",
        "\n",
        "<img src=\"https://raw.githubusercontent.com/SirTurtle/ML-BME-UofA-imgs/main/Week-3-Classification-models/imgs/roc-curve.png\" width = \"400\" style=\"float: right;\">\n",
        "\n",
        "In an ROC curve, we plot the true positive rate (sensitivity) against the false positive rate (1-specificity) to see how they trade-off.\n",
        "\n",
        "Subsequently, we determine the area under the curve (AUC), which serves as a valuable metric to compare different models.\n",
        "\n",
        "With this metric, with a **larger AUC** are **ranked higher **than models with a **lower AUC**. **This assumes we are interested in models with a balance between sensitivity and specificity.**\n",
        "\n",
        "Generally, a model with a higher AUC is favored as it indicates a good balance between sensitivity and specificity, showcasing a model’s ability to maintain a healthy rate of true positives while minimizing false positives. Remember, we are aiming for a larger AUC to ensure the best blend of sensitivity and specificity!"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "39296d7f-9804-46c8-b16b-3692a04fb492",
      "metadata": {
        "id": "39296d7f-9804-46c8-b16b-3692a04fb492"
      },
      "source": [
        "## Classification Models\n",
        "\n",
        "Classification tasks can be approached using a variety of models, each suitable for different kinds of data. The ideal approach to selecting a model is by evaluating its performance using your specific dataset or drawing from past experiences. In the subsequent notebooks, we will delve into examples utilizing various models to give you a comprehensive understanding."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Linear Binary Classification\n",
        "\n",
        "Linear binary classification is a type of classification algorithm that is used to separate data into two classes (hence \"binary\") based on a linear combination of the features of the data (hence \"linear\"). In other words, it is a method that uses a linear equation to categorize data into one of two groups.\n",
        "\n",
        "\n",
        "<img src=\"https://raw.githubusercontent.com/MaralAminpour/ML-BME-Course-UofA-Fall-2023/main/Week-3-Classification-models/imgs/decision_boundry2.png\" width = \"500\">"
      ],
      "metadata": {
        "id": "LhYYv59ckFHX"
      },
      "id": "LhYYv59ckFHX"
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "#### Decision function\n",
        "\n",
        "In the context of linear binary classification, the linear decision function is a mathematical equation that helps in distinguishing between two classes based on a set of features or inputs. The linear decision function is represented using a linear combination of the input features and a set of weights, alongside a bias term.\n",
        "\n",
        "In otherwords, the decision function is a linear function that takes a vector of input features and returns a scalar value that is used to decide the class of the input. It is represented mathematically as follows:\n",
        "\n",
        "$$\n",
        "f(\\mathbf{x}) = \\mathbf{w}^T \\mathbf{x} + b\n",
        "$$\n",
        "\n",
        "or\n",
        "\n",
        "$$\n",
        "f(\\mathbf{x}) = w_1 x_1 + w_2 x_2 + \\ldots + w_n x_n + b\n",
        "$$\n",
        "\n",
        "We have the feature vector, \\(\\mathbf{x}\\), and the weight vector, \\(\\mathbf{w}\\), defined as:\n",
        "\n",
        "$$\n",
        "\\mathbf{x} = (1, x_1, x_2, \\ldots, x_N)^T\n",
        "$$\n",
        "\n",
        "$$\n",
        "\\mathbf{w} = (w_0, w_1, w_2, \\ldots, w_N)^T\n",
        "$$\n",
        "\n",
        "where\n",
        "\n",
        "- \\(f(\\mathbf{x})\\) is the decision function that we want to find.\n",
        "- \\(w_1, w_2, \\ldots, w_n\\) are the weights assigned to the respective features\n",
        "- \\(x_1, x_2, \\ldots, x_n\\) represent the feature values\n",
        "- \\(b\\) is the bias term, helping to shift the decision boundary away from the origin."
      ],
      "metadata": {
        "id": "UEm3v3PqkGZw"
      },
      "id": "UEm3v3PqkGZw"
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Classifying the data points\n",
        "\n",
        "The linear decision function plays a pivotal role in:**Classifying the data points**:\n",
        "\n",
        "Depending on the sign of $f(\\mathbf{x})$, the data point $\\mathbf{x}$ is classified into one of the two classes. Typically:\n",
        "   - If $f(\\mathbf{x}) > 0$, we predict the label as 1.\n",
        "   - If $f(\\mathbf{x}) \\leq 0$, we predict the label as 0.\n"
      ],
      "metadata": {
        "id": "iytusBB7pIkc"
      },
      "id": "iytusBB7pIkc"
    },
    {
      "cell_type": "markdown",
      "id": "e02ee42e-3b1c-43e4-ab30-290412a27136",
      "metadata": {
        "id": "e02ee42e-3b1c-43e4-ab30-290412a27136"
      },
      "source": [
        "#### Decision Boundary\n",
        "\n",
        "The decision boundary is a hypersurface that separates the feature space into regions, each corresponding to a different class label. For a two-dimensional feature space, this boundary is a line, while in a three-dimensional space, it is a plane, and so on. It is established based on the decision function, typically where the decision function equals zero:\n",
        "\n",
        "$$ f(\\mathbf{x}) = 0 $$.\n",
        "\n",
        "As we said, it is a hypersurface that partitions the feature space into regions corresponding to the different classes.\n",
        "\n",
        "Once a decision boundary is established using the training data, it can be used to classify new, unseen data points into one of the classes based on which side of the boundary they fall on.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Finding the Optimal Decision Function**\n",
        "\n",
        "In the learning phase, the goal is to find the optimal weights and bias term that minimizes the errors in classification. This is often done using algorithms like the Perceptron Learning Algorithm, which iteratively updates the weights based on the misclassified samples in each iteration to find a decision function that best separates the two classes.\n",
        "\n"
      ],
      "metadata": {
        "id": "TUwg2HY1grPW"
      },
      "id": "TUwg2HY1grPW"
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Perceptron Criterion**\n",
        "\n",
        "When working with a perceptron, the objective is to minimize the perceptron criterion, which penalizes misclassified samples proportionally to their distance from the decision boundary. This can be formulated mathematically as:\n",
        "\n",
        "$$\n",
        "\\hat{w} = \\arg\\min_w \\sum_{\\xi_i \\in M} |\\mathbf{w}^T \\xi_i|\n",
        "$$\n",
        "\n",
        "Where:\n",
        "- $\\hat{w}$: is the weight vector that minimizes the criterion.\n",
        "- $\\mathbf{w}$: is the weight vector.\n",
        "- $\\xi_i$: represents individual samples in the dataset.\n",
        "- $M$: is the set of all misclassified samples.\n",
        "- $\\mathbf{w}^T \\xi_i$: is the dot product between the transpose of the weight vector and the sample vector, giving a measure of the distance of the sample from the decision boundary.\n",
        "\n",
        "This formula essentially sums the absolute distances of all misclassified points from the decision boundary, aiming to find the weight vector that minimizes this sum, thus correctly classifying as many points as possible."
      ],
      "metadata": {
        "id": "IWX-aJDeple8"
      },
      "id": "IWX-aJDeple8"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Extra Materials (not mandatory)\n",
        "\n",
        "#### **Perceptron Learning Algorithm**\n",
        "\n",
        "The perceptron learning algorithm is a supervised learning method used primarily for binary classification problems — that is, categorizing a given input into one of two possible classes. The algorithm operates through an iterative process, continuously updating the weights associated with the inputs in order to find the optimal decision boundary that separates the classes. Here, I detail each step of the process along with the pertinent mathematical formulas:\n",
        "\n",
        "#### **1. Initialization**\n",
        "\n",
        "We initiate the weight vector (\\(w\\)) and the bias term (\\(b\\)) with random small numbers or zeros.\n",
        "\n",
        "$$\n",
        "\\mathbf{w} = [0, 0, \\ldots, 0] \\quad \\text{(or small random values)}\n",
        "$$\n",
        "$$\n",
        "b = 0 \\quad \\text{(or a small random value)}\n",
        "$$\n",
        "\n",
        "#### **2. Activation Function**\n",
        "\n",
        "The activation function, typically a step function, determines the output label based on the input features and the current weights:\n",
        "\n",
        "$$\n",
        "f(\\mathbf{x}) = \\begin{cases}\n",
        "  1 & \\text{if } \\mathbf{w}^T \\mathbf{x} + b > 0 \\\\\n",
        "  0 & \\text{if } \\mathbf{w}^T \\mathbf{x} + b \\leq 0\n",
        "\\end{cases}\n",
        "$$\n",
        "\n",
        "#### **3. Weight Update Rule**\n",
        "\n",
        "For each misclassified point, we update the weights and bias using the following rules:\n",
        "\n",
        "- **Positive mistake (false negative)**: If an instance belonging to class 1 is misclassified (the current weights classify it as 0), we update the weights and bias as:\n",
        "   \n",
        "$$\n",
        "\\mathbf{w} = \\mathbf{w} + \\eta y_i \\mathbf{x}_i\n",
        "$$\n",
        "$$\n",
        "b = b + \\eta y_i\n",
        "$$\n",
        "\n",
        "- **Negative mistake (false positive)**: Conversely, if an instance belonging to class 0 is misclassified (the current weights classify it as 1), we update the weights and bias as:\n",
        "\n",
        "$$\n",
        "\\mathbf{w} = \\mathbf{w} - \\eta y_i \\mathbf{x}_i\n",
        "$$\n",
        "$$\n",
        "b = b - \\eta y_i\n",
        "$$\n",
        "\n",
        "Here,\n",
        "- \\( y_i \\) is the true label of the \\( i^{th} \\) instance.\n",
        "- \\( \\mathbf{x}_i \\) is the \\( i^{th} \\) instance.\n",
        "\n",
        "#### **4. Learning Rate**\n",
        "\n",
        "The learning rate (\\( \\eta \\)) is a hyperparameter which controls the step size during the weight updates:\n",
        "\n",
        "$$\n",
        "\\eta \\in (0, 1]\n",
        "$$\n",
        "\n",
        "#### **5. Iterative Process**\n",
        "\n",
        "We repeat the weight update rule for a fixed number of iterations or until convergence, i.e., when no more mistakes are made, or the mistakes are below a certain threshold:\n",
        "\n",
        "$$\n",
        "\\text{Repeat until convergence or for a fixed number of epochs}\n",
        "$$\n",
        "\n",
        "#### **6. Outcome**\n",
        "\n",
        "At the end of the learning process, the final weights and bias are used to define the decision function that will classify new data points according to:\n",
        "\n",
        "$$\n",
        "f(\\mathbf{x}) = \\begin{cases}\n",
        "  1 & \\text{if } \\mathbf{w}^T \\mathbf{x} + b > 0 \\\\\n",
        "  0 & \\text{if } \\mathbf{w}^T \\mathbf{x} + b \\leq 0\n",
        "\\end{cases}\n",
        "$$\n",
        "\n",
        "### **Summary**\n",
        "\n",
        "By following these steps, the perceptron learning algorithm iteratively finds a hyperplane that best separates the two classes in the feature space, provided that the two classes are linearly separable. This involves a process of learning from mistakes, iteratively updating the weights to minimize the classification errors, eventually converging to a solution if one exists. It's a foundational algorithm in machine learning, giving rise to more complex models like neural networks in deep learning."
      ],
      "metadata": {
        "id": "t17sN-O6rCmP"
      },
      "id": "t17sN-O6rCmP"
    },
    {
      "cell_type": "markdown",
      "id": "4418682b-6dec-4966-90dd-e90634c25d31",
      "metadata": {
        "id": "4418682b-6dec-4966-90dd-e90634c25d31"
      },
      "source": [
        "### Logistic Regression Classifier\n",
        "\n",
        "\n",
        "\n",
        "<img src=\"https://raw.githubusercontent.com/SirTurtle/ML-BME-UofA-imgs/main/Week-3-Classification-models/imgs/LogisticRegression2.png\">\n",
        "\n",
        "Logistic regression model allows us to do that. It converts the output of the decision function h to probability of the positive class using sigmoid function. This function squashes the output of decision function into rage [0,1].\n",
        "\n",
        "Probability of label 1 given the feature x is therefore sigmoid of h(x), plotted here using the red solid line. The probability of label 0 for the same feature is 1 minus probability of label 1. It is plotted using the blue dotted line.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Logistic regression\n",
        "The next model we'll look at is the logistic regression classifier. Note that despite the name this is a classification model not a regression model!\n",
        "\n",
        "An advantage of the logistic regression classifier is that the output of the model is a probability.\n",
        "\n",
        "This is done using the sigmoid function which maps any real-valued number to the range (0, 1), making it suitable to represent a probability score.\n",
        "\n",
        "Here is how it works in detail:\n",
        "\n",
        "#### **1. Decision Function**\n",
        "\n",
        "The decision function in logistic regression is given by the linear combination of input features (\\( \\mathbf{x} \\)) and their respective weights (\\( \\mathbf{w} \\)):\n",
        "\n",
        "$$\n",
        "f(\\mathbf{x}) = \\mathbf{w}^T \\mathbf{x} + b\n",
        "$$\n",
        "\n",
        "#### **2. Sigmoid Function**\n",
        "\n",
        "The output of the decision function is then transformed using the sigmoid function to yield a probability value. The sigmoid function (\\( \\sigma \\)) is defined as:\n",
        "\n",
        "$$\n",
        "\\sigma(z) = \\frac{1}{1 + e^{-z}}\n",
        "$$\n",
        "\n",
        "So, applying this to our decision function, we get:\n",
        "\n",
        "$$\n",
        "P(y=1|\\mathbf{x}) = \\sigma(f(\\mathbf{x})) = \\sigma(\\mathbf{w}^T \\mathbf{x} + b) = \\frac{1}{1 + e^{-(\\mathbf{w}^T \\mathbf{x} + b)}}\n",
        "$$\n",
        "\n",
        "#### **3. Probabilities of Each Class**\n",
        "\n",
        "Using the sigmoid function, we can determine the probability that a given input (\\( \\mathbf{x} \\)) belongs to class 1 (\\( y = 1 \\)) or class 0 (\\( y = 0 \\)):\n",
        "\n",
        "- **Probability of \\( y = 1 \\) given \\( \\mathbf{x} \\)**\n",
        "  \n",
        "  We already derived this above:\n",
        "\n",
        "  $$\n",
        "  P(y=1|\\mathbf{x}) = \\frac{1}{1 + e^{-(\\mathbf{w}^T \\mathbf{x} + b)}}\n",
        "  $$\n",
        "\n",
        "- **Probability of \\( y = 0 \\) given \\( \\mathbf{x} \\)**\n",
        "  \n",
        "  This can be computed as one minus the probability of \\( y = 1 \\):\n",
        "\n",
        "  $$\n",
        "  P(y=0|\\mathbf{x}) = 1 - P(y=1|\\mathbf{x}) = 1 - \\frac{1}{1 + e^{-(\\mathbf{w}^T \\mathbf{x} + b)}}\n",
        "  $$\n",
        "\n",
        "#### **4. Logistic Regression Training**\n",
        "\n",
        "During the training process of logistic regression, the goal is to find the best parameters \\( \\mathbf{w} \\) and \\( b \\) that minimize the loss function, commonly the log-loss defined as:\n",
        "\n",
        "$$\n",
        "\\text{Log-Loss} = -\\frac{1}{N} \\sum_{i=1}^{N} [y_i \\log(\\hat{y}_i) + (1 - y_i) \\log(1 - \\hat{y}_i)]\n",
        "$$\n",
        "\n",
        "where:\n",
        "- \\( N \\) is the number of training examples\n",
        "- \\( y_i \\) is the actual label of the \\( i^{th} \\) training example\n",
        "- \\( \\hat{y}_i \\) is the predicted probability of the \\( i^{th} \\) training example being in class 1.\n",
        "\n",
        "#### **5. Making Predictions**\n",
        "\n",
        "After training, we can make predictions on new data points using the following decision rule:\n",
        "\n",
        "$$\n",
        "\\hat{y} = \\begin{cases}\n",
        "  1 & \\text{if } P(y=1|\\mathbf{x}) \\geq 0.5 \\\\\n",
        "  0 & \\text{if } P(y=1|\\mathbf{x}) < 0.5\n",
        "\\end{cases}\n",
        "$$\n",
        "\n",
        "### **Summary**\n",
        "\n",
        "Logistic regression uses the sigmoid function to squash the output of the linear decision function, producing a probability score between 0 and 1. This score represents the likelihood of a given input belonging to class 1. The logistic regression model is trained to find the optimal weights and bias that minimize the log-loss over the training data. Once the optimal parameters are found, the model can make predictions on new inputs, assigning them to one of the two classes based on whether the computed probability is greater or less than a threshold, commonly set at 0.5."
      ],
      "metadata": {
        "id": "rX0sjMbBuwvw"
      },
      "id": "rX0sjMbBuwvw"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Cross-entropy loss function\n",
        "\n",
        "<img src=\"https://raw.githubusercontent.com/SirTurtle/ML-BME-UofA-imgs/main/Week-3-Classification-models/imgs/CrossEntropy2.png\">\n",
        "\n",
        "So how do we fit the logistic regression model? We minimise cross entropy loss. Let’s consider a single sample with index i and see what cross entropy loss means for this sample. The probability pi for this sample is the probability of the class one.\n",
        "\n",
        "If the label for this sample is 1, the penalty will be equal to minus logarithm pi. If pi is one, it will result in zero penalty. If $p_i$ is close to zero, it will result in large penalty. The loss function is therefore forcing the probability to 1 for samples with label 1.\n",
        "\n",
        "If the label for this sample is 0, the penalty will be equal to minus logarithm 1\n",
        "$p_i$. If pi is zero, the penalty is zero as well. If $p_i$ is close to 1, it will result in large penalty. For samples with label 0 the loss function forces probability to zero as well.\n",
        "\n",
        "We can therefore see that minimisation of cross entropy ensures that probabilities $p_i$ are similar to labels $y_i$. The solution is found using numerical methods and in this case, the convergence is guaranteed.\n",
        "\n"
      ],
      "metadata": {
        "id": "AidrgPTKsBOX"
      },
      "id": "AidrgPTKsBOX"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Extra Materials (not mandatory)\n",
        "### **Cross-Entropy Loss Function (or log-loss)**\n",
        "\n",
        "#### **Definition**\n",
        "\n",
        "The cross-entropy loss function, also known as log-loss, is a loss function used in binary and multiclass classification problems. In binary classification, it is defined for a single sample as:\n",
        "\n",
        "$$\n",
        "\\text{Cross-Entropy Loss} = - [y \\log(\\hat{y}) + (1 - y) \\log(1 - \\hat{y})]\n",
        "$$\n",
        "\n",
        "where:\n",
        "- \\(y\\) is the true label (0 or 1)\n",
        "- \\(\\hat{y}\\) is the predicted probability of the sample being in class 1.\n",
        "\n",
        "#### **Properties**\n",
        "\n",
        "1. **Non-negative**: The cross-entropy loss is always non-negative, and is zero if and only if the predicted probability matches the true label exactly.\n",
        "   \n",
        "2. **Penalizes Confident Incorrect Predictions**: The loss grows infinitely when the predicted probability diverges from the true label.\n",
        "\n",
        "#### **Intuition**\n",
        "\n",
        "The intuition behind the cross-entropy loss function can be understood from the perspective of information theory. Essentially, it measures how well the predicted probabilities align with the actual classes. It quantifies the \"surprise\" from observing a different outcome than predicted, with predictions that are further from the true label incurring a larger loss.\n",
        "\n",
        "#### **Application to Logistic Regression**\n",
        "\n",
        "In logistic regression, we use the cross-entropy loss function during the training process to find the optimal parameters. This is typically done through a process of gradient descent, where we iteratively update the parameters to minimize the loss function. The overall loss over a dataset of \\(N\\) samples is given by the average loss over all samples:\n",
        "\n",
        "$$\n",
        "\\text{Average Cross-Entropy Loss} = -\\frac{1}{N} \\sum_{i=1}^{N} [y_i \\log(\\hat{y}_i) + (1 - y_i) \\log(1 - \\hat{y}_i)]\n",
        "$$\n",
        "\n",
        "#### **Multiclass Classification**\n",
        "\n",
        "In multiclass classification scenarios, we generalize the cross-entropy loss function to handle more than two classes. The loss for a single sample in this case is given by:\n",
        "\n",
        "$$\n",
        "\\text{Cross-Entropy Loss} = -\\sum_{c=1}^{C} y_c \\log(\\hat{y}_c)\n",
        "$$\n",
        "\n",
        "where:\n",
        "- $C$ is the number of classes\n",
        "- $y_c$ is 1 if the true class is $c$ and 0 otherwise\n",
        "- $\\hat{y}_c$ is the predicted probability of the sample belonging to class $c$.\n",
        "\n",
        "### **Conclusion**\n",
        "\n",
        "In conclusion, the cross-entropy loss function is a core component in logistic regression and many other classification algorithms. I**t helps in quantifying the difference between the actual and predicted labels**, **guiding the optimization of the model parameters during training** to achieve better accuracy in classification. It is defined slightly differently for binary and multiclass classification problems, but maintains the same underlying principle of penalizing predictions that are further away from the true labels."
      ],
      "metadata": {
        "id": "T_-rQ8FIu85s"
      },
      "id": "T_-rQ8FIu85s"
    },
    {
      "cell_type": "markdown",
      "id": "9ba17139-a8f1-471c-9718-856a5a3d6be2",
      "metadata": {
        "id": "9ba17139-a8f1-471c-9718-856a5a3d6be2"
      },
      "source": [
        "### Support Vector Classifier\n",
        "\n",
        "Next we'll take a look at the support vector classifier (SVC). This is also often called support vector machine (SVM).\n",
        "\n",
        "<img src=\"https://raw.githubusercontent.com/SirTurtle/ML-BME-UofA-imgs/main/Week-3-Classification-models/imgs/LinearlySeparableDataset.png\" width = \"400\" style=\"float: right;\">\n",
        "\n",
        "- First, let's assume we have a linearly separable dataset\n",
        "- All 3 decision boundaries result in accuracy = 1\n",
        "- Which boundary is likely to generalise well?\n",
        "- The red boundary is most likely to generalise well\n",
        "\n",
        "Linearly separable datasets can be perfectly separated by a linear decision boundary and we can achieve classification accuracy 1. In our example of diagnosis of heart failure, this is the case for healthy patients and patients with severe heart failure.\n",
        "\n",
        "There are many decision boundaries with accuracy 1 for separable datasets. So how do we choose the one that is most likely to generalise well?\n",
        "\n",
        "The red boundary seem to be the best because it is far from the samples unlike the other two."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6cabfacb-2032-4124-a8a2-1959aef8b6bd",
      "metadata": {
        "id": "6cabfacb-2032-4124-a8a2-1959aef8b6bd"
      },
      "source": [
        "**Large margin classifier (hard margin)**\n",
        "\n",
        "<img src=\"https://raw.githubusercontent.com/SirTurtle/ML-BME-UofA-imgs/main/Week-3-Classification-models/imgs/HardMargin.png\" width = \"400\" style=\"float: right;\">\n",
        "\n",
        "With a large margin classifier, the decision boundary is\n",
        "- as far as possible from the samples\n",
        "- determined by samples on the margins - **support vectors**\n",
        "\n",
        "Support vector classifier is a large margin classifier, which means that it searches for a decision boundary that is as far as possible from the samples.\n",
        "\n",
        "The decision boundary is determined by samples that lie on the margins and are called support vectors, here denoted by pink circles."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "27971451-a28f-42c1-af6a-499a09313f1d",
      "metadata": {
        "id": "27971451-a28f-42c1-af6a-499a09313f1d"
      },
      "source": [
        "**Large margin classifier (soft margin)**\n",
        "\n",
        "<img src=\"https://raw.githubusercontent.com/SirTurtle/ML-BME-UofA-imgs/main/Week-3-Classification-models/imgs/SoftMargin.png\" width = \"400\" style=\"float: right;\">\n",
        "\n",
        "With a soft margin classifier, the decision boundary\n",
        "- minimises margin violations\n",
        "- is determined by samples on or inside the margins or on the wrong side of the decision boundary - **support vectors**\n",
        "\n",
        "Large margin classifiers can be generalised to non-separable datasets by minimising the margin violations. The decision boundary is again determined by support vectors, which lie on or inside the margin or on the wrong side of the decision boundary."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1ad91a81-264f-46c6-80b0-5c8761eae577",
      "metadata": {
        "id": "1ad91a81-264f-46c6-80b0-5c8761eae577"
      },
      "source": [
        "### Decision Tree Classifier\n",
        "\n",
        "<img src=\"https://raw.githubusercontent.com/SirTurtle/ML-BME-UofA-imgs/main/Week-3-Classification-models/imgs/Decision-tree.png\" width = \"400\" style=\"float: right;\">\n",
        "\n",
        "A decision tree is a type of graph\n",
        "\n",
        "- Nodes (questions)\n",
        "- Edges (binary choices\n",
        "\n",
        "Sets of tests that are hierarchically organised. Each test is a _weak learner_.\n",
        "\n",
        "Advantages of decision trees:\n",
        "\n",
        "- Easy to interpret\n",
        "- Able to handle both numerical and categorical data\n",
        "- Able to handle multi-class problems\n",
        "- Requires little data preparation (e.g. no normalization)\n",
        "- Can be used for both classification and regression"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4aa392e8-30e3-4659-a248-3b536de33289",
      "metadata": {
        "id": "4aa392e8-30e3-4659-a248-3b536de33289"
      },
      "source": [
        "Imagine it like a game of 20 questions. The model asks questions about the data and makes decisions based on the answers. And the cool thing? It can change or fine-tune its answers as it gets more information!\n",
        "\n",
        "<img src='https://raw.githubusercontent.com/SirTurtle/ML-BME-UofA-imgs/main/Week-3-Classification-models/imgs/youdroppedfood.jpg' width=500px align=\"center\">\n",
        "\n",
        "(Image courtsey: Audrey Fukman and Andy Wright on SFoodie, via Serious Eats)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3470eb95-ea9a-4327-a4fe-c882982de90e",
      "metadata": {
        "id": "3470eb95-ea9a-4327-a4fe-c882982de90e"
      },
      "source": [
        "### Random Forest Classifier\n",
        "\n",
        "Ensemble of decision trees\n",
        "\n",
        "- Increases randomisation by restricting each tree node's choice of optimal feature from a subset of the total feature space\n",
        "- Further decorrelates predictive models\n",
        "- Further decreases model variance\n",
        "- Increases stability against feature noise and thus chance of overfitting"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e8b5eda6-4d32-40ae-ba91-21d9ee78c541",
      "metadata": {
        "id": "e8b5eda6-4d32-40ae-ba91-21d9ee78c541"
      },
      "source": [
        "**If a tree is good, is a forest better?**\n",
        "\n",
        "<img src='https://raw.githubusercontent.com/SirTurtle/ML-BME-UofA-imgs/main/Week-3-Classification-models/imgs/random_forest.jpeg' width=500px align=\"right\">\n",
        "\n",
        "<img src='https://raw.githubusercontent.com/SirTurtle/ML-BME-UofA-imgs/main/Week-3-Classification-models/imgs/Random_forest_explain.png' width=500px align=\"right\">\n",
        "\n",
        "Think about it this way - if one tree is handy, wouldn't a whole forest be even better?\n",
        "\n",
        "Imagine we could create a bunch of decision trees, each one asking different questions. Then, we could combine their answers to make a final prediction. This is exactly what a Random Forest does.\n",
        "\n",
        "A Random Forest is what we call an 'ensemble model'. It's like a supergroup band made up of lots of individual musicians, all working together to create a harmonious sound. Here, each model contributes to a final, hopefully better, prediction."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5daa67c6-4605-4ec3-ae1b-0f4f6f71d6f7",
      "metadata": {
        "id": "5daa67c6-4605-4ec3-ae1b-0f4f6f71d6f7"
      },
      "source": [
        "## Extra techniques\n",
        "\n",
        "Finally we will cover examples of some additional techniques that are important for classification problems:\n",
        "\n",
        "1. Encoding classes\n",
        "2. One-hot encoding\n",
        "3. Creating training and test sets\n",
        "4. Handling unbalanced datasets"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ccf26e3b-a7d3-40fd-b88c-30a940c59c10",
      "metadata": {
        "id": "ccf26e3b-a7d3-40fd-b88c-30a940c59c10"
      },
      "source": [
        "### One-hot encoding\n",
        "\n",
        "<img src='https://raw.githubusercontent.com/SirTurtle/ML-BME-UofA-imgs/main/Week-3-Classification-models/imgs/one-hot-encoding.png' width=500px align=\"right\">\n",
        "\n",
        "Machine learning is a bit like a kid who only likes to play with numbers and not with words. So, when we have categories like **'apple' and 'peer'**, we need to find a way to turn them into numbers.\n",
        "\n",
        "This is where one-hot encoding comes in handy! It's a cool trick that turns these categories into something machine learning algorithms can work with. It's like giving each category its own 'on' and 'off' switch.\n",
        "\n",
        "And guess what? There's an easy way to do this if you're using pandas, a tool in Python. It has a function called 'get_dummies' that does all the work for you.\n",
        "\n",
        "**I thought it would be intresting for you to know**: the term \"one-hot\" in \"one-hot encoding\" comes from the way digital circuits are designed. In digital electronics, a one-hot signal is a group of bits among which the legal combinations of values are only those with a single high bit (1) and all the others low (0)."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c3cbee22-ca0d-45d4-840d-2cf08a0d0865",
      "metadata": {
        "id": "c3cbee22-ca0d-45d4-840d-2cf08a0d0865"
      },
      "source": [
        "## Example - The Titanic Kaggle challenge: A case study for classification\n",
        "\n",
        "Titanic Kaggle Challenge is a competition where you'll use data to predict who could've survived the infamous Titanic disaster.\n",
        "\n",
        "Classification \"survived\" or \"not survived\"\"\n",
        "\n",
        "<img src='https://raw.githubusercontent.com/SirTurtle/ML-BME-UofA-imgs/main/Week-3-Classification-models/imgs/Kaggle-Titanic-Project-Getting-Started.png' width=700px align=\"center\">\n",
        "\n",
        "Let's explore machine learning with a fun example from Kaggle, a competition site owned by Google. These contests can be for giggles, cash prizes, or even a job offer sometimes! The Titanic Kaggle Challenge is known as one of the classic examples for learning classification in a hands-on way.\n",
        "\n",
        "One beginner's challenge is based on the Titanic disaster, a famous shipwreck that happened on April 15, 1912. The ship, thought to be unsinkable, hit an iceberg and sank, sadly causing 1502 out of 2224 people onboard to lose their lives because there weren't enough lifeboats.\n",
        "\n",
        "But it's not just about guessing who made it. It's about deeply exploring the data, finding patterns, and understanding how different factors might have affected survival rates. It poses questions like 'Did socioeconomic status influence survival rates?' and 'What was the impact of the \"women and children first\" policy? Was the 'women and children first' policy strictly followed?'\n",
        "\n",
        "Here's the interesting part: it seems that some people were more likely to survive than others. The challenge asks us to figure out who these folks were, using data like their names, ages, genders, and social classes.\n",
        "\n",
        "We get a file with details about 891 passengers, including whether they survived or not. We'll use this data to teach our machine to make smart guesses.\n",
        "\n",
        "But the real test comes with another file, this one has information on 418 passengers, but doesn't tell us if they survived. That's where our machine's predictions come in!\n",
        "\n",
        "Suggested tutorial about Kaggle's titanic challange: [Titanic - Machine Learning from Disaster](https://www.kaggle.com/competitions/titanic)\n",
        "\n",
        "You can follow Chris White's tutorial on this subject through this Jupyter notebook: [01-intro-classification.ipynb](https://colab.research.google.com/github/ualberta-rcg/python-machine-learning/blob/main/notebooks/01-intro-classification.ipynb#scrollTo=Gd80ekh_zQu4)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7f0b1e67-92c6-4407-9fcb-7c36d7278eac",
      "metadata": {
        "id": "7f0b1e67-92c6-4407-9fcb-7c36d7278eac"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.4"
    },
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}