{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MaralAminpour/ML-BME-UofA/blob/main/Week-3-Classification-models/3.1-Classification-intro.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3a664f09-1cb3-4f9b-b798-d01190827bcd",
      "metadata": {
        "id": "3a664f09-1cb3-4f9b-b798-d01190827bcd"
      },
      "source": [
        "# Classification\n",
        "\n",
        "This week, we will cover machine learning for classification. First, we will cover some of the basic concepts. Then we will look at some hands-on examples using Scikit-Learn."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "32b8259e-265d-46eb-bca0-714156a657a8",
      "metadata": {
        "id": "32b8259e-265d-46eb-bca0-714156a657a8"
      },
      "source": [
        "### What is classification?\n",
        "\n",
        "In a classification task in machine learning, we want to determine the **class or group** a particular **sample ** belongs to based on **one or more of its features**. These groups are sometimes referred to as **labels**. Our model aims to accurately predict the correct label for each sample.\n",
        "\n",
        "We carry out this task through supervised learning, which means that we use a **dataset** where the **correct labels are already known** to **train** our model. For instance, we might have a dataset where samples are labeled either \"**healthy\" or \"disease.**\" In scenarios like this with just two possible labels, we are dealing with a binary classification problem. It is standard practice to represent one class as **positive (or label 1)** and the other as **negative (or label 0)**.\n",
        "\n",
        "When there are more than two potential labels for our samples, we are facing a **multiclass or multilabel classificatio**n problem. For instance, samples could be categorized as **\"healthy**,\" \"**minor disease**,\" or \"**serious disease**.\"\n",
        "\n",
        "**In multiclass problems, each sample is assigned to one and only one label, while in multilabel problems, a sample can be associated with multiple labels. **"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "93d9f8d5-9125-4788-970a-40e32ae7c9d1",
      "metadata": {
        "id": "93d9f8d5-9125-4788-970a-40e32ae7c9d1"
      },
      "source": [
        "### Prediction of labeling and prediction of confidence\n",
        "\n",
        "At the very least, our model should **indicate the predicted class for each sample**. Additionally, it can be beneficial for the model to offer a **level of confidence** in its predictions or to give the **predicted probabilities for each class**."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f4a511d3-20d3-464b-8b6e-2b2c71e61f7a",
      "metadata": {
        "id": "f4a511d3-20d3-464b-8b6e-2b2c71e61f7a"
      },
      "source": [
        "### Confusion matrix\n",
        "\n",
        "Alright, it's time to get into the world of stats and technical terms! We're going to explore something called a 'confusion matrix'. This is a handy tool that allows us to **compare our model's predictions with the actual results in our test data**.\n",
        "\n",
        "The matrix itself can be confusing for people who haven’t seen it before. Maybe that’s **why it was called confusion **matrix. It’s a good idea to extract some understandable data from the matrix, that’s why we have **precision, accuracy, recall, and F1-score.**\n",
        "\n",
        "**First**, we need to **select** one class to be the **\"positive\" class** and the other class to be the **\"negative\" class**. Usually we will use label 0 as the negative class and label 1 and the positive class.\n",
        "\n",
        "A confusion matrix is a bit like a report card for our predictions. It gives us the numbers for when we got things right and when we got them wrong, in a handy little table:\n",
        "\n",
        "<img src=\"https://raw.githubusercontent.com/SirTurtle/ML-BME-UofA-imgs/main/Week-3-Classification-models/imgs/Confusion-Matrix-2.png\" width = \"350\" style=\"float: left;\">\n",
        "\n",
        "<img src='https://raw.githubusercontent.com/SirTurtle/ML-BME-UofA-imgs/main/Week-3-Classification-models/imgs/confusion-matrix.png' width=500px align=\"center\">\n",
        "\n",
        "**True positive (TP)**: The actual and predicted labels are both positive.\n",
        "\n",
        "**True negative (TN)**: The actual and predicted labels are both negative.\n",
        "\n",
        "**False positive (FP)**: The actual label is negative but the predicted label is positive.\n",
        "\n",
        "**False negative (FN)**: The actual label is positive but the predicted label is negative.\n",
        "\n",
        "The **True negative** and **true positives** are when our predictions match reality.\n",
        "\n",
        "**True negatives are when we said a person wouldn't survive, and they didn't.**\n",
        "\n",
        "True positives are when we said a person would survive, and they did.\n",
        "\n",
        "**False negatives and false positives are where we slipped up**. False negatives are when we said a person wouldn't survive, but they ended up surviving. Ooops!\n",
        "\n",
        "False positives are when we said a person would survive, but they didn't. Sorry!\n",
        "\n",
        "**Confusion matrices** can also be used for multiclass classification, not just binary classification."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6cd9a9a1-ae9c-4734-981f-59e3d303a07c",
      "metadata": {
        "id": "6cd9a9a1-ae9c-4734-981f-59e3d303a07c"
      },
      "source": [
        "### Scoring classification models\n",
        "\n",
        "There are several scores (or metrics) that are useful for evaluating the perfomance of a classification model, and for comparing different classification models.\n",
        "\n",
        "####1. Accuracy\n",
        "\n",
        "<font color=blue>*Accuracy: correct prediction/total number of predictions.*</font>\n",
        "\n",
        "That’s the sum of items in the diagonal divided by the total number of items!\n",
        "\n",
        "**accuracy** = $\\frac{(TP + TN)}{TP + TN + FP + FN}$\n",
        "\n",
        "The simplest score is accuracy. **We can calculate accuracy as the percentage of times we got it right.** It's the number of true negatives and true positives divided by the total number of predictions.\n",
        "\n",
        "Consider a team of medical researchers trying to predict whether or not a patient is at risk of heart failure:\n",
        "Just like in our previous examples, **accuracy is the proportion of correct predictions**.\n",
        "\n",
        "**Issue with accuracy**\n",
        "\n",
        "But, as we noted earlier, if heart failures are **rare**, a model that always predicts \"**no heart failure**\" might have a **high accuracy** but it's not really helpful in the medical context. So, it's important to consider more than just accuracy when evaluating such predictions!\n",
        "\n",
        "**So, there is an issue with accuracy. **Accuracy might not always give us the **full picture**. Let's say we want to predict a **rare disease** that only affects 1 in every 1,000 people. If we create a model that always predicts that no one has the disease, it will be 99.9% accurate, but it's not very useful, right?\n",
        "\n",
        "In the machine learning world, **recall and precision** are more commonly used:\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "<img src=\"https://raw.githubusercontent.com/MaralAminpour/ML-BME-UofA/main/Week-3-Classification-models/imgs/Precision_recall_Representation_1052507280.png\" width = \"350\" style=\"float: left;\">\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "####2. Precision\n",
        "\n",
        "<font color=blue>*Precision: correct positive prediction/total number of positive prediction*</font>\n",
        "\n",
        "Precision only cares about positive class most of the time (Sometimes we have negative predictive value, and it only cares about negative class).\n",
        "\n",
        "**precision** = $\\frac{TP}{TP + FP}$\n",
        "\n",
        "Precision is like asking,\n",
        "\n",
        "\"*Out of all the patients we predicted would experience heart failure, how many actually did?*\"\n",
        "\n",
        "**High precision** is key when we really want to avoid **false positives**.\n",
        "\n",
        "- For example, in court trials, we want to be sure that if we declare someone guilty, they really are.\n",
        "\n",
        "- Another example is email **spam filters**. We don't want to accidentally label important emails as spam.\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "####3. Recall(sensitivity)\n",
        "\n",
        "<font color=blue>*Recall: correct positive prediction/total number of positive class (in original data)*</font>\n",
        "\n",
        "**recall** = **true positive rate** = $\\frac{TP}{TP + FN}$\n",
        "\n",
        "The recall of the positive class is the same as sensitivity. The recall of the negative class is the same as specificity.\n",
        "\n",
        "Recall, on the other hand, is like asking,\n",
        "\n",
        "\"*Out of all the patients who actually experienced heart failure, how many did we correctly predict?*\"\n",
        "\n",
        "**High recall** is critical when we want to avoid **false negatives**.\n",
        "\n",
        "- For example, in cancer screenings, a false negative could mean a patient who actually has cancer gets a clean bill of health.\n",
        "\n",
        "Think about it, what do you think is more important for recommendation engines like YouTube, Netflix, or Spotify? Would it be precision or recall?\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "####4. Specificity\n",
        "\n",
        "<font color=blue>Specificity: True Negative predictions / (True Negative predictions + False Positive predictions)</font>\n",
        "\n",
        "**specificity** = **true negative rate** = $\\frac{TN}{TN + FP}$\n",
        "\n",
        "Specificity is a metric used in the context of binary classification in machine learning, which quantifies the ability of the classification model to correctly identify the negative instances from all the actual negative instances available. It's a very important metric, especially in medical diagnosis where we want to be very sure not to falsely identify a condition as being present (positive).\n",
        "\n",
        "We want our test to have a high specificity, which means it should correctly identify as many healthy individuals as possible without labeling them as diseased.\n",
        "\n",
        "\n",
        " If the diagnostic test has a specificity of 99%, indicating that it has a very high accuracy in correctly identifying individuals who do not have the disease, thus avoiding unnecessary distress and further testing for those individuals. This also shows that the test has a very low rate of false alarms, which is very important to prevent overdiagnosis and overtreatment.\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "####5. $F_1$ score\n",
        "\n",
        "Furthermore, the **$F_1$ score is a combination of recall and precision**, and is often used a single score to evaluate models.\n",
        "\n",
        "$F_1 = 2 \\frac{\\text{precision} \\cdot \\text{recall}}{\\text{precision} + \\text{recall}}$\n",
        "\n",
        "Imagine a circle that represents all the patients we predicted would experience heart failure. Now, the left part of that circle represents the patients who actually did experience heart failure.\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "036b6281-b407-40d9-8018-57c0e7e09417",
      "metadata": {
        "id": "036b6281-b407-40d9-8018-57c0e7e09417"
      },
      "source": [
        "#### Analogy to explain precision and recall\n",
        "\n",
        "[Here](https://towardsdatascience.com/precision-and-recall-88a3776c8007) is a great analogy to explain Precision and Recall, and I tried to summarize it below:\n",
        "\n",
        "**Fishing with a net**\n",
        "\n",
        "Think of it like fishing with a net. If you cast a **wide net** into a lake and **catch 80 out of 100 fish**, that's 80% **recall**. However, you also end up with 80 rocks in your net, which means your **precision is 50%** since half of the net's contents are unwanted junk.\n",
        "\n",
        "On the other hand, you could use a **smaller net** and focus on a **specific area of the lake** where there are lots of fish and no rocks. In this case, you might only catch 20 out of 100 the fish, but you'll have zero rocks. This results in **20% recall and 100% precision**.\n",
        "\n",
        "<img src='https://raw.githubusercontent.com/SirTurtle/ML-BME-UofA-imgs/main/Week-3-Classification-models/imgs/precision_recall.gif' width=400px align=\"right\">\n",
        "\n",
        "<img src='https://raw.githubusercontent.com/SirTurtle/ML-BME-UofA-imgs/main/Week-3-Classification-models/imgs/recall.gif' width=400px align=\"left\">"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "250e808e-f78c-450a-bd6b-495a1a4da870",
      "metadata": {
        "id": "250e808e-f78c-450a-bd6b-495a1a4da870"
      },
      "source": [
        "### Receiver-operator-characteristic (ROC) curves\n",
        "\n",
        "ROC curves are a handy tool to assess the **performance** of binary classification models, helping us visualize how well our model is doing.\n",
        "\n",
        "Before focusing on ROC curves, ensure your model can provide **confidence scores** or allows for tuning of the **decision threshold**.\n",
        "\n",
        "<img src=\"https://raw.githubusercontent.com/SirTurtle/ML-BME-UofA-imgs/main/Week-3-Classification-models/imgs/roc-curve.png\" width = \"500\" style=\"float: right;\">\n",
        "\n",
        "In an ROC curve, we plot the true positive rate (sensitivity) against the false positive rate (1-specificity) to see how they trade-off.\n",
        "\n",
        "Subsequently, we determine the area under the curve (AUC), which serves as a valuable metric to compare different models.\n",
        "\n",
        "With this metric, with a **larger AUC** are **ranked higher **than models with a **lower AUC**. **This assumes we are interested in models with a balance between sensitivity and specificity.**\n",
        "\n",
        "Generally, a model with a higher AUC is favored as it indicates a good balance between sensitivity and specificity, showcasing a model’s ability to maintain a healthy rate of true positives while minimizing false positives. Remember, we are aiming for a larger AUC to ensure the best blend of sensitivity and specificity!"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "39296d7f-9804-46c8-b16b-3692a04fb492",
      "metadata": {
        "id": "39296d7f-9804-46c8-b16b-3692a04fb492"
      },
      "source": [
        "## Classification Models\n",
        "\n",
        "Classification tasks can be approached using a variety of models, each suitable for different kinds of data. The ideal approach to selecting a model is by evaluating its performance using your specific dataset or drawing from past experiences. In the subsequent notebooks, we will delve into examples utilizing various models to give you a comprehensive understanding."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e02ee42e-3b1c-43e4-ab30-290412a27136",
      "metadata": {
        "id": "e02ee42e-3b1c-43e4-ab30-290412a27136"
      },
      "source": [
        "### Linear Binary Classification\n",
        "\n",
        "Let's take a closer look at how linear binary classification works.\n",
        "\n",
        "This method uses a **decision function**, which is basically a **multi-variable linear function**, to make **predictions**.\n",
        "Imagine we are using a special rule or formula (which we call a \"decision function\") to decide if someone is healthy or has heart failure.\n",
        "\n",
        "The core idea here is the \"decision boundary,\" created where the decision function equals zero. This boundary has a crucial role in determining the labels for new data points.\n",
        "\n",
        "Here's how it operates:\n",
        "\n",
        "When the decision function yields a positive value, it directs us to predict the label as 1.\n",
        "Conversely, a negative outcome from the decision function leads to a prediction of label 0.\n",
        "\n",
        "To give a concrete context, consider a health monitoring system distinguishing between \"healthy\" individuals and those experiencing \"heart failure.\" **The linear decision function would analyze various parameters, and depending upon the derived function value (positive or negative)**, it would categorize individuals into either group, thus forming a predictive model that can be a vital tool in healthcare settings.\n",
        "\n",
        "In other words, we have something called a \"decision boundary.\" You can think of this as a line that separates the healthy people from those with heart failure. This \"line\" is created based on the rule where the decision function equals zero.\n",
        "\n",
        "\n",
        "<img src=\"https://raw.githubusercontent.com/SirTurtle/ML-BME-UofA-imgs/main/Week-3-Classification-models/imgs/LinearBinaryClassification4.png\">\n",
        "\n",
        "\n",
        "**Linear Decision Function:**\n",
        "The decision function, denoted as $f(\\mathbf{x})$, is given by the linear equation\n",
        "\n",
        "$ f(x) = w_1 x_1 + w_2 x_2 + \\ldots + w_n x_n + b $\n",
        "\n",
        "$$ x = (1, x_1, x_2, \\ldots, x_N)^T $$\n",
        "$$ w = (w_0, w_1, w_2, \\ldots, w_N)^T $$\n",
        "\n",
        "\n",
        "where:\n",
        "*    $f(\\mathbf{x})$ is the decision function\n",
        "\n",
        "*    $w_1, w_2, \\ldots, w_n$ are the weights for the respective features\n",
        "\n",
        "*    $x_1, x_2, \\ldots, x_n$ are the feature values\n",
        "\n",
        "*    $b$ is the bias term\n",
        "\n",
        "**Decision Boundary:**\n",
        "The decision boundary occurs where the decision function equals zero, described by the equation\n",
        "$$ f(x) = 0 $$\n",
        "\n",
        "**Prediction Model:**\n",
        "The prediction model utilizes the decision function to classify data points as either \"healthy\" (label 1) or \"heart failure\" (label 0) using the following rules:\n",
        "\n",
        "*    If $f(\\mathbf{x}) > 0$, we predict the label as 1, which signifies \"healthy\".\n",
        "\n",
        "*    If $f(\\mathbf{x}) \\leq 0$, we predict the label as 0, indicating \"heart failure\".\n",
        "\n",
        "\n",
        "Let's now look in detail how linear binary classification works. The prediction is based on decision function, which is a multivariate linear function.\n",
        "\n",
        "The decision boundary is defined by decision function being equal to zero.\n",
        "\n",
        "If the value of decision function is positive, we will predict label 1. If the value of decision function is negative, we will predict label 0."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "83ea4c01-f150-44f5-ab9a-927f5fba00a1",
      "metadata": {
        "id": "83ea4c01-f150-44f5-ab9a-927f5fba00a1"
      },
      "source": [
        "### Perceptron\n",
        "\n",
        "<img src=\"https://raw.githubusercontent.com/SirTurtle/ML-BME-UofA-imgs/main/Week-3-Classification-models/imgs/Perceptron3.png\">\n",
        "\n",
        "here are the equations pertaining to the perceptron learning algorithm:\n",
        "\n",
        "**1. Input vector and weight vector:** We represent the input vector using x and the weight vector using w, each including their respective bias terms as the first element:\n",
        "\n",
        "$$ x = (1, x_1, x_2, ..., x_N)^T $$\n",
        "$$ w = (w_0, w_1, w_2, ..., w_N)^T $$\n",
        "\n",
        "**2. Linear decision function (perceptron model):** The decision function is formulated using the dot product of the input vector and the weight vector plus a bias term b:\n",
        "\n",
        "$$ f(x) = w^T x + b $$\n",
        "\n",
        "**3. Decision boundary:** The decision boundary is where the decision function is equal to zero:\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4418682b-6dec-4966-90dd-e90634c25d31",
      "metadata": {
        "id": "4418682b-6dec-4966-90dd-e90634c25d31"
      },
      "source": [
        "### Logistic Regression Classifier\n",
        "\n",
        "The next model we'll look at is the logistic regression classifier. Note that despite the name this is a classification model not a regression model!\n",
        "\n",
        "An advantage of the logistic regression classifier is that the output of the model is a probability.\n",
        "\n",
        "<img src=\"https://raw.githubusercontent.com/SirTurtle/ML-BME-UofA-imgs/main/Week-3-Classification-models/imgs/LogisticRegression2.png\">\n",
        "\n",
        "Logistic regression model allows us to do that. It converts the output of the decision function h to probability of the positive class using sigmoid function. This function squashes the output of decision function into rage [0,1].\n",
        "\n",
        "Probability of label 1 given the feature x is therefore sigmoid of h(x), plotted here using the red solid line. The probability of label 0 for the same feature is 1 minus probability of label 1. It is plotted using the blue dotted line.\n",
        "\n",
        "<img src=\"https://raw.githubusercontent.com/SirTurtle/ML-BME-UofA-imgs/main/Week-3-Classification-models/imgs/CrossEntropy2.png\">\n",
        "\n",
        "So how do we fit the logistic regression model? We minimise cross entropy loss. Let’s consider a single sample with index i and see what cross entropy loss means for this sample. The probability pi for this sample is the probability of the class one.\n",
        "\n",
        "If the label for this sample is 1, the penalty will be equal to minus logarithm pi. If pi is one, it will result in zero penalty. If $p_i$ is close to zero, it will result in large penalty. The loss function is therefore forcing the probability to 1 for samples with label 1.\n",
        "\n",
        "If the label for this sample is 0, the penalty will be equal to minus logarithm 1\n",
        "$p_i$. If pi is zero, the penalty is zero as well. If $p_i$ is close to 1, it will result in large penalty. For samples with label 0 the loss function forces probability to zero as well.\n",
        "\n",
        "We can therefore see that minimisation of cross entropy ensures that probabilities $p_i$ are similar to labels $y_i$. The solution is found using numerical methods and in this case, the convergence is guaranteed."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9ba17139-a8f1-471c-9718-856a5a3d6be2",
      "metadata": {
        "id": "9ba17139-a8f1-471c-9718-856a5a3d6be2"
      },
      "source": [
        "### Support Vector Classifier\n",
        "\n",
        "Next we'll take a look at the support vector classifier (SVC). This is also often called support vector machine (SVM).\n",
        "\n",
        "<img src=\"https://raw.githubusercontent.com/SirTurtle/ML-BME-UofA-imgs/main/Week-3-Classification-models/imgs/LinearlySeparableDataset.png\" width = \"400\" style=\"float: right;\">\n",
        "\n",
        "- First, let's assume we have a linearly separable dataset\n",
        "- All 3 decision boundaries result in accuracy = 1\n",
        "- Which boundary is likely to generalise well?\n",
        "- The red boundary is most likely to generalise well\n",
        "\n",
        "Linearly separable datasets can be perfectly separated by a linear decision boundary and we can achieve classification accuracy 1. In our example of diagnosis of heart failure, this is the case for healthy patients and patients with severe heart failure.\n",
        "\n",
        "There are many decision boundaries with accuracy 1 for separable datasets. So how do we choose the one that is most likely to generalise well?\n",
        "\n",
        "The red boundary seem to be the best because it is far from the samples unlike the other two."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6cabfacb-2032-4124-a8a2-1959aef8b6bd",
      "metadata": {
        "id": "6cabfacb-2032-4124-a8a2-1959aef8b6bd"
      },
      "source": [
        "<img src=\"https://raw.githubusercontent.com/SirTurtle/ML-BME-UofA-imgs/main/Week-3-Classification-models/imgs/HardMargin.png\" width = \"400\" style=\"float: right;\">\n",
        "\n",
        "**Large margin classifier (hard margin)**\n",
        "\n",
        "With a large margin classifier, the decision boundary is\n",
        "- as far as possible from the samples\n",
        "- determined by samples on the margins - **support vectors**\n",
        "\n",
        "Support vector classifier is a large margin classifier, which means that it searches for a decision boundary that is as far as possible from the samples.\n",
        "\n",
        "The decision boundary is determined by samples that lie on the margins and are called support vectors, here denoted by pink circles."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "27971451-a28f-42c1-af6a-499a09313f1d",
      "metadata": {
        "id": "27971451-a28f-42c1-af6a-499a09313f1d"
      },
      "source": [
        "<img src=\"https://raw.githubusercontent.com/SirTurtle/ML-BME-UofA-imgs/main/Week-3-Classification-models/imgs/SoftMargin.png\" width = \"400\" style=\"float: right;\">\n",
        "\n",
        "**Large margin classifier (soft margin)**\n",
        "\n",
        "With a soft margin classifier, the decision boundary\n",
        "- minimises margin violations\n",
        "- is determined by samples on or inside the margins or on the wrong side of the decision boundary - **support vectors**\n",
        "\n",
        "Large margin classifiers can be generalised to non-separable datasets by minimising the margin violations. The decision boundary is again determined by support vectors, which lie on or inside the margin or on the wrong side of the decision boundary."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1ad91a81-264f-46c6-80b0-5c8761eae577",
      "metadata": {
        "id": "1ad91a81-264f-46c6-80b0-5c8761eae577"
      },
      "source": [
        "### Decision Tree Classifier\n",
        "\n",
        "<img src=\"https://raw.githubusercontent.com/SirTurtle/ML-BME-UofA-imgs/main/Week-3-Classification-models/imgs/Decision-tree.png\" width = \"400\" style=\"float: right;\">\n",
        "\n",
        "A decision tree is a type of graph\n",
        "\n",
        "- Nodes (questions)\n",
        "- Edges (binary choices\n",
        "\n",
        "Sets of tests that are hierarchically organised. Each test is a _weak learner_.\n",
        "\n",
        "Advantages of decision trees:\n",
        "\n",
        "- Easy to interpret\n",
        "- Able to handle both numerical and categorical data\n",
        "- Able to handle multi-class problems\n",
        "- Requires little data preparation (e.g. no normalization)\n",
        "- Can be used for both classification and regression"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4aa392e8-30e3-4659-a248-3b536de33289",
      "metadata": {
        "id": "4aa392e8-30e3-4659-a248-3b536de33289"
      },
      "source": [
        "Imagine it like a game of 20 questions. The model asks questions about the data and makes decisions based on the answers. And the cool thing? It can change or fine-tune its answers as it gets more information!\n",
        "\n",
        "<img src='https://raw.githubusercontent.com/SirTurtle/ML-BME-UofA-imgs/main/Week-3-Classification-models/imgs/youdroppedfood.jpg' width=500px align=\"center\">\n",
        "\n",
        "(Image courtsey: Audrey Fukman and Andy Wright on SFoodie, via Serious Eats)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3470eb95-ea9a-4327-a4fe-c882982de90e",
      "metadata": {
        "id": "3470eb95-ea9a-4327-a4fe-c882982de90e"
      },
      "source": [
        "### Random Forest Classifier\n",
        "\n",
        "Ensemble of decision trees\n",
        "\n",
        "- Increases randomisation by restricting each tree node's choice of optimal feature from a subset of the total feature space\n",
        "- Further decorrelates predictive models\n",
        "- Further decreases model variance\n",
        "- Increases stability against feature noise and thus chance of overfitting"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e8b5eda6-4d32-40ae-ba91-21d9ee78c541",
      "metadata": {
        "id": "e8b5eda6-4d32-40ae-ba91-21d9ee78c541"
      },
      "source": [
        "**If a tree is good, is a forest better?**\n",
        "\n",
        "<img src='https://raw.githubusercontent.com/SirTurtle/ML-BME-UofA-imgs/main/Week-3-Classification-models/imgs/random_forest.jpeg' width=500px align=\"right\">\n",
        "\n",
        "<img src='https://raw.githubusercontent.com/SirTurtle/ML-BME-UofA-imgs/main/Week-3-Classification-models/imgs/Random_forest_explain.png' width=500px align=\"right\">\n",
        "\n",
        "Think about it this way - if one tree is handy, wouldn't a whole forest be even better?\n",
        "\n",
        "Imagine we could create a bunch of decision trees, each one asking different questions. Then, we could combine their answers to make a final prediction. This is exactly what a Random Forest does.\n",
        "\n",
        "A Random Forest is what we call an 'ensemble model'. It's like a supergroup band made up of lots of individual musicians, all working together to create a harmonious sound. Here, each model contributes to a final, hopefully better, prediction."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5daa67c6-4605-4ec3-ae1b-0f4f6f71d6f7",
      "metadata": {
        "id": "5daa67c6-4605-4ec3-ae1b-0f4f6f71d6f7"
      },
      "source": [
        "## Extra techniques\n",
        "\n",
        "Finally we will cover examples of some additional techniques that are important for classification problems:\n",
        "\n",
        "1. Encoding classes\n",
        "2. One-hot encoding\n",
        "3. Creating training and test sets\n",
        "4. Handling unbalanced datasets"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ccf26e3b-a7d3-40fd-b88c-30a940c59c10",
      "metadata": {
        "id": "ccf26e3b-a7d3-40fd-b88c-30a940c59c10"
      },
      "source": [
        "### One-hot encoding\n",
        "\n",
        "<img src='https://raw.githubusercontent.com/SirTurtle/ML-BME-UofA-imgs/main/Week-3-Classification-models/imgs/one-hot-encoding.png' width=500px align=\"right\">\n",
        "\n",
        "Machine learning is a bit like a kid who only likes to play with numbers and not with words. So, when we have categories like **'apple' and 'peer'**, we need to find a way to turn them into numbers.\n",
        "\n",
        "This is where one-hot encoding comes in handy! It's a cool trick that turns these categories into something machine learning algorithms can work with. It's like giving each category its own 'on' and 'off' switch.\n",
        "\n",
        "And guess what? There's an easy way to do this if you're using pandas, a tool in Python. It has a function called 'get_dummies' that does all the work for you.\n",
        "\n",
        "**I thought it would be intresting for you to know**: the term \"one-hot\" in \"one-hot encoding\" comes from the way digital circuits are designed. In digital electronics, a one-hot signal is a group of bits among which the legal combinations of values are only those with a single high bit (1) and all the others low (0)."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c3cbee22-ca0d-45d4-840d-2cf08a0d0865",
      "metadata": {
        "id": "c3cbee22-ca0d-45d4-840d-2cf08a0d0865"
      },
      "source": [
        "## Example - The Titanic Kaggle challenge: A case study for classification\n",
        "\n",
        "Titanic Kaggle Challenge is a competition where you'll use data to predict who could've survived the infamous Titanic disaster.\n",
        "\n",
        "Classification \"survived\" or \"not survived\"\"\n",
        "\n",
        "<img src='https://raw.githubusercontent.com/SirTurtle/ML-BME-UofA-imgs/main/Week-3-Classification-models/imgs/Kaggle-Titanic-Project-Getting-Started.png' width=700px align=\"center\">\n",
        "\n",
        "Let's explore machine learning with a fun example from Kaggle, a competition site owned by Google. These contests can be for giggles, cash prizes, or even a job offer sometimes! The Titanic Kaggle Challenge is known as one of the classic examples for learning classification in a hands-on way.\n",
        "\n",
        "One beginner's challenge is based on the Titanic disaster, a famous shipwreck that happened on April 15, 1912. The ship, thought to be unsinkable, hit an iceberg and sank, sadly causing 1502 out of 2224 people onboard to lose their lives because there weren't enough lifeboats.\n",
        "\n",
        "But it's not just about guessing who made it. It's about deeply exploring the data, finding patterns, and understanding how different factors might have affected survival rates. It poses questions like 'Did socioeconomic status influence survival rates?' and 'What was the impact of the \"women and children first\" policy? Was the 'women and children first' policy strictly followed?'\n",
        "\n",
        "Here's the interesting part: it seems that some people were more likely to survive than others. The challenge asks us to figure out who these folks were, using data like their names, ages, genders, and social classes.\n",
        "\n",
        "We get a file with details about 891 passengers, including whether they survived or not. We'll use this data to teach our machine to make smart guesses.\n",
        "\n",
        "But the real test comes with another file, this one has information on 418 passengers, but doesn't tell us if they survived. That's where our machine's predictions come in!\n",
        "\n",
        "Suggested tutorial about Kaggle's titanic challange: [Titanic - Machine Learning from Disaster](https://www.kaggle.com/competitions/titanic)\n",
        "\n",
        "You can follow Chris White's tutorial on this subject through this Jupyter notebook: [01-intro-classification.ipynb](https://colab.research.google.com/github/ualberta-rcg/python-machine-learning/blob/main/notebooks/01-intro-classification.ipynb#scrollTo=Gd80ekh_zQu4)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7f0b1e67-92c6-4407-9fcb-7c36d7278eac",
      "metadata": {
        "id": "7f0b1e67-92c6-4407-9fcb-7c36d7278eac"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.4"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}