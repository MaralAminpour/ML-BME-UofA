{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 5: Explore Kernel SVC\n",
    "\n",
    "In this notebook we will review concepts of kernel support vector classifier and demostrate them in scikit-learn.\n",
    "We will do that by fitting Gaussian Kernel SVC to a simulated dataset.\n",
    "\n",
    "### Dataset\n",
    "\n",
    "The dataset consist of 9 1D feature vectors, with labels -1 and 1. Run the code below to create and plot the dataset. \n",
    "\n",
    "Note function `PlotData`, you will need it later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# SIMULATED DATASET\n",
    "# feature vector\n",
    "x = np.linspace(-4,4,9).reshape(-1,1)\n",
    "# all samples have zero y coordinate\n",
    "x0 = np.zeros(9)\n",
    "# Label lector consists of -1 and 1 labels\n",
    "y = -np.ones(9)\n",
    "y[3:6]=1\n",
    "\n",
    "# PLOT DATA\n",
    "def PlotData():\n",
    "    \n",
    "    # plot zero axis\n",
    "    plt.plot(x,x0,'k--', alpha=0.75)\n",
    "    \n",
    "    # plot dataset with labels shown by different markers \n",
    "    plt.plot(x[y==-1],x0[y==-1],'bo',alpha = 0.75, markeredgecolor='k', label = 'y=-1')\n",
    "    plt.plot(x[y==1],x0[y==1],'rd',alpha = 0.75, markeredgecolor='k', label = 'y=1')\n",
    "    \n",
    "    # annotate the plot\n",
    "    plt.legend(loc = 'upper right')\n",
    "    plt.title('Simulated dataset', fontsize = 16)\n",
    "    plt.xlabel('Feature', fontsize = 12)\n",
    "    plt.ylabel('Decision function', fontsize = 12)\n",
    "    \n",
    "PlotData()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1: Decision function\n",
    "\n",
    "Next, let's fit the `SVC` model with a Gaussian kernel and plot the decision function. Decision function can be predicted directly from the fitted `SVC` model.\n",
    "\n",
    "Complete the code and the function `PlotDecisionFunction` to do that. Note that the decision function is non-linear. You can use values `C=1e5` and `gamma=0.1`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "def PlotDecisionFunction():\n",
    "    \n",
    "    # create feature space\n",
    "    Feature_space = np.linspace(-4,4,100).reshape(-1,1)\n",
    "    # predict decision function\n",
    "    df = None\n",
    "    # plot decision function\n",
    "    plt.plot(None,None,'k-',label='Decision function', linewidth = 3)\n",
    "\n",
    "# parameters\n",
    "C=1e5\n",
    "gamma = 0.1\n",
    "    \n",
    "# create the model\n",
    "model = SVC(kernel='rbf', C=C, gamma=gamma)\n",
    "\n",
    "# fit the model\n",
    "\n",
    "\n",
    "# Plot decision function \n",
    "\n",
    "\n",
    "# Plot data\n",
    "PlotData()\n",
    "\n",
    "# annotate\n",
    "plt.title('Decision function', fontsize = 16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to understand how this decision function is created, let's recall from the lecture how the decision function $h(x)$ is expressed using the **dual representation** $\\mathbf{a}=(a_1,...,a_N)$:\n",
    "$$h(x)=\\sum_{i,a_i>0}a_iy_i\\kappa(x,x_i)+b$$\n",
    "Non-zero elements $a_i$ of the dual representation $\\mathbf{a}$ correspond the **support vectors** $x_i$. Labels $y_i$ take values -1 or 1, $\\kappa(x,x_i)$ is a kernel (in our case Gaussian) placed around the support vectors and $b$ is the intercept. Let's now recreate the decision function ourselves from the fitted SVC model that we have already created and is now stored in variable `model`.\n",
    "\n",
    "### Task 2: Support vectors\n",
    "\n",
    "The support vectors of the fitted `SVC` model can be found using `support_vectors_`. Note that it is a 2D array. \n",
    "\n",
    "Run the cell to see the support vectors. Note that they are all non-zero and they are fewer than the number of datapoints. How many do we have?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print support vectors. Printing transposed vector.\n",
    "print('Support vectors: ', model.support_vectors_.T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The indices of the support vector are in `support_` which is again a 2D array. Note that they are not in order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print indices of support vectors. Printing transposed vector.\n",
    "print('Indices of support vectors: ', model.support_.T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are now ready to plot our support vectors $x_i$. Complete the function `PlotSupportVectors` and the other code to do that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def PlotSupportVectors():\n",
    "    # support vectors\n",
    "    sv = None\n",
    "    # array of zeros of the same shape\n",
    "    sv0 = np.zeros(model.support_vectors_.shape)\n",
    "    # plot support vectors\n",
    "    plt.plot(None,sv0,'ro', label = 'Support vectors', markersize = 15, alpha = 1, markeredgecolor='k',c='#FFAAAA')\n",
    "    \n",
    "# plot support vectors\n",
    "\n",
    "\n",
    "# plot data\n",
    "PlotData()\n",
    "\n",
    "# annotate\n",
    "plt.title('Support vectors', fontsize = 16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 3: Gaussian kernels\n",
    "\n",
    "The decision function is a linear combination of the kernels $\\kappa(x,x_i)$ placed around the support vectors $x_i$ for which $a_i>0$. We will now plot these kernels.\n",
    "\n",
    "In the cell below there is the function `GaussianKernel` that returns values of $\\kappa(x,x_i)$ for a vector $x$. Run the cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a function to calculate the kernel is prepared for you\n",
    "from scipy.stats import norm\n",
    "def GaussKernel(f,mu,sigma):\n",
    "    kernel = norm.pdf((f-mu)/sigma)\n",
    "    kernel = kernel/norm.pdf(0)\n",
    "    return kernel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will plot the kernels placed around the support vectors. Complete the code below to do that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def PlotKernels():\n",
    "    # number of support vectors\n",
    "    n = model.support_vectors_.shape[0]\n",
    "    \n",
    "    # sigma\n",
    "    sigma = np.sqrt(1/(2*gamma))\n",
    "    \n",
    "    # create feature space\n",
    "    Feature_space = np.linspace(-4,4,100).reshape(-1,1)\n",
    "    \n",
    "    # loop over support vectors to generate kernels\n",
    "    for i in range(n):\n",
    "        # evaluate kernel for the whole feature space\n",
    "        kernel = GaussKernel(Feature_space,model.support_vectors_[i],sigma)\n",
    "        # plot kernel\n",
    "        plt.plot(None,None,'g--')\n",
    "        # connecting line to the corresponding support vector\n",
    "        plt.plot([model.support_vectors_[i],model.support_vectors_[i]],[0,1],'k:', alpha=0.5)\n",
    "        \n",
    "# Plot kernels\n",
    "\n",
    "\n",
    "# Plot support vectors\n",
    "PlotSupportVectors()\n",
    "\n",
    "# Plot data\n",
    "PlotData()\n",
    "\n",
    "# annotate\n",
    "plt.ylim([-0.1,1.5])\n",
    "plt.title(r'Kernels $\\kappa(x,x_i)$', fontsize = 16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 4: Dual representation\n",
    "\n",
    "Dual representation is stored in the fitted `SVC` model as `dual_coef_`. In fact its elements correspond to $a_iy_i$ for the support vectors only ($a_i>0$)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Dual coefficients: ', np.around(model.dual_coef_,1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The intercept $b$ is stored at `model.intercept_`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Intercept: ',np.around(model.intercept_,1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can plot dual representation by putting support vectors on x axis and the dual coefficient of y axis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def PlotDualCoef():\n",
    "    # support vectors\n",
    "    sv = None\n",
    "    # dual coeffs\n",
    "    dc = model.dual_coef_[0]\n",
    "    # plot dual coeffs\n",
    "    plt.plot(sv,None ,'g*', \n",
    "             label = 'Dual coef $y_ia_i$', markersize = 10, markeredgecolor='k')\n",
    "\n",
    "# Plot support vectors\n",
    "PlotSupportVectors()\n",
    "\n",
    "# Plot dual coeffs\n",
    "\n",
    "\n",
    "# Plot data\n",
    "PlotData()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 5: My own decision function\n",
    "\n",
    "The decision function is evaluated as sum over all support vector indices $i$ of $a_iy_i$ times $\\kappa(x,x_i)$ with the intercept $b$ added to it as well.\n",
    "\n",
    "Complete the code below to evaluate and plot the decision function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MyDecisionFunction():\n",
    "    # number of support vectors\n",
    "    n = model.support_vectors_.shape[0]\n",
    "    \n",
    "    # sigma\n",
    "    sigma = np.sqrt(1/(2*gamma))\n",
    "    \n",
    "    # create feature space\n",
    "    Feature_space = np.linspace(-4,4,100).reshape(-1,1)\n",
    "    \n",
    "    # Initialise decision function as zero vector\n",
    "    df = np.zeros(Feature_space.shape)\n",
    "    \n",
    "    # loop over support vectors to \n",
    "    # 1. generate kernels\n",
    "    # 2. multiply them with dual coefficients\n",
    "    # 3. add them to the decision function\n",
    "    \n",
    "    for i in range(n):\n",
    "        # evaluate kernel for the whole feature space\n",
    "        kernel = None\n",
    "        # multiply by dual coeficient\n",
    "        kernel = kernel*model.dual_coef_[0][i]\n",
    "        # add to the decision function\n",
    "        df = df+kernel\n",
    "    \n",
    "    # add intercept\n",
    "    df = df + None\n",
    "    \n",
    "    # plot\n",
    "    plt.plot(Feature_space,df,'r', label = 'Decision function')\n",
    "\n",
    "    \n",
    "# figure\n",
    "plt.figure(figsize = (8,8))\n",
    "\n",
    "# plot my own decision function\n",
    "\n",
    "\n",
    "# Plot support vectors\n",
    "PlotSupportVectors()\n",
    "\n",
    "# Plot dual coefs\n",
    "PlotDualCoef()\n",
    "\n",
    "# Plot data\n",
    "PlotData()\n",
    "\n",
    "# annotate \n",
    "plt.title('My own decision function', fontsize = 16)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
