{"cells":[{"cell_type":"markdown","metadata":{"id":"gtMaE1Qog7cJ"},"source":["# Feature Selection\n","In this notebook, we'll revisit the topic of brain structure volumes in preterm babies. Specifically, we'll explore how feature selection can be a powerful tool to prevent overfitting, enhance model performance, and aid in interpreting features.\n","\n","Feature selection can often make the difference between a model that performs well and one that doesn't. By selecting the right features, we can enhance our model's ability to understand and learn from our data.\n","\n","Let's get started and see feature selection in action!"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"EJvw_8Ghg7cL"},"outputs":[],"source":["###################################\n","## RUN THIS\n","###################################\n","# this code is to suppress warnings\n","import warnings\n","from sklearn.exceptions import ConvergenceWarning\n","warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n","warnings.filterwarnings(\"ignore\", category=ConvergenceWarning)\n","warnings.filterwarnings(\"ignore\", category=FutureWarning)\n","####################################"]},{"cell_type":"markdown","metadata":{"id":"OJdAItSBg7cM"},"source":["## Brain structure volumes\n","\n","let's get back into working with our dataset of 86 brain structure volumes from 164 preterm babies. If you recall, our goal was to predict the gestational age (GA) from the volumes. You might remember that using Multivariate Linear Regression resulted in overfitting of the data, and we used Lasso and Ridge penalties to combat this.\n","\n","### Load data\n","\n","The code below will help you get started. It takes care of loading the data, creating the feature matrix and the target vector, and performing feature scaling. Remember, feature scaling is an essential step when working with machine learning models as it ensures all features contribute equally to the model's performance. Let's continue!"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"s4lkY8_ng7cM"},"outputs":[],"source":["import pandas as pd\n","import numpy as np\n","from sklearn.preprocessing import StandardScaler\n","\n","# read spreadsheet using pandas\n","data = pd.read_csv(\"datasets/GA-structure-volumes-preterm.csv\",header=None)\n","# convert from 'DataFrame' to numpy array\n","structure_volumes = data.to_numpy()\n","# Features\n","X = structure_volumes[:,1:]\n","# Targets\n","y = structure_volumes[:,0]\n","# checking the size of the feature and target arrays\n","# note they must agree in the first dimension\n","print('Features shape: {}; Targets shape: {}'.format(X.shape,y.shape))\n","# we have 86 features and 164 samples\n","\n","# Scale features\n","X = StandardScaler().fit_transform(X)\n","print('Performed feature scaling.')"]},{"cell_type":"markdown","metadata":{"id":"Vj4L-Uqxg7cN"},"source":["It's often incredibly insightful to identify which features our models consider most predictive. It helps us understand the underlying patterns in our data better and can guide future data collection or feature engineering efforts.\n","\n","The code below will assist in achieving this objective. It reads in the names of the brain structures and stores them in a `dataframe` object named `structure_names`. So, not only will we know how many features are considered important, but we'll also know exactly which ones they are! Let's move on."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1bdYUEZcg7cN"},"outputs":[],"source":["# read file with structure names\n","structure_names = pd.read_csv('datasets/labels', header = None, sep='\\t')\n","structure_names[1]"]},{"cell_type":"markdown","metadata":{"id":"EDxH8ZsRg7cN"},"source":["### Multivariate linear regression\n","\n","As you'll remember, Multivariate Linear Regression tended to overfit the data when applied to our problem. Overfitting happens when a model is too complex and captures the noise in the data rather than the underlying pattern. This results in great performance on the training data but poor generalization to unseen data.\n","\n","Now, let's take a look at the performance of this linear regression model to set a baseline. As we progress, we'll be able to compare this with our models after applying feature selection, to clearly see any improvements made. Ready to continue? Let's go!"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Q5aZbGcWg7cO"},"outputs":[],"source":["from sklearn.linear_model import LinearRegression\n","from sklearn.model_selection import cross_val_score\n","\n","model = LinearRegression()\n","scores = cross_val_score(model, X, y, scoring = 'neg_mean_squared_error')\n","print('Linear regression: Cross-validated RMSE is ', round(np.sqrt(-scores.mean()),2))"]},{"cell_type":"markdown","metadata":{"id":"OgCFh_Wbg7cO"},"source":["Alright, ready for a little flashback? We discovered earlier that the magic alpha number for Ridge regression was about 45. Using this setting helped us dodge a lot of that pesky overfitting.\n","\n","### Ridge\n","\n","Let's rerun this Ridge regression model. Keep in mind the performance of Ridge regression, as it will serve as a good baseline for us to compare with future results. Remember, Ridge regression adds a penalty equivalent to the square of the magnitude of the coefficients to the loss function, which helps prevent overfitting by constraining the model.\n","\n","This setting significanlty reduced overfitting. Let's rerun this model. Remember the performance of Ridge regression as a baseline for good performance."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wWMP92MGg7cO"},"outputs":[],"source":["from sklearn.linear_model import Ridge\n","\n","model = Ridge(alpha = 45)\n","scores = cross_val_score(model, X, y, scoring = 'neg_mean_squared_error')\n","print('Ridge regression: Cross-validated RMSE is ', round(np.sqrt(-scores.mean()),2))"]},{"cell_type":"markdown","metadata":{"id":"llDDDfAug7cO"},"source":["Fantastic! We will now explore different feature selection techniques in Scikit-learn.\n","\n","Feature selection is like a filter that sifts out all the redundant or less meaningful stuff from our data, letting the truly valuable features shine through. It's a key step to avoid overfitting, reduce complexity, and improve our model's performance.\n","\n","Alright, are you ready? Let's unravel the potential of different feature selection techniques together!\n","\n","## Univatiate feature selection\n","\n","### Pearson's correlation coefficient\n","\n","The Pearson correlation coefficient is a great way to understand the linear relationship between our features and the target variable. This function (`pearsonr`) from the `scipy.stats` module helps us do exactly that.\n","\n","The correlation coefficient ranges from -1 to 1. A high absolute value (close to -1 or 1) means there's a strong linear relationship. This could be either a positive relationship (as one value goes up, so does the other) or a negative relationship (as one value goes up, the other goes down).\n","\n","Keep in mind that while many brain volumes have high correlation with gestational age (GA), not all of them do. This is where feature selection can be particularly handy, helping us focus on those features that contribute the most to predicting GA.\n","\n","So, ready to explore more about these relationships? Let's keep going!"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xWN2M6Ygg7cP"},"outputs":[],"source":["import matplotlib.pyplot as plt\n","%matplotlib inline\n","\n","from scipy.stats import pearsonr\n","\n","n = X.shape[1]\n","cc = np.zeros(n)\n","for i in range(n):\n","    cc[i]=pearsonr(X[:,i],y)[0]\n","\n","plt.figure(figsize = [16,4])\n","plt.bar(np.arange(n),cc)\n","plt.title('Pearsons correlation coefficient', fontsize = 18)\n","plt.xlabel('Feature', fontsize = 16)\n","plt.ylabel('Correlation coefficient', fontsize = 16)\n","plt.axis([-1,86,0,1])"]},{"cell_type":"markdown","metadata":{"id":"d9lgafi6g7cP"},"source":["### F-score\n","\n","The F-score is another useful statistic when it comes to feature selection.\n","\n","Just to clarify, Scikit-learn tends to work with F-values, but don't worry, for feature selection, they're just as effective as Pearson's Correlation Coefficient.\n","\n","The cool part about F-values is that they can be calculated directly using the `f_regression` function in `sklearn` from Scikit-learn.\n","\n","**Activity 1.1:** Your task now is to complete the code below to create a `bar ` plot of the F-scores. This plot will give you a good idea of how the F-values of your features are distributed. Ready to take it on? You've got this!"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3fuylvKXg7cP"},"outputs":[],"source":["from sklearn.feature_selection import f_regression\n","\n","f_score = f_regression(X,y)[0]\n","\n","# plot f-scores\n","plt.figure(figsize = [16,4])\n","plt.bar(np.arange(n),None)\n","plt.title('F-value', fontsize = 18)\n","plt.xlabel('Feature', fontsize = 16)\n","plt.ylabel('F-value', fontsize = 16)"]},{"cell_type":"markdown","metadata":{"id":"AJSFJsxjg7cP"},"source":["**Activity 1.2:** Your task now is to plot the relationship between Pearson's correlation coefficient and the F-score using `plot`. By creating this plot, we can better understand the relationship between these two metrics. Ready to uncover their relationship? Let's jump right in!"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"myw5SHEfg7cP"},"outputs":[],"source":["# plot relationship\n","plt.plot(None,None,'*')\n","plt.xlabel(\"Person's correlation coefficient\", fontsize = 16)\n","plt.ylabel('F-value', fontsize = 16)"]},{"cell_type":"markdown","metadata":{"id":"JtXqtT-og7cP"},"source":["### Selecting features based on F-value\n","<img src=\"pictures/brain.png\" width = \"250\" style=\"float: right;\">\n","\n","Great, let's start refining our model by selecting the most impactful features.\n","\n","### Selecting Top Scoring Features\n","Scikit-learn has some handy tools to make this process easier, specifically `SelectKBest` and `SelectPercentile`. To start, we're going to use `SelectKBest` to pick out the top 4 features.\n","\n","Now, you'll notice some code below. It's transforming our original feature matrix `X` into a new one, `X_selected`, which will only include our top 4 selected features. Neat, huh?\n","\n","**Activity 1.3:** Now, let's make sure everything's gone to plan. Check the following:\n","\n","- Size of the new matrix - does it match what you expect, considering we're selecting 4 features?\n","\n","- Indices of the features that have been selected - these will tell us which features from the original matrix have been chosen.\n","\n","- Names of the selected features - because it's always nice to know who made the cut!"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vgrk_6hXg7cP"},"outputs":[],"source":["from sklearn.feature_selection import SelectKBest\n","\n","# define feature selection model\n","k=4\n","selector = SelectKBest(f_regression, k = k)\n","\n","# select features\n","X_selected = selector.fit_transform(X,y)\n","\n","# Shape of the matrix\n","print('Shape of the new matrix: ', X_selected.shape)\n","\n","# Indices of the selected features\n","ind = np.where(selector.get_support())[0]\n","print('Indices: ', ind)\n","\n","# Print the names of the selected structures\n","print('\\n')\n","for i in range(k):\n","    print(structure_names.loc[ind[i],1])"]},{"cell_type":"markdown","metadata":{"id":"-ziGPfLSg7cP"},"source":["### Univariate feature selection for improved prediction\n","\n","Perfect, now let's see how these selected features fare when we use them for our multivariate linear regression.\n","\n","**Activity 1.4:** Here's your next mission - apply multivariate linear regression to our carefully selected features and let's see if our performance improves.\n","\n","Remember, the goal of feature selection is to enhance our model's performance by using only the most relevant features. By reducing the 'noise' from less important features, we're hoping for a more effective model."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2PXgYCi6g7cQ"},"outputs":[],"source":["# Select and fit linear regression model to selected features\n","model = None\n","model.fit(None,y)\n","\n","# Calculate and print RMSE\n","scores = cross_val_score(model, X_selected, y, scoring = 'neg_mean_squared_error')\n","print('Linear regression: Cross-validated RMSE is ', round(np.sqrt(-scores.mean()),2))"]},{"cell_type":"markdown","metadata":{"id":"_j1ler-yg7cQ"},"source":["Alright, it seems like we've managed to reduce overfitting, which is fantastic! However, our model's performance still doesn't match that of the Lasso or Ridge methods.\n","\n","### Tweaking the Number of Selected Features\n","\n","**Activity 5:** But don't worry, we've got one more trick up our sleeve. It's time to experiment with the number of selected features. Adjust this number to find out what delivers the best performance.\n","\n","Keep in mind that the right balance might not always be the maximum number of features – sometimes, less is more!\n","\n","Once you've found that sweet spot, make a note of it. This is now the benchmark for the best performance we can get from univariate feature selection.\n","\n","Ready to find that perfect number? Let's get experimenting!"]},{"cell_type":"markdown","metadata":{"id":"h9ZdfA88g7cQ"},"source":["## Exercise 1\n","\n","Select 4 top scoring features using mutual information. Do you obtain the same or different features as for correlation coefficient?\n","\n","### Mutual Information for Feature Selection\n","Mutual information can provide a deeper understanding of the relationship between features, as it captures any kind of dependency, not just linear.\n","\n","Now, let's select the 4 top scoring features using mutual information and see how it compares to our previous method.\n","\n","**Activity:** Your task is to run the mutual information feature selection and compare the selected features with those chosen based on the correlation coefficient.\n","\n","Remember, the same features may not always be chosen by different selection methods. That's what makes this so interesting!\n","\n","Ready to uncover the mutual information in our dataset? Let's do it!"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"if6gZ1h0g7cQ"},"outputs":[],"source":["from sklearn.feature_selection import mutual_info_regression\n","\n","# set number of features to select\n","k=4\n","\n","# Create feature selector\n","selector = None\n","\n","# select features\n","X_selected = None\n","\n","# Indices of the selected features\n","ind = None\n","\n","# Print the names of the selected structures\n","print('\\n')\n","for i in range(k):\n","    print(structure_names.loc[ind[i],1])"]},{"cell_type":"markdown","metadata":{"id":"6PV0QCC2g7cQ"},"source":["## Model based feature selection\n","\n","### Lasso\n","\n","We will now select the features based on `Lasso` model. We have previously found that setting `alpha=0.16` results in a best Lasso model for our example. Code below creates the model, calculates its performance and prints out the number of sparse coefficients.\n","\n","Now, we're moving to another type of feature selection, where we'll use models to guide our decisions. To start with, let's revisit our friend, the Lasso model.\n","\n","### Model-Based Feature Selection: Lasso\n","We've previously found that setting alpha=0.16 resulted in an optimal Lasso model for our dataset. The code provided below will take us through creating this model, assessing its performance, and checking out the number of sparse coefficients.\n","\n","**Activity:** Your task here is to run the provided code and observe the model's performance. Note the number of sparse coefficients - these represent the features Lasso considers irrelevant. They're what makes Lasso such a handy tool for feature selection!\n","\n","Ready to let Lasso guide us towards the most important features? Let's get started!"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DqT-IM1yg7cQ"},"outputs":[],"source":["from sklearn.linear_model import Lasso\n","\n","model = Lasso(alpha=0.16)\n","scores = cross_val_score(model, X, y, scoring = 'neg_mean_squared_error')\n","print('Lasso regression: Cross-validated RMSE is ', round(np.sqrt(-scores.mean()),2))\n","\n","model.fit(X,y)\n","print('\\n Non-zero coefficients')\n","print(model.sparse_coef_)\n","print('\\n There are {} non-zero coefficients.'.format(model.sparse_coef_.count_nonzero()))"]},{"cell_type":"markdown","metadata":{"id":"2NAm7wLNg7cQ"},"source":["Fantastic! Now, we're going to dig a little deeper into the results from our Lasso model.\n","\n","### Digging into Lasso's Decisions\n","The following code snippet will help us identify exactly which features Lasso deemed important. We'll find out the indices of non-zero Lasso coefficients, which correspond to the selected features.\n","\n","After this, we'll also pull out the names of the selected structures for a clear view of what Lasso suggests we focus on.\n","\n","**Activity:** Just run the following code and let's uncover the features our Lasso model has selected!"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zAs_mQw8g7cQ"},"outputs":[],"source":["# indices of non-zero elements\n","ind = model.sparse_coef_.nonzero()[1]\n","print('Indices of non-zero elements: ', ind)\n","print('\\n')\n","\n","# print names of selected structures\n","print('Selected structures: \\n')\n","for i in range(ind.size):\n","    print(structure_names.loc[ind[i],1])"]},{"cell_type":"markdown","metadata":{"id":"XvvRCtQng7cQ"},"source":["## Exercise 2\n","\n","## **Exercise 2:** Combining LassoCV and Linear Regression\n","In this exercise, we'll combine the strengths of `LassoCV` and `LinearRegression`. We'll use the `LassoCV` model for feature selection, and then we'll use `LinearRegression` to make predictions using the selected features.\n","\n","Here's a step-by-step breakdown of your tasks:\n","\n","- Implement feature selection using the SelectFromModel selector, and choose the LassoCV model.\n","- With the selected features, calculate the performance of a `LinearRegression` model.\n","- Finally, experiment with different thresholds for Lasso coefficients to see which value gives us the best performance.\n","- Remember, you're aiming for a good balance between the number of features selected and the model's performance. Too few features and the model may not perform well; too many and you risk overfitting.\n","\n","Ready to fine-tune your feature selection skills? Let's go!"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"XEaUhN83g7cQ"},"outputs":[],"source":["from sklearn.linear_model import LassoCV\n","from sklearn.feature_selection import SelectFromModel\n","\n","# Create selector with LassoCV model\n","selector = None\n","\n","# Perform feature transformation\n","X_selected = None\n","\n","# Create and fit linear regression model to selected features\n","model = None\n","model.fit(None,y)\n","\n","# Calculate and print RMSE\n","scores = cross_val_score(model, None, y, scoring = 'neg_mean_squared_error')\n","print('Linear regression: Cross-validated RMSE is ', round(np.sqrt(-scores.mean()),2))\n","\n","# List the number and names of the selected features\n","ind = None\n","print('\\nSelected {} features: '.format(ind.size),)\n","for i in range(ind.size):\n","    print(structure_names.loc[ind[i],1])"]},{"cell_type":"markdown","metadata":{"id":"MCoj-yWng7cQ"},"source":["when we employ feature selection using `LassoCV `with an optimised threshold, the performance of the Linear Regression model reaches levels similar to the optimised `Ridge` regression. That's quite impressive!\n","\n","This outperforms the results we got when using univariate feature selection. So what have we learned? Well, this essentially shows us that using model-based feature selection methods like `LassoCV` can help us achieve more accurate results by intelligently deciding which features contribute the most to our predictions.\n","\n","Keep going! You're doing wonderfully, and your understanding of these techniques is getting better with each step. Let's move on to the next part."]},{"cell_type":"markdown","metadata":{"id":"sFo8_Ck6g7cQ"},"source":["### Random forest\n","\n","So let's roll up our sleeves and get our hands dirty with some Random Forest modeling. Run the following cell to train the Random Forest regressor and assess its performance on our data. Don't worry if you don't grasp all the details right now; just try to understand the big picture!\n","\n","**Note:**\n","One of the great things about Random Forests is that they are highly resilient to overfitting, which can be a common issue with other models. This is largely due to their ensemble nature—by aggregating the results of many different trees, they're able to maintain robust performance even when some individual trees may overfit the data.\n","\n","On top of this, Random Forests also have the ability to model complex, non-linear relationships, giving them a leg up over linear regression models in certain scenarios."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"c5pOjP5Ag7cR"},"outputs":[],"source":["from sklearn.ensemble import RandomForestRegressor\n","\n","# Select and fit the model\n","model = RandomForestRegressor(n_estimators=20)\n","\n","# Calculate CV RMSE\n","scores = cross_val_score(model, X, y, scoring = 'neg_mean_squared_error')\n","print('Cross-validated RMSE is ', round(np.sqrt(-scores.mean()),2))"]},{"cell_type":"markdown","metadata":{"id":"iEbq1btdg7cR"},"source":["**Activity 2.1:** Feature importances can be access as `model.feature_importances_`. Plot the feature importances using a `bar` plot.\n","\n","\n","---\n","We're going to dive into one of the coolest features of Random Forest models, the **feature importances**. These importances provide an easy-to-understand ranking of which features of the model considers most useful in making its predictions.\n","\n","To do this, we're going to call upon `model.feature_importances_`. This will return an array where each number represents the importance of a feature. Higher numbers mean the feature is more important to the model's decision-making process.\n","\n","And what better way to visualize this data than with a `bar` plot? The length of each bar will show us the relative importance of each feature in a very intuitive manner.\n","\n","Go ahead and run the following cell to complete this activity. As you look over the results, consider what insights you can gain from this visualization. Are there any surprises? Any features that are more or less important than you expected?"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"K2hPdPSxg7cR"},"outputs":[],"source":["# fit the model\n","model.fit(X,y)\n","\n","# plot feature importances\n","plt.figure(figsize = [16,4])\n","n = X.shape[1]\n","plt.bar(np.arange(n),None)\n","plt.title('Feature importances', fontsize = 18)\n","plt.xlabel('Features', fontsize = 16)\n","plt.ylabel('importances', fontsize = 16)"]},{"cell_type":"markdown","metadata":{"id":"0NkQ3wyzg7cR"},"source":["**Activity 2.2:** Use selector `SelectFromModel` to select the features from `RandomForestRegressor(n_estimators=20)`. Choose threshold 0.05 and print the names of the selected features. Are they consistent with the ones selected by Lasso or Correlation Coefficient?\n","\n","\n","---\n","\n","\n","Alright, now we're going to take our exploration of the Random Forest model a step further by selecting specific features using the `SelectFromModel` function from `sklearn`.\n","\n","`RandomForestRegressor(n_estimators=20)`\n","\n","This is a powerful method for feature selection that uses the weights of your model's features to choose the most important ones. We'll use it with our Random Forest model to identify the features that have the most impact on our predictions.\n","\n","To do this, we will set a **threshold of 0.05**. This means that any feature with a importance score less than this number will not be selected. Once we've selected these features, we'll print their names to see which ones made the cut.\n","\n","Then, we'll compare the features selected by the Random Forest model to those selected by the `Lasso` model and Correlation Coefficient. This will give us a broader understanding of which features are consistently identified as significant across different models.\n","\n","Let's get to it! Run the cell below to execute this activity."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2JI8lCDxg7cR"},"outputs":[],"source":["# Create selector with LassoCV model\n","selector = None\n","\n","# Perform feature transformation\n","X_selected = selector.fit_transform(X, y)\n","\n","# List the number and names of the selected features\n","ind = selector.get_support(indices=True)\n","print('Selected {} features: '.format(ind.size),)\n","for i in range(ind.size):\n","    print(structure_names.loc[ind[i],1])"]},{"cell_type":"markdown","metadata":{"id":"fM5zGf3Kg7cR"},"source":["## Recursive feature elimination\n","\n","Scikit learn offers functions `RFE` and `RFECV` to perform recursive feature elimination. Any model can be used to do that, and this time we will chose `Ridge` regression.  \n","\n","Let's first find 6 best features using `RFE` with `Ridge`. Run the code below to fit `RFE` model and print the names of the selected features.\n","\n","\n","---\n","\n","\n","let's continue our journey in feature selection with a technique called Recursive Feature Elimination (RFE). This method works by fitting a model and removing the weakest feature (or features) until the specified number of features is reached.\n","\n","What makes RFE powerful is that it takes advantage of the model to identify which features (or combinations of features) contribute the most to predicting the target variable.\n","\n","For this particular task, we will use the `RFE `function from `sklearn` along with the `Ridge` regression model. We will aim to find the top 6 features. Once we've selected these features, we'll print out their names to see which ones were selected.\n","\n","Excited to find out which features are most important according to `RFE?` Run the cell below and let's see the result!"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"W8-R75mNg7cR"},"outputs":[],"source":["from sklearn.feature_selection import RFE\n","\n","k=6\n","\n","# create ranking model\n","model = Ridge(alpha=45)\n","\n","# create selector\n","selector = RFE(model, n_features_to_select=k)\n","\n","# fit selector\n","selector.fit(X,y)\n","\n","# Print the indices of the selected features\n","ind = np.where(selector.get_support())[0]\n","print('Indices: ', ind)\n","\n","# Print the names of the selected structures\n","print('\\n')\n","for i in range(k):\n","    print(structure_names.loc[ind[i],1])"]},{"cell_type":"markdown","metadata":{"id":"TigfIJLyg7cR"},"source":["**Activity 3.1:** Transform the features, fit linear regression and calculate CV RMSE to see whether we reduced overfitting.\n","\n","\n","---\n","\n","Now we're diving into Activity 3.1 - the plot thickens! This part of our journey involves transforming the features, strapping on a linear regression model and calculating the Cross-Validation Root Mean Squared Error (CV RMSE). Exciting, isn't it?"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"LXw_Lsi3g7cV"},"outputs":[],"source":["# Select features\n","X_selected = None\n","\n","# Linear regression\n","model = None\n","scores = cross_val_score(model, X_selected, y,scoring = 'neg_mean_squared_error')\n","print('Linear regression: Cross-validated RMSE is ', round(np.sqrt(-scores.mean()),2))"]},{"cell_type":"markdown","metadata":{"id":"2DOPuWQfg7cV"},"source":["## Exercise 3\n","Let's now use method `RFECV` that can also automatically select optimal number of features using cross-validation. Write the code to:\n","* Fit the `RFECV` feature selection with `Ridge(alpha=45)` ranking model\n","* Transform the features and fit the `Ridge(alpha = 45)` model to the selected features\n","* Calculate the CV RMSE\n","* Print indices of selected features\n","* Print number of selected features\n","\n","\n","---\n","\n","\n","Time for Exercise 3! We're stepping up our game now. We'll be using `RFECV`, a really cool method that not only performs feature elimination but also picks the optimal number of features using cross-validation. Neat, huh?\n","\n","Alright, let's break this down into bite-sized tasks:\n","\n","1. First up, we'll fit `RFECV` with our `Ridge(alpha=45)` model. It's like pairing a dynamic duo ready to rank our features.\n","\n","2. Then, we'll transform the features and fit them back to the `Ridge(alpha = 45)` model. It's like giving our features a new look and seeing how they perform in the Ridge model's spotlight.\n","\n","3. Up next, we'll calculate the Cross-Validation Root Mean Squared Error (CV RMSE). It's like our trusty measuring tape to see how well our model is doing.\n","\n","4. After that, let's print out the indices of our selected features - like shining a spotlight on our all-star features!\n","\n","5. Finally, we'll print out the number of features that were selected. It's like doing a headcount of our all-star features.\n","\n","I bet you're as excited as I am to see what we discover. So, let's dive in and get our hands dirty with some coding! You've got this!\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"UyeiCOvkg7cW"},"outputs":[],"source":["from sklearn.feature_selection import RFECV\n","\n","# create ranking model\n","model = None\n","\n","# Create selector\n","selector = None\n","\n","# Fit the selector and transform the features\n","X_selected = None\n","\n","# Calculate performace of Ridge with selected features\n","scores = None\n","print('Linear regression: Cross-validated RMSE is ', round(np.sqrt(-scores.mean()),2))\n","\n","# Print indices of the selected features\n","ind = None\n","print('Indices: ', ind)\n","\n","# Print number of selected features\n","print('Number of selected features: ', ind.size)"]},{"cell_type":"markdown","metadata":{"id":"CYp2Qpnyg7cW"},"source":["### Recursive feature elimination using Random Forest\n","\n","**Activity 3.2:** Perform recursive feature elimination using `RFECV` and `RandomForestRegressor(n_estimators=20)`. Be patient, this process might take time.\n","\n","\n","---\n","\n","We're now moving onto Activity 3.2, a more adventurous task. Here we'll perform recursive feature elimination but this time, we're bringing in the big guns - `RFECV` and `RandomForestRegressor(n_estimators=20)`. This is like assembling a superhero team for feature selection!\n","\n","Now, a heads-up - the process might take a bit more time than usual. You know how it goes, the Random Forest algorithm can be quite a powerhouse and takes its time to churn through the data. But remember, all good things come to those who wait!\n","\n","So, while the code runs, grab yourself a cup of coffee, or perhaps plan out your next coding adventure. Trust me, the insights you'll gain will be well worth the wait.\n","\n","Happy coding and patience, my friend!\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"KUsyc5L2g7cW"},"outputs":[],"source":["from sklearn.feature_selection import RFECV\n","\n","model = RandomForestRegressor(n_estimators=20)\n","selector = None\n","selector.fit(X,y)\n","\n","# Print selected features\n","ind = np.where(selector.get_support())[0]\n","print('Indices: ', ind)\n","\n","print('Number of selected features: ', ind.size)"]},{"cell_type":"markdown","metadata":{"id":"9XDrZUdpg7cW"},"source":["**Activity 3.3:** Transform the features (no need to fit the feature selector again) and fit the `RandomForestRegressor(n_estimators=20)` to see whether CV RMSE improved.\n","\n","\n","---\n","\n","Ready for Activity 3.3? We're going to keep the momentum going with some exciting transformations. This time, we're transforming the features and using our sturdy `RandomForestRegressor(n_estimators=20)`.\n","\n","We're not fitting the feature selector again. Nope, no need for that. It's already had its run and it's done a great job for us. Now we're moving on to see what the transformed features can do.\n","\n","So, the spotlight is on the Random Forest Regressor now. We're going to fit our model and then check the Cross-Validation Root Mean Squared Error (CV RMSE). I'm sure you're eager to see if the CV RMSE has improved. Fingers crossed!\n","\n","Let's dive in and continue this machine learning journey. Remember, this is about learning and having fun. And you're doing an amazing job at both. Let's rock it!"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"lSZUoNu-g7cW"},"outputs":[],"source":["# Select features\n","X_selected = None\n","\n","# Random Forest with reduced features\n","model = None\n","scores = cross_val_score(model, X_selected, y, scoring = 'neg_mean_squared_error')\n","print('Linear regression: Cross-validated RMSE is ', round(np.sqrt(-scores.mean()),2))"]},{"cell_type":"markdown","metadata":{"id":"NcpW6Ryjg7cW"},"source":["# Conclusion"]},{"cell_type":"markdown","metadata":{"id":"vccbxWy0g7cW"},"source":["We have seen that feature selection can prevent overfiting and improve performance of the model. We have also seen that Random forest is very resilient against overfitting and does not particularly benefit from feature selection in our example. On contrary, it is a very good tool for selecting features for other methods.\n","\n","We have also seen that selected features varied a lot dependent on the selection method. We therefore need to be careful when interpreting the selected features.\n","\n","\n","---\n","\n","\n","As we've seen, feature selection is like a superpower that helps us keep overfitting at bay and improves our model's performance. It's an integral part of machine learning that makes all the difference in the world.\n","\n","Interestingly, we've discovered that the Random Forest is a tough little cookie when it comes to overfitting. It's almost as if it's built a fort around itself and does not particularly need feature selection to thrive. But guess what? It still proves to be an awesome tool for selecting features for other methods. Versatility at its best, don't you think?\n","\n","Now, it's important to mention that the features selected can be quite the chameleons, changing their colors based on the selection method. It's a good reminder for us to be cautious when interpreting the selected features.\n","\n","Remember, there's no one-size-fits-all in machine learning. It's always a blend of different techniques, a bit of trial and error, and lots of learning. That's what makes it fun and fascinating!\n","\n","Keep up the great work!"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.3"},"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":0}