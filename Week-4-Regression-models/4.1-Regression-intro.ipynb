{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "57170492-140c-4d56-8e94-4a49232293ef",
   "metadata": {},
   "source": [
    "# Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6320406c-7cb4-4c24-8773-fe1aa94795f1",
   "metadata": {},
   "source": [
    "## Application: neonatal brain growth\n",
    "\n",
    "- Around the time of birth the brain grows very quickly\n",
    "- Preterm birth alters brain development\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/SirTurtle/ML-BME-UofA-imgs/main/Week-4-Regression-models/imgs/DevelopingPretermBrain.png\" width = \"300\" style=\"float: left;\">\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/SirTurtle/ML-BME-UofA-imgs/main/Week-4-Regression-models/imgs/Baby.png\" width = \"200\" style=\"float: right;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4519d6ff-afc0-4cfe-9592-ccbbe13f318e",
   "metadata": {},
   "source": [
    "We will demonstrate the regression concepts using the application of brain growth in preterm neonates. Around the time of birth the brain grows very quickly. Preterm birth can disrupt this process, and therefore preterm brain development is a subject of extensive research.\n",
    "\n",
    "To investigate the changes caused by preterm birth, we can acquire MRI scans of newborn babies. We can perform automatic segmentations of various brain structures and measure their volumes. The features in the datasets we'll be working with are brain volumes, either for (1) the whole brain, (2) six brain tissues, or (3) 86 brain structures.\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/SirTurtle/ML-BME-UofA-imgs/main/Week-4-Regression-models/imgs/BrainMRI-BrainSegmentation.png\" width = \"700\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fe8f6db-e261-45eb-8d19-1483be8b96c6",
   "metadata": {},
   "source": [
    "The machine learning regression is the predict the age (target value) of a baby from volumes of brain structures (features).\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/SirTurtle/ML-BME-UofA-imgs/main/Week-4-Regression-models/imgs/AgeVolumePlot.png\" width = \"400\">\n",
    "\n",
    "In this plot each circle corresponds to a baby (a sample), on the x-axis we have the brain volume (a feature scaled with StandardScaler). The age (target value) is plotted on the y-axis. In this case we are showing a univariate linear regression model in red, with two parameters (slope and intercept). We have achieved R2 score 0.85 which is quite good, so the linear model fits this dataset well."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "297d6c82-b7a5-4ef5-9799-6e377d0dda5d",
   "metadata": {},
   "source": [
    "## Types of errors\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/SirTurtle/ML-BME-UofA-imgs/main/Week-4-Regression-models/imgs/Errors.png\" width = \"700\">\n",
    "\n",
    "We can classify the errors into three types:\n",
    "\n",
    "**Bias:** Error between average predict values and the true model\n",
    "\n",
    "**Variance:** Variance of the values predicted by different models\n",
    "\n",
    "**Noise:** Error between sample target values and the true model\n",
    "\n",
    "In the case of the linear model, what type error do we have? It is mainly noise."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb30d104-3cb7-4f8a-828c-ff9796f95495",
   "metadata": {},
   "source": [
    "## Multivariate linear regression\n",
    "\n",
    "Can we improve the predictions by using the 6-feature or 86-feature datasets? We can use multivariate linear regression for these datasets.\n",
    "\n",
    "Multiple features: $$x_{i1}, ... , x_{iD}$$\n",
    "\n",
    "Target values: $$y_i$$\n",
    "\n",
    "Samples: $$i = 1, ..., N$$\n",
    "\n",
    "Prediction model: $$\\hat{y} = w_0  + w_1x_1 + ... + w_D x_D$$\n",
    "\n",
    "Loss function: Sum of square errors\n",
    "\n",
    "$$F(\\textbf{w}) = \\frac{1}{2} \\sum_{i}(y_i - \\sum_{k} w_k x_{ik} - w_0)^2$$\n",
    "\n",
    "The model is fitted by minimizing the loss function (sum of squared errors)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "476f761a-2ca5-4ce6-8f04-429246a6bc4c",
   "metadata": {},
   "source": [
    "One way to minimize the loss function is by taking its derivative and finding the solution where it is equal to 0.\n",
    "\n",
    "This results in the **Normal Equation**:\n",
    "\n",
    "$$\\hat{\\textbf{w}} = (\\textbf{X}^T \\textbf{X})^{-1} \\textbf{X}^T \\textbf{y}$$\n",
    "\n",
    "Where $\\hat{\\textbf{w}}$ is the weight vector that minimizes the loss function, $\\textbf{X}$ is the feature matrix (with a first column of ones to model the intercept), and $\\textbf{y}$ is the target values vector."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78d1f798-6582-4779-bb5e-f5a5aa7740bf",
   "metadata": {},
   "source": [
    "## Gradient descent\n",
    "\n",
    "Unfortunately if the number of features is very large (e.g. 100,000+) using the normal equation becomes impractical, because of the computational cost of inverting the feature matrix. An alternative is to use gradient descent. Gradient descent is an iterative process, where the weight vector is initialised to a random value and then iteratively updated until it converges to a local minimum.\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/SirTurtle/ML-BME-UofA-imgs/main/Week-4-Regression-models/imgs/GradientDescent.png\" width = \"300\">\n",
    "\n",
    "It is important that the learning rate is set well for the algorithm to converge. If the learning rate is too small, the algorithm may take a very large number of steps to converge. If the learning rate is too large the algorithm may oscillate instead of converging.\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/SirTurtle/ML-BME-UofA-imgs/main/Week-4-Regression-models/imgs/LearrningRate.png\" width = \"700\">\n",
    "\n",
    "Another issue to consider with gradient descent is how much of the training data to use at each iteration.\n",
    "\n",
    "**Batch gradient descent:** Uses all the training data at each iteration but can be very slow if we have a large number of samples.\n",
    "\n",
    "**Stochastic gradient descent:** Uses only one random training sample at each iteration. This is very fast but can be unstable.\n",
    "\n",
    "**Mini-batch gradient descent:** A compromise between batch and stochastic, this uses a subset of the training data at each iteration.\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/SirTurtle/ML-BME-UofA-imgs/main/Week-4-Regression-models/imgs/GradientDescentBatches.png\" width = \"700\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a35708e-7636-4e4b-af93-648971e6d87e",
   "metadata": {},
   "source": [
    "## Evaluating performance of regression models\n",
    "\n",
    "### $R^2$ score\n",
    "\n",
    "This is the proportion of variance in the data explained by the model. A perfect fit would have $R^2 = 1$.\n",
    "\n",
    "$$R^2 = 1 - \\frac{\\sum_{i} (y_i - \\hat{y}_i)^2}{\\sum_{i} (y_i - \\bar{y})}$$\n",
    "\n",
    "where the $\\hat{y}_i$s are the predicted target values and $\\hat{y}$ is the average target value."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b31ae29-b668-4921-9b59-b553f94979d2",
   "metadata": {},
   "source": [
    "Let's use the $R^2$ score to compare linear regression models for each of the datasets.\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/SirTurtle/ML-BME-UofA-imgs/main/Week-4-Regression-models/imgs/R2_scores.png\" width = \"700\">\n",
    "\n",
    "The $R^2$ scores improve with the larger number of features. However, when we calculate cross-validation scores $R^2$ scores, the best performance is with the 6-feature dataset. This indicates that the 86-feature dataset is being overfitted to the noise in the data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73a4a7e6-df61-4263-a668-8e2df1c135a5",
   "metadata": {},
   "source": [
    "### Root mean squared error (RMSE)\n",
    "\n",
    "Another way to measure performance of regression models is with root mean squared error (RMSE).\n",
    "\n",
    "$$ \\text{RMSE} = \\sqrt{\\frac{1}{N} \\sum_{i} (y_i - \\hat{y}_i)^2}$$\n",
    "\n",
    "where the $y_i$s are the target values and the $\\hat{y}_i$s are the predicted values.\n",
    "\n",
    "Note that unlike $R^2$, the RMSE is in the same units as the target values (i.e. weeks in our example).\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/SirTurtle/ML-BME-UofA-imgs/main/Week-4-Regression-models/imgs/RMSE_scores.png\" width = \"700\">\n",
    "\n",
    "Better RMSE scores are lower values (closer to 0). The RMSE scores agree with $R^2$ scores. The best model is on the 6-feature dataset. The 86-feature dataset has been overfitted."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9983bbdc-245d-4179-b9b0-3c64326c61bb",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Penalised Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91330a3f-6f54-4433-b6e9-5c79103dba81",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f8b0c82c-58c0-47f3-abae-229457b9abca",
   "metadata": {},
   "source": [
    "## Nonlinear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fcc2dac-0071-4b08-ab9b-1b10150f6119",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4c0354a3-e246-4b5e-ac15-393f68e97f62",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
