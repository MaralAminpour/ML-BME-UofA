{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "57170492-140c-4d56-8e94-4a49232293ef",
   "metadata": {},
   "source": [
    "# Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6320406c-7cb4-4c24-8773-fe1aa94795f1",
   "metadata": {},
   "source": [
    "## Application: neonatal brain growth\n",
    "\n",
    "- Around the time of birth the brain grows very quickly\n",
    "- Preterm birth alters brain development\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/SirTurtle/ML-BME-UofA-imgs/main/Week-4-Regression-models/imgs/DevelopingPretermBrain.png\" width = \"300\" style=\"float: left;\">\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/SirTurtle/ML-BME-UofA-imgs/main/Week-4-Regression-models/imgs/Baby.png\" width = \"200\" style=\"float: right;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4519d6ff-afc0-4cfe-9592-ccbbe13f318e",
   "metadata": {},
   "source": [
    "We will demonstrate the regression concepts using the application of brain growth in preterm neonates. Around the time of birth the brain grows very quickly. Preterm birth can disrupt this process, and therefore preterm brain development is a subject of extensive research.\n",
    "\n",
    "To investigate the changes caused by preterm birth, we can acquire MRI scans of newborn babies. We can perform automatic segmentations of various brain structures and measure their volumes. The features in the datasets we'll be working with are brain volumes, either for (1) the whole brain, (2) six brain tissues, or (3) 86 brain structures.\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/SirTurtle/ML-BME-UofA-imgs/main/Week-4-Regression-models/imgs/BrainMRI-BrainSegmentation.png\" width = \"700\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fe8f6db-e261-45eb-8d19-1483be8b96c6",
   "metadata": {},
   "source": [
    "The machine learning regression is the predict the age (target value) of a baby from volumes of brain structures (features).\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/SirTurtle/ML-BME-UofA-imgs/main/Week-4-Regression-models/imgs/AgeVolumePlot.png\" width = \"400\">\n",
    "\n",
    "In this plot each circle corresponds to a baby (a sample), on the x-axis we have the brain volume (a feature scaled with StandardScaler). The age (target value) is plotted on the y-axis. In this case we are showing a univariate linear regression model in red, with two parameters (slope and intercept). We have achieved R2 score 0.85 which is quite good, so the linear model fits this dataset well."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb30d104-3cb7-4f8a-828c-ff9796f95495",
   "metadata": {},
   "source": [
    "## Multivariate linear regression\n",
    "\n",
    "Can we improve the predictions by using the 6-feature or 86-feature datasets? We can use multivariate linear regression for these datasets.\n",
    "\n",
    "Multiple features: $$x_{i1}, ... , x_{iD}$$\n",
    "\n",
    "Target values: $$y_i$$\n",
    "\n",
    "Samples: $$i = 1, ..., N$$\n",
    "\n",
    "Prediction model: $$\\hat{y} = w_0  + w_1x_1 + ... + w_D x_D$$\n",
    "\n",
    "Loss function: Sum of square errors\n",
    "\n",
    "$$F(\\textbf{w}) = \\frac{1}{2} \\sum_{i}(y_i - \\sum_{k} w_k x_{ik} - w_0)^2$$\n",
    "\n",
    "The model is fitted by minimizing the loss function (sum of squared errors)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "476f761a-2ca5-4ce6-8f04-429246a6bc4c",
   "metadata": {},
   "source": [
    "One way to minimize the loss function is by taking its derivative and finding the solution where it is equal to 0.\n",
    "\n",
    "This results in the **Normal Equation**:\n",
    "\n",
    "$$\\hat{\\textbf{w}} = (\\textbf{X}^T \\textbf{X})^{-1} \\textbf{X}^T \\textbf{y}$$\n",
    "\n",
    "Where $\\hat{\\textbf{w}}$ is the weight vector that minimizes the loss function, $\\textbf{X}$ is the feature matrix (with a first column of ones to model the intercept), and $\\textbf{y}$ is the target values vector."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78d1f798-6582-4779-bb5e-f5a5aa7740bf",
   "metadata": {},
   "source": [
    "## Gradient descent\n",
    "\n",
    "Unfortunately if the number of features is very large (e.g. 100,000+) using the normal equation becomes impractical, because of the computational cost of inverting the feature matrix. An alternative is to use gradient descent. Gradient descent is an iterative process, where the weight vector is initialised to a random value and then iteratively updated until it converges to a local minimum.\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/SirTurtle/ML-BME-UofA-imgs/main/Week-4-Regression-models/imgs/GradientDescent.png\" width = \"300\">\n",
    "\n",
    "It is important that the learning rate is set well for the algorithm to converge. If the learning rate is too small, the algorithm may take a very large number of steps to converge. If the learning rate is too large the algorithm may oscillate instead of converging.\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/SirTurtle/ML-BME-UofA-imgs/main/Week-4-Regression-models/imgs/LearrningRate.png\" width = \"700\">\n",
    "\n",
    "Another issue to consider with gradient descent is how much of the training data to use at each iteration.\n",
    "\n",
    "**Batch gradient descent:** Uses all the training data at each iteration but can be very slow if we have a large number of samples.\n",
    "\n",
    "**Stochastic gradient descent:** Uses only one random training sample at each iteration. This is very fast but can be unstable.\n",
    "\n",
    "**Mini-batch gradient descent:** A compromise between batch and stochastic, this uses a subset of the training data at each iteration.\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/SirTurtle/ML-BME-UofA-imgs/main/Week-4-Regression-models/imgs/GradientDescentBatches.png\" width = \"700\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a35708e-7636-4e4b-af93-648971e6d87e",
   "metadata": {},
   "source": [
    "## Evaluating performance of regression models\n",
    "\n",
    "### $R^2$ score\n",
    "\n",
    "This is the proportion of variance in the data explained by the model. A perfect fit would have $R^2 = 1$.\n",
    "\n",
    "$$R^2 = 1 - \\frac{\\sum_{i} (y_i - \\hat{y}_i)^2}{\\sum_{i} (y_i - \\bar{y})}$$\n",
    "\n",
    "where the $\\hat{y}_i$s are the predicted target values and $\\hat{y}$ is the average target value."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b31ae29-b668-4921-9b59-b553f94979d2",
   "metadata": {},
   "source": [
    "Let's use the $R^2$ score to compare linear regression models for each of the datasets.\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/SirTurtle/ML-BME-UofA-imgs/main/Week-4-Regression-models/imgs/R2_scores.png\" width = \"700\">\n",
    "\n",
    "The $R^2$ scores improve with the larger number of features. However, when we calculate cross-validation scores $R^2$ scores, the best performance is with the 6-feature dataset. This indicates that the 86-feature dataset is being overfitted to the noise in the data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73a4a7e6-df61-4263-a668-8e2df1c135a5",
   "metadata": {},
   "source": [
    "### Root mean squared error (RMSE)\n",
    "\n",
    "Another way to measure performance of regression models is with root mean squared error (RMSE).\n",
    "\n",
    "$$ \\text{RMSE} = \\sqrt{\\frac{1}{N} \\sum_{i} (y_i - \\hat{y}_i)^2}$$\n",
    "\n",
    "where the $y_i$s are the target values and the $\\hat{y}_i$s are the predicted values.\n",
    "\n",
    "Note that unlike $R^2$, the RMSE is in the same units as the target values (i.e. weeks in our example).\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/SirTurtle/ML-BME-UofA-imgs/main/Week-4-Regression-models/imgs/RMSE_scores.png\" width = \"700\">\n",
    "\n",
    "Better RMSE scores are lower values (closer to 0). The RMSE scores agree with $R^2$ scores. The best model is on the 6-feature dataset. The 86-feature dataset has been overfitted."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9983bbdc-245d-4179-b9b0-3c64326c61bb",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Penalised Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91330a3f-6f54-4433-b6e9-5c79103dba81",
   "metadata": {},
   "source": [
    "### Ridge regression\n",
    "\n",
    "Overfitting can be avoided by reducing the number of parameters in the model (in the case of linear regression models this can be done by reducing the number of features). An alternative is **regularization**, where a penalty term is added to the loss function.\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/SirTurtle/ML-BME-UofA-imgs/main/Week-4-Regression-models/imgs/RidgePenalty.png\" width = \"500\">\n",
    "\n",
    "The **ridge penalty** penalizes weights with a large magnitude. It is also called the **L2 norm**. $\\lambda$ is a hyperparameter that determines the strength of the regularization.\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/SirTurtle/ML-BME-UofA-imgs/main/Week-4-Regression-models/imgs/L2_norm.png\" width = \"200\">\n",
    "\n",
    "The ridge penalty rises quadratically as the magnitude of the weight increases."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a36ace0e-d9d3-4b34-9a25-bfbe38968eee",
   "metadata": {},
   "source": [
    "The behavior of ridge regression depends on hyperparameter $\\lambda$.\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/SirTurtle/ML-BME-UofA-imgs/main/Week-4-Regression-models/imgs/RidgeRegressionBehavior.png\" width = \"600\">\n",
    "\n",
    "You can use a grid search to find the optimal lambda values."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fe37998-1afc-4f5f-8ede-7715e87b3039",
   "metadata": {},
   "source": [
    "Does ridge regression help avoid overfitting on our sample datasets?\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/SirTurtle/ML-BME-UofA-imgs/main/Week-4-Regression-models/imgs/RidgeRegresion_scores.png\" width = \"600\">\n",
    "\n",
    "Yes, with an optimal lambda, the overfitting on the 86-feature dataset is avoided."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3f6a165-5faa-414e-914a-b7d9c9239ba5",
   "metadata": {},
   "source": [
    "### Lasso regression\n",
    "\n",
    "An alternative type of regulariztion is **Lasso regression**. This uses the **Lasso penalty** or **L1 norm**.\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/SirTurtle/ML-BME-UofA-imgs/main/Week-4-Regression-models/imgs/LassoPenalty.png\" width = \"500\">\n",
    "\n",
    "The **ridge penalty** penalizes weights with a large magnitude. It is also called the **L2 norm**. $\\lambda$ is a hyperparameter that determines the strength of the regularization.\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/SirTurtle/ML-BME-UofA-imgs/main/Week-4-Regression-models/imgs/L1_norm.png\" width = \"200\">\n",
    "\n",
    "Compared with Ridge, Lasso tends to reduce weights to 0 and thus produces sparse solutions (many weights with value = 0)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dc15015-b888-4ca2-bdaa-f028b38847ae",
   "metadata": {},
   "source": [
    "### Comparison of Ridge and Lasso\n",
    "\n",
    "- Ridge and Lasso penalties both decrease the magnitude of weights\n",
    "- Ridge penalises weights with large magnitude more\n",
    "- Lasso penalises weights with small magnitude more\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/SirTurtle/ML-BME-UofA-imgs/main/Week-4-Regression-models/imgs/ComparisonRidgeLasso.png\" width = \"700\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8b0c82c-58c0-47f3-abae-229457b9abca",
   "metadata": {},
   "source": [
    "## Non-linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fcc2dac-0071-4b08-ab9b-1b10150f6119",
   "metadata": {},
   "source": [
    "Let’s now have a look how we can formulate a non linear regression model. In machine learning it is common to consider non-linear _feature transformation_ followed by multivariate linear regression model. We have already seen this when we introduced polynomial regression. In general terms, we denote the non linear feature transformation by $\\phi$ (_phi_), and the prediction model will then become:\n",
    "\n",
    "$$\\hat{y} = \\phi(x)^T \\textbf{w}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3be4cad4-69c0-4590-ade8-44e86eaeac79",
   "metadata": {},
   "source": [
    "For example, if we use a polynomial regression model of the second degree, then:\n",
    "\n",
    "Feature transformation for 1D feature vector x:\n",
    "\n",
    "$$\\phi(x) = (1, x, x^2)^T$$\n",
    "\n",
    "Prediction model:\n",
    "\n",
    "$$\\hat{y} = \\phi(x)^T \\textbf{w} = w_0 + x w_1 + x^2 w_2$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c0354a3-e246-4b5e-ac15-393f68e97f62",
   "metadata": {},
   "source": [
    "Note: Feature transformation usually increases dimension of a feature vector. In the case of univariate polynomial regression 1D feature vector x is transformed to an $M + 1$ dimensional vector $(1, x, x^2, ..., x^M)^T$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a0c9eeb-ec84-4969-bdfe-8318c9c05a61",
   "metadata": {},
   "source": [
    "Loss function for non-linear regression:\n",
    "\n",
    "$$F(\\textbf{w}) = \\frac{1}{2} \\sum_{i}(y_i - \\phi(\\textbf{x}_i)^T \\textbf{w})^2$$\n",
    "\n",
    "Non-linear ridge regression (to avoid overfitting)\n",
    "\n",
    "$$F(\\textbf{w}) = \\frac{1}{2} \\sum_{i}(y_i - \\phi(\\textbf{x}_i)^T \\textbf{w})^2 + \\frac{\\lambda}{2}\\textbf{w}^T\\textbf{w}$$\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/SirTurtle/ML-BME-UofA-imgs/main/Week-4-Regression-models/imgs/PolynomialRidgeRegression2.png\" width = \"700\">\n",
    "\n",
    "The loss function will then become a sum of squared errors between the expected target values $y_i$ and the target values predicted using the modified model. Because we tend to significantly increase the number of features by introducing feature transformation, we will include ridge penalty in our non linear regression model.\n",
    "\n",
    "In this plot you can see how ridge penalty can be useful for polynomial regression. In the plot on the left we have fitted polynomial of 10 th degree to predict age from cortical volume. We can see that model slightly overfitted the data and CV R2 is 0.88. If we add ridge penalty with lambda 0.25, the overfitting is reduced and the CV $R^2$ increases to 0.9."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40f76f5d-5786-4f3f-86ac-0a205d563586",
   "metadata": {},
   "source": [
    "How does polynomial ridge regression change with increasing number of features?\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/SirTurtle/ML-BME-UofA-imgs/main/Week-4-Regression-models/imgs/PolynomialRidgeRegressionPerformance.png\" width = \"700\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09aa6d55-a695-48b6-90ed-3e731bab5b2c",
   "metadata": {},
   "source": [
    "### Kernel trick\n",
    "\n",
    "The kernel trick helps us to design more versatile non-linear regression models.\n",
    "\n",
    "We define a **dual representation** $a$ of our model parameter vector $\\textbf{w}$\n",
    "\n",
    "$$\\textbf{w} = \\phi^T a$$\n",
    "\n",
    "The prediction model then becomes\n",
    "\n",
    "\n",
    "$$\\hat{y} = \\phi^T(x) \\textbf{w} = \\phi^T(x) \\phi^T a = \\sum^N_{i=1} \\phi^T(x) + \\phi(x_i) a_i$$\n",
    "\n",
    "We now define a **kernel** $\\kappa$\n",
    "\n",
    "$$\\kappa(x, x_i) = \\phi^T(x)\\phi(x_i)$$\n",
    "\n",
    "The kernel represents a similarity between two feature vectors.\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/SirTurtle/ML-BME-UofA-imgs/main/Week-4-Regression-models/imgs/GaussianKernel.png\" width = \"300\"  align=\"right\">\n",
    "\n",
    "For example, we can use a **gaussian kernel**\n",
    "\n",
    "$$\\kappa(x, x_i) = e^{-\\frac{\\lvert x - x_i \\rvert_2^2}{2\\sigma}}$$\n",
    "\n",
    "$$\\lvert x - x_i \\rvert_2^2 = \\sum_k(x_k- x_{ik})^2$$\n",
    "\n",
    "Note:\n",
    "- The original parameter vector $\\textbf{w}$ vector has $D$ elements (one per each feature)\n",
    "- The dual parameter vector $a$ has $N$ elements (one per each sample)\n",
    "\n",
    "The resulting **dual prediction model** is a linear combination of kernels placed around the feature vectors $x_i$\n",
    "\n",
    "$$\\hat{y} = \\sum^N_{i=1} \\kappa(x, x_i) a_i$$\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/SirTurtle/ML-BME-UofA-imgs/main/Week-4-Regression-models/imgs/DualPredictionModel.png\" width = \"700\">\n",
    "\n",
    "In this plot we can see three samples with feature vectors $x_1$, $x_2$ and $x_3$. The gaussian kernels are placed around them, multiplied by\n",
    "coefficients ai and then summed up to produce the prediction model. We can also see, that large kernels produce smoother models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aba9e1d8-5152-4854-bcd2-dfd06141fc32",
   "metadata": {},
   "source": [
    "We will see more details on non-linear regression in Notebook 4.4."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
