{"cells":[{"cell_type":"markdown","metadata":{"id":"N2ZMEve3pYq4"},"source":["# Predicting age from brain structures\n","\n","<img src=\"https://drive.google.com/uc?export=view&id=1bSgR27zLYH_eW0VD4jjGYF2uLHQJWvJh\" width = \"400\" style=\"float: right;\">\n","\n","In this tutorial, we will take the concepts we have learned so far and apply them to our case study, which involves predicting a baby's age based on the volumes of their brain structures. We will learn how to tune the parameters of the network and devise a non-linear solution.\n","\n","To begin, we will load our dataset, which comprises **86 brain volume observations**. Afterward, we'll transform both the feature matrix and target vector into tensors. Run the cell below to load the dataset with 86 structures."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"d09suPr0pYq8"},"outputs":[],"source":["# only do this if you work on Google Colab\n","# run the cell\n","# then upload file 'GA-brain-volumes-86-features.csv'\n","\n","from google.colab import files\n","files.upload()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"cIEoLXrNpYq-"},"outputs":[],"source":["import torch\n","import pandas as pd\n","import numpy as np\n","from sklearn.preprocessing import StandardScaler\n","\n","import warnings\n","warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n","\n","def CreateFeaturesTargets(filename):\n","\n","    df = pd.read_csv(filename,header=None)\n","\n","    # convert from 'DataFrame' to numpy array\n","    data = df.values\n","\n","    # Features are in columns one to end\n","    X = data[:,1:]\n","\n","    # Scale features\n","    X = StandardScaler().fit_transform(X)\n","\n","    # Labels are in the column zero\n","    y = data[:,0].reshape(-1,1)\n","\n","    # return Features and Labels\n","    return X, y\n","\n","X,y = CreateFeaturesTargets('GA-brain-volumes-86-features.csv')\n","\n","# perform scaling of the target values to support better convergence\n","target_scaler = StandardScaler()\n","y = target_scaler.fit_transform(y)\n","\n","print('Number of samples is', X.shape[0])\n","print('Number of features is', X.shape[1])\n","\n","# convert to tensors\n","X = torch.from_numpy(X).float()\n","y = torch.from_numpy(y).float()\n","print('X shape: ', X.shape)\n","print('y shape: ', y.shape)\n","print('X type: ', X.type())\n","print('y type: ', y.type())"]},{"cell_type":"markdown","metadata":{"id":"QK_NvQCDpYq_"},"source":["Observation 1: It's important to note that we've transformed our target values into a two-dimensional vector. Both the feature matrix and target vector are Pytorch tensors of type `float`, as Pytorch requires this format.\n","\n","Observation 2: Notably, we've also scaled the target values, which enhances the convergence of stochastic gradient descent. This is a change from previous exercises, where we used regression techniques that either relied on analytical solutions or alternative optimisers.\n","\n","The function `PlotTargets` displayed below has been used earlier to illustrate both the actual and predicted target values. Please review the function and execute the following cell."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"GJLe3ceVpYq_"},"outputs":[],"source":["import matplotlib.pyplot as plt\n","def PlotTargets(y_pred,y, label = 'Target values', plot_line=True):\n","    if plot_line:\n","        plt.plot([-3,3],[-3,3],'r', label = '$y=\\hat{y}$')\n","    plt.plot(y,y_pred,'o', label = label)\n","\n","    plt.xlabel('Expected target values')\n","    plt.ylabel('Predicted target values')\n","    plt.legend()"]},{"cell_type":"markdown","metadata":{"id":"SetdrY06pYq_"},"source":["## Exercise 4\n","\n","In this exercise you will train and evaluate a single layer perceptron to predict age of a baby from volumes of 86 brain structures. First we will split the dataset into **training, validation and test set**.\n","\n","This is different from what we have done before, but cross-validation is rarely used in deep learning, due to long training times. You will see later in this exercise how these three sets are used.\n","\n","n this exercise you will train and evaluate a single layer perceptron to predict age of a baby from volumes of 86 brain structures. First we will split the dataset into training, validation and test set.\n","\n","This is different from what we have done before, but cross-validation is rarely used in deep learning, because it tends to extend training times.\n","\n","But don't worry! As we navigate through this exercise together, you'll get a better understanding of how we utilize these three sets. Ready to jump in?"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"KuRWIFofpYrA"},"outputs":[],"source":["from sklearn.model_selection import train_test_split\n","\n","# extract test set\n","groups = np.round(y/3)\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.15, random_state=42, stratify=groups)\n","\n","# extract validation set\n","groups_val = np.round(y_train/3)\n","X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.15, random_state=42, stratify=groups_val)\n","\n","# display info\n","print('Training samples: ', y_train.shape[0])\n","print('Validation samples: ', y_val.shape[0])\n","print('Test samples: ', y_test.shape[0])"]},{"cell_type":"markdown","metadata":{"id":"-kDfxvAIpYrA"},"source":["In order to simplify the code and keep things organized, we've provided you with three handy functions below:\n","* `train` will perform one training epoch and return the current loss value\n","* `validate` will return the loss value without performing any training.\n","* `RMSE` will calculate the Root Mean Squared Error (RMSE) for the trained network and the specific dataset you've selected. An additional feature is that it considers the scaling of the target values, providing the result in weeks GA (Gestational Age).\n","\n","Feel free to take a moment to familiarize yourself with these functions, then go ahead and run the code."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1jIOdi-ApYrB"},"outputs":[],"source":["# performs one training epoch\n","# returns MSE loss\n","def train(net,X,y):\n","    # 1. Clear gradients\n","    optimizer.zero_grad()\n","    # 2. Forward pass\n","    prediction = net(X)\n","    # 3. Compute loss\n","    loss = loss_function(prediction, y)\n","    # 4. Calculate gradients\n","    loss.backward()\n","    # 5. Update network parameters\n","    optimizer.step()\n","    # return MSE loss\n","    return loss.data # we want only value, not gradients\n","\n","# calculates and returns the loss any training\n","def validate(net,X,y):\n","    with torch.no_grad(): # no need to calculate gradients\n","        # Forward pass\n","        prediction = net(X)\n","        # Calculate loss\n","        loss = loss_function(prediction, y)\n","        # return MSE loss\n","        return loss\n","\n","# Calculates RMSE in weeks GA\n","def RMSE(net,X,y):\n","    loss = validate(net,X,y).numpy()\n","    rmse = np.sqrt(loss*target_scaler.var_[0])"]},{"cell_type":"markdown","metadata":{"id":"U5xyEADmpYrB"},"source":["You might recall that we previously used a linear regression model in Notebook 9.1 to predict brain volume from age. Now, we'll be taking things a step further. However, for our present dataset, we need to make some adjustments to the existing code. Let's go through these modifications one-by-one:\n","\n","**Task 4.1:** Adjust **architecture** of the network so that it can be used to predict age from 86 structures.\n","\n","**Task 4.2:** Don't be disheartened if you notice that the network isn't training properly at first. In fact, an increasing training loss is often indicative that the **learning rate **is too high. By testing smaller learning rates, we might be able to solve this issue. Our aim is to select the highest learning rate which doesn't cause an increase in the training loss.\n","\n","**Task 4.3:** After tuning your learning rate, you might notice that the training loss continues to decrease quite significantly. This might mean that the network hasn't yet converged to an optimal solution. Try increasing the number of **epochs** to 1000 and see what happens. Can you figure out how many epochs were needed for the network to converge by looking at the MSE loss plot?\n","\n","**Task 4.4:** The number of epochs we've set may seem a bit arbitrary, and it might leave you wondering if it's either too few (with the network not having converged) or too many (resulting in the network overfitting). You may also be curious about the role of the **validation set**. Well, this set is used to monitor the performance of the network during training. In this task, you'll implement a monitoring system during epochs using the validation set. Here's how you can do it:\n","\n","- First, create a variable val_losses that will store the validation loss at each epoch. Initiate it before the `for` loop, just like `train_losses`.\n","\n","- At each epoch, call the function `validate` to calculate the loss on the validation set `X_val`, `y_val`. The validation loss returned by validate should then be appended to `val_losses`.\n","\n","- On the subplot `133`, plot the validation loss along with the training loss.\n","\n","- If required, adjust the number of epochs to 10000 to identify when the validation loss begins to rise.\n","\n","**Task 4.5:**Naturally, we'd like to select the model that shows the best performance on the validation set as our final trained model. This means we need to continue training as long as the validation set's loss is decreasing. The moment it starts increasing, we should stop training to prevent overfitting. This strategy is known as **early stopping** and serves as a form of regularisation. To implement early stopping, we need to `break` the `for` loop as soon as the validation loss starts rising. To do this, add the following code at the end of the for loop:\n","\n","`if(i>1):\n","     if val_losses[i-1]>val_losses[i-2]:\n","         print('Final iteration: ', i)\n","         break`\n","\n","\n"," **NOTE** Don't be surprised if not all network runs deliver equally well. Since we're using gradient descent and the weights of the network are initially set to random values, the fit might not always converge to an optimal solution. However, take heart that some runs will yield a good solution, not unlike the penalised regression techniques that we've discussed earlier in this module. Happy coding!"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"N0oWnDVSpYrC"},"outputs":[],"source":["import torch.nn as nn\n","class ANRegressor(nn.Module):\n","    def __init__(self):\n","        super(ANRegressor, self).__init__()\n","        self.layer = nn.Linear(1, 1)\n","\n","    def forward(self, x):\n","        x = self.layer(x)\n","        return x\n","\n","# create network\n","net = ANRegressor()\n","print(net)\n","\n","# mean squared error loss\n","loss_function = nn.MSELoss()\n","\n","# stochastic gradient descent optimiser\n","optimizer = torch.optim.SGD(net.parameters(), lr=0.2)\n","\n","# train\n","train_losses=[]\n","for i in range(10):\n","    loss = train(net, X_train, y_train)\n","    train_losses.append(loss) # we save losses to display them at the end\n","\n","# calculate training and test performance\n","rmse_train = RMSE(net,X_train,y_train)\n","print('Training RMSE: ', rmse_train)\n","rmse_val = RMSE(net,X_val,y_val)\n","print('Validation RMSE: ', rmse_val)\n","rmse_test = RMSE(net,X_test,y_test)\n","print('Test RMSE: ', rmse_test)\n","\n","# display results\n","plt.figure(figsize=(14,4))\n","\n","# plot training set predictions\n","plt.subplot(131)\n","PlotTargets(net(X_train).data,y_train)\n","plt.title('Training set')\n","\n","# plot validation and test set predictions\n","plt.subplot(132)\n","PlotTargets(net(X_val).data, y_val, label = 'val targets')\n","PlotTargets(net(X_test).data,y_test, label = 'test targets', plot_line=False)\n","plt.title('Validation and Test set')\n","\n","# plot training and validation loss\n","plt.subplot(133)\n","plt.plot(train_losses,label='training loss')\n","plt.title('MSE loss')\n","plt.legend()"]},{"cell_type":"markdown","metadata":{"id":"n_aluaL_pYrC"},"source":["## Exercise 5 (optional)\n","\n","Do this exercise if you finished early and have time to play with a more complex neural network. We will now tune a multi-layer perceptron to predict age based on the volumes of six distinct brain regions.\n","\n","First, we will load the dataset with 6 brain structures. Please note that executing the code below will replace the previous dataset.\n","\n","**Task 5.1:** FThere are a few missing parts in the code that need to be filled in to convert the feature matrix and target vector from numpy arrays to a format that's suitable for Pytorch training. Can you spot and fill them? Your journey into the deeper realms of neural networks begins now!"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Ilsf7gnJpYrC"},"outputs":[],"source":["# only do this if you work on Google Colab\n","# run the cell\n","# then upload file 'GA-brain-volumes-6-features.csv'\n","\n","from google.colab import files\n","files.upload()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"GyTQzhbZpYrD"},"outputs":[],"source":["X,y = CreateFeaturesTargets('GA-brain-volumes-6-features.csv')\n","\n","# perform scaling of the target values to support better convergence\n","target_scaler = StandardScaler()\n","y = target_scaler.fit_transform(y)\n","\n","print('Number of samples is', X.shape[0])\n","print('Number of features is', X.shape[1])\n","\n","# convert to tensors\n","X = None\n","y = None\n","print('X shape: ', X.shape)\n","print('y shape: ', y.shape)\n","print('X type: ', X.type())\n","print('y type: ', y.type())"]},{"cell_type":"markdown","metadata":{"id":"kDRjIEikpYrD"},"source":["**Task 5.2:** Create training, validation and test set similarly to exercise 4."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vCPFISsBpYrD"},"outputs":[],"source":["# extract test set\n","\n","\n","# extract validation set\n","\n","\n","# display info\n","print('Training samples: ', y_train.shape[0])\n","print('Validation samples: ', y_val.shape[0])\n","print('Test samples: ', y_test.shape[0])"]},{"cell_type":"markdown","metadata":{"id":"3Uvs1XTkpYrD"},"source":["**Task 5.3:** Time for some fun with training and performance evaluation! Use the same code you developed in Exercise 4. All you need to change is the architecture, adapting it to take in 6 input features. Everything else should work as is. Fiddle around with the learning rate to achieve the best performance.\n","\n","**Task 5.4:** Implement a multi-layer perceptron architecture with the following specs:\n","\n","* The first `Linear` layer should have with 6 outputs\n","* Apply `ReLU` activation\n","* Follow this up with a second `Linear` layer, with 6 inputs and 1 output.\n","\n","Does this non-linear network give you better results? Dive in and discover for yourself! Good luck!"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"x8AwhYxxpYrD"},"outputs":[],"source":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.3"},"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":0}