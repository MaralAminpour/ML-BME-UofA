{"cells":[{"cell_type":"markdown","metadata":{"id":"wJQowT-MkuXM"},"source":["# Your own single layer perceptron\n","\n","## Exercise 6\n","\n","In this notebook you will implement and train your own single layer perceptron classifier using **Wiscosin Breast Cancer dataset**.\n","\n","### Dataset\n","\n","Run the cells to load feature matrix and label vector and split the data into training and validation set."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"nCHv9VR5kuXQ","executionInfo":{"status":"ok","timestamp":1686858062648,"user_tz":360,"elapsed":962,"user":{"displayName":"Maral Aminpour","userId":"09973321392016137206"}},"outputId":"a8f583c8-0027-4e36-dab3-4527805302b8"},"outputs":[{"output_type":"stream","name":"stdout","text":["Features dim:  (569, 30)\n","Labels dim:  (569,)\n","We have 569 samples and 30 features.\n","We have labels:  [0 1]\n"]}],"source":["from sklearn import datasets\n","from sklearn.preprocessing import StandardScaler\n","import numpy as np\n","import matplotlib.pyplot as plt\n","\n","# load feature matrix and label vector\n","data = datasets.load_breast_cancer()\n","X = StandardScaler().fit_transform(data.data)\n","y = data.target\n","\n","print('Features dim: ', X.shape)\n","print('Labels dim: ', y.shape)\n","print('We have {} samples and {} features.'.format(X.shape[0],X.shape[1]))\n","print('We have labels: ', np.unique(y))"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"2B6VLY31kuXS","executionInfo":{"status":"ok","timestamp":1686858062648,"user_tz":360,"elapsed":7,"user":{"displayName":"Maral Aminpour","userId":"09973321392016137206"}},"outputId":"f76d16a5-07da-4239-87f1-215bc47315da"},"outputs":[{"output_type":"stream","name":"stdout","text":["Training samples:  455\n","Validation samples:  114\n"]}],"source":["from sklearn.model_selection import train_test_split\n","\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n","print('Training samples: ', y_train.shape[0])\n","print('Validation samples: ', y_test.shape[0])"]},{"cell_type":"markdown","metadata":{"id":"AnKKi1GDkuXS"},"source":["### Architecture\n","\n","Our single layer perceptron will consist of\n","\n","- A single linear layer that takes in 30 inputs and gives out 1 output\n","- A sigmoid activation function to add that non-linear twist\n","\n","We will minimise **mean squared error loss**, , a common and efficient measure of prediction error.\n","\n","The functions below take care of implementing these architectural elements, along with a handy function to calculate a generic derivative. Give them a quick look-over and run the cell when you're ready. The foundations of your model are all set!"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"MX_nYXSSkuXT"},"outputs":[],"source":["# Sigmoid activation function\n","def sigmoid(x):\n","    return 1/(1+np.exp(-x))\n","\n","# Linear layer - we do not have a bias here for simplicity\n","def linear(x,w):\n","    return np.matmul(x, w)\n","\n","# Mean squared error loss\n","def mse(y, y_pred):\n","    return np.mean(np.square(y-y_pred))\n","\n","# A general derivative of a function (no need to implement individual derivatives)\n","def derivative(f, z, eps=0.000001):\n","    return (f(z + eps) - f(z - eps))/(2 * eps)"]},{"cell_type":"markdown","metadata":{"id":"QHFs2b-LkuXT"},"source":["### Theory\n","\n","The single layer perceptron is composed of one **linear layer** of neurons that can be expressed as\n","$$z=Xw$$\n","where $X$ is the feature matrix, $w$ are the parameters of the network and $z$ is the output of the linear layer. NOTE: to keep things simple, we're not going to implement the bias term in this exercise.\n","\n","The output of the linear layer then paasses trhough the activation function, denoted by **sigmoid** $\\sigma$:\n","\n","$$p=\\sigma(z)$$\n","\n","The output $p$ is essentially the predicted probability of the positive class 1.\n","\n","The goal here is to find the network parameter vector $w$ by minimizing the **minimising the mean squared error loss** between predicted probability $p$ and the actual labels $y$\n","$$loss=\\sum_{n=1}^N(y_n-p_n)^2$$\n","\n","We can iteratively **update the weight vector** $w$ using gradient descent equation:\n","$$w^{iter+1}=w^{iter}-\\eta \\frac{\\partial loss}{\\partial w}$$\n","where $w^{iter}$ is estimated weight vector at iteration $iter$ and $\\eta$ is the learning rate.\n","\n","As we've learned in the lecture, for the MSE loss and sigmoid activation, the derivative is calculated as follows:\n","\n","$$\\frac{\\partial loss}{\\partial w}=(y-p)\\sigma'(z)X$$\n","\n","Take a moment to absorb these equations, they'll be the engine behind your perceptron!"]},{"cell_type":"markdown","metadata":{"id":"_Q7gWUIekuXU"},"source":["### Training\n","\n","Complete the code below to train the single layer perceptron. The learning rate and number of epoch has been already set for you. We start by initialising the weight vector `w` to random values.\n","\n","**Task 6.1:** Implement the forward pass. To do that\n","* evaluate the linear layer for the training feature matrix `X_train`\n","* pass the result through the sigmoid activation\n","\n","**Task 6.2:** Calculate the training loss. There is also code to save it for plotting after we finished training.\n","\n","**Task 6.3:** Implement the backward pass. To do that you need to implement the calculation of the gradients of the\n","* loss\n","* activation\n","* linear layer\n","Code to multiply them together using the chain rule is there for you.\n","\n","**Task 6.4:** Implement the update of the network weights.\n","\n","Once you successfully complete the code, run it. You will see how the loss is evolving."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":425},"id":"EJrYrqkPkuXU","executionInfo":{"status":"error","timestamp":1686858106791,"user_tz":360,"elapsed":736,"user":{"displayName":"Maral Aminpour","userId":"09973321392016137206"}},"outputId":"f3662611-3123-4e36-ac75-df3d24859626"},"outputs":[{"output_type":"stream","name":"stdout","text":["Weight vector dimension:  (30,)\n"]},{"output_type":"error","ename":"TypeError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-4-4653d282b49b>\u001b[0m in \u001b[0;36m<cell line: 13>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0mtrain_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Epoch {}: MSE = {}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miter\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mround\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m     \u001b[0;31m# backward pass - calculate gradients\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/numpy/core/overrides.py\u001b[0m in \u001b[0;36mround_\u001b[0;34m(*args, **kwargs)\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py\u001b[0m in \u001b[0;36mround_\u001b[0;34m(a, decimals, out)\u001b[0m\n\u001b[1;32m   3771\u001b[0m     \u001b[0maround\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mequivalent\u001b[0m \u001b[0mfunction\u001b[0m\u001b[0;34m;\u001b[0m \u001b[0msee\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mdetails\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3772\u001b[0m     \"\"\"\n\u001b[0;32m-> 3773\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0maround\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecimals\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdecimals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3774\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3775\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/numpy/core/overrides.py\u001b[0m in \u001b[0;36maround\u001b[0;34m(*args, **kwargs)\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py\u001b[0m in \u001b[0;36maround\u001b[0;34m(a, decimals, out)\u001b[0m\n\u001b[1;32m   3346\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3347\u001b[0m     \"\"\"\n\u001b[0;32m-> 3348\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_wrapfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'round'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecimals\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdecimals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3349\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3350\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py\u001b[0m in \u001b[0;36m_wrapfunc\u001b[0;34m(obj, method, *args, **kwds)\u001b[0m\n\u001b[1;32m     52\u001b[0m     \u001b[0mbound\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mbound\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 54\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_wrapit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     55\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py\u001b[0m in \u001b[0;36m_wrapit\u001b[0;34m(obj, method, *args, **kwds)\u001b[0m\n\u001b[1;32m     41\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m         \u001b[0mwrap\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     44\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mwrap\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmu\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mTypeError\u001b[0m: unsupported operand type(s) for *: 'NoneType' and 'float'"]}],"source":["# set learning rate\n","lr = 0.01\n","\n","# set number of epochs\n","epochs = 20\n","\n","# initialize the weights randomly with mean 0\n","w = 2*np.random.random(np.array(X).shape[1]) - 1\n","print('Weight vector dimension: ', w.shape)\n","\n","# training loop\n","train_loss = []\n","for iter in range(epochs):\n","\n","    # forward pass - predict network output\n","    # implement linear layer\n","    z = None\n","    # predict output probabilities\n","    p_pred = None\n","\n","    # calculate and save training loss\n","    loss = None\n","    train_loss.append(loss)\n","\n","    print('Epoch {}: MSE = {}'.format(iter,np.round(loss,4)))\n","\n","    # backward pass - calculate gradients\n","    # gradient of the loss: error between predicted and true training labels\n","    loss_grad = None\n","    # gradient of activation: derivative of sigmoid\n","    act_grad = None\n","    # gradient of linear layer: feature matrix\n","    lin_grad = None\n","    # multiply gradients (chain rule)\n","    grad = np.matmul(loss_grad*act_grad,lin_grad)\n","\n","    # update the weights of the network\n","    w -= None\n","\n","# plot training loss during epochs\n","plt.plot(train_loss)"]},{"cell_type":"markdown","metadata":{"id":"UaqUbpdzkuXV"},"source":["**Task 6.5:** Evaluate the accuracy of the trained network on training and test set. To do that\n","* implement the forward pass of the network on training set to calculate the predicted labels (code given)\n","* evaluate accuracy on the training set using the given function `accuracy`\n","* implement the forward pass of the network in test set\n","* evaluate accuracy for the test set"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"QzGR3pC5kuXV","outputId":"cc1e8596-0b2f-441c-bdf5-67e8b2d816cb"},"outputs":[{"name":"stdout","output_type":"stream","text":["Train accuracy:  0.97\n","Test accuracy:  0.91\n"]}],"source":["def accuracy(y,y_pred):\n","    a = np.sum(y==y_pred)/y.shape[0]\n","    return a\n","\n","# predict on training set\n","y_pred = np.around(sigmoid(linear(X_train,w)))\n","acc_train = None\n","print('Train accuracy: ', np.round(acc_train,2))\n","\n","# predict on test set\n","y_pred_test = None\n","acc_test = None\n","print('Test accuracy: ', np.round(acc_test,2))"]},{"cell_type":"markdown","metadata":{"id":"6dggC3gikuXV"},"source":["### Cross Entropy Loss\n","\n","You already know that binary cross-entropy loss is more common to use for binary classification rather than MSE loss. **Binary Cross-entropy loss** is given by\n","$$BCELoss=\\sum_{n=1}^N(-y_n \\log(p_n)-(1-y_n)\\log(1-p_n))$$\n","\n","We have used MSE loss in the lecture instead, because it is easier to calculate the derivatives. However, the derivative of BCE loss with sigmoid activation with respect to the weights actually results in a much simple derivative:\n","\n","$$\\frac{\\partial BCELoss}{\\partial w}=(y-p)X$$\n","\n","**Task 6.6 (optional):** Derive the derivative of cross entropy loss. (Solution not given for this task)\n","\n","**Task 6.7:** Complete the implementation of BCE loss in the cell below."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"f9u0oJZukuXV"},"outputs":[],"source":["# Binary Cross-entropy loss\n","def BCE(y,p):\n","    return -np.mean(np.log(p)*y_train + None)"]},{"cell_type":"markdown","metadata":{"id":"oudGqnfzkuXW"},"source":["**Task 6.8:** Implement the gradient of the binary cross entropy loss and train the single layer perceptron using the code below."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Ujt2sxWtkuXW","outputId":"a45ba667-a525-48cf-b34f-0287b76d9f6f"},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch 0: BCE = 0.5745\n","Epoch 1: BCE = 0.2917\n","Epoch 2: BCE = 0.2335\n","Epoch 3: BCE = 0.2032\n","Epoch 4: BCE = 0.1832\n","Epoch 5: BCE = 0.1685\n","Epoch 6: BCE = 0.1572\n","Epoch 7: BCE = 0.1481\n","Epoch 8: BCE = 0.1406\n","Epoch 9: BCE = 0.1343\n","Epoch 10: BCE = 0.129\n","Epoch 11: BCE = 0.1243\n","Epoch 12: BCE = 0.1203\n","Epoch 13: BCE = 0.1167\n","Epoch 14: BCE = 0.1135\n","Epoch 15: BCE = 0.1107\n","Epoch 16: BCE = 0.1081\n","Epoch 17: BCE = 0.1057\n","Epoch 18: BCE = 0.1036\n","Epoch 19: BCE = 0.1016\n","Train accuracy:  0.96\n","Test accuracy:  0.95\n"]},{"data":{"image/png":"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\n","text/plain":["<Figure size 432x288 with 1 Axes>"]},"metadata":{"needs_background":"light"},"output_type":"display_data"}],"source":["# set learning rate\n","lr = 0.001\n","\n","# set number of epochs\n","epochs = 20\n","\n","# initialize the weights randomly with mean 0\n","w = 2*np.random.random(np.array(X).shape[1]) - 1\n","\n","# training loop\n","train_loss = []\n","for iter in range(epochs):\n","\n","    # forward pass - predict labels\n","    z = linear(X_train,w)\n","    p_pred = sigmoid(z) # probabilities\n","\n","    # calculate and save training loss (need to thredhol)\n","    loss = BCE(y_train,p_pred)\n","    train_loss.append(loss)\n","\n","    print('Epoch {}: BCE = {}'.format(iter,np.round(loss,4)))\n","\n","    # backward pass - calculate gradients\n","    # error\n","    error = p_pred-y_train\n","    # gradient\n","    grad = np.matmul(error,X_train)\n","\n","    # update the weights of the network\n","    w -= grad * lr\n","\n","plt.plot(train_loss)\n","\n","# predict on training set\n","y_pred = np.around(sigmoid(linear(X_train,w)))\n","acc_train = accuracy(y_train,y_pred)\n","print('Train accuracy: ', np.round(acc_train,2))\n","\n","# predict on test set\n","y_pred_test = np.around(sigmoid(linear(X_test,w)))\n","acc_test = accuracy(y_test,y_pred_test)\n","print('Test accuracy: ', np.round(acc_test,2))"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.3"},"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":0}